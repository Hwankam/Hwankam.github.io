<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="CDA - chap1" /><meta property="og:locale" content="en" /><meta name="description" content="Introduction : Distributions and Inference for Categorical Data" /><meta property="og:description" content="Introduction : Distributions and Inference for Categorical Data" /><link rel="canonical" href="https://hwankam.github.io/posts/CDA1/" /><meta property="og:url" content="https://hwankam.github.io/posts/CDA1/" /><meta property="og:site_name" content="For Statistics" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-10-25T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="CDA - chap1" /><meta name="twitter:site" content="@twitter_username" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"description":"Introduction : Distributions and Inference for Categorical Data","url":"https://hwankam.github.io/posts/CDA1/","headline":"CDA - chap1","dateModified":"2021-11-02T00:21:02+09:00","datePublished":"2021-10-25T00:00:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://hwankam.github.io/posts/CDA1/"},"@type":"BlogPosting","@context":"https://schema.org"}</script><title>CDA - chap1 | For Statistics</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="For Statistics"><meta name="application-name" content="For Statistics"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> </a></div><div class="site-title mt-3"> <a href="/">For Statistics</a></div><div class="site-subtitle font-italic">kam's world</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/Hwankam" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['example','doamin.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>CDA - chap1</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>CDA - chap1</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> your_full_name </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Mon, Oct 25, 2021, 12:00 AM +0900" >Oct 25, 2021<i class="unloaded">2021-10-25T00:00:00+09:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Tue, Nov 2, 2021, 12:21 AM +0900" >Nov 2, 2021<i class="unloaded">2021-11-02T00:21:02+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2722 words">15 min read</span></div></div><div class="post-content"><h1 id="introduction--distributions-and-inference-for-categorical-data">Introduction : Distributions and Inference for Categorical Data</h1><p><br /></p><h3 id="11-categorical-response-data">1.1 Categorical Response Data</h3><p>범주형 자료의 다양한 예시 e.g. classification, how good a product</p><p><br /></p><h4 id="111-response-explanatory-variable-distinction">1.1.1 Response-Explanatory variable Distinction</h4><p>설명변수의 데이터 형태는 다양하다!</p><p><br /></p><h4 id="112-binary-nominal-ordinal-scale-distinction">1.1.2 Binary-Nominal-Ordinal Scale Distinction</h4><ol><li>binary - two categories<li>nominal - more than two categories without ordering<li>ordinal - ordered categories - distances between categories are unknown - e.g. patient condition<li>interval variable - numerical distances exist - e.g. annual income +<li>discrete interval variable<li>continuous variable</ol><p>어떻게 변수를 측정하는지에 따라 같은 자료에 대해 여러가지 형태의 변수를 만들 수 있다.</p><p>binary ~ interval variable 은 lower level 에서 higher level로 정렬되어 있는데, 낮은 수준에서 사용한 분석법은 높은 수준의 변수에서도 사용가능 (역은 안됨)</p><p><br /></p><h4 id="113-discrete-continuous-variable-distinction">1.1.3 Discrete-Continuous Variable Distinction</h4><p>discrete 와 continuous 의 차이는 셀수 있는지 여부. discrete 변수의 값이 너무 많으면 그건 연속형 변수로 보겠다!</p><p><br /></p><h4 id="114-quantitative---qualitative-variable-distinction">1.1.4 Quantitative - Qualitative Variable Distinction</h4><p>Nomial - Qualitative</p><p>oridnal - fuzzy (순서형 자료의 경우 크고 작고는 있으므로 이를 활용해서 양적변수처럼 인식할 수도 있을 것이다 =&gt; scale을 잘 활용해서 numerical score를 할당하면 가능할 것) (nomial data에서 ordinal data로 바뀌면 자유도가 줄어들기 때문에 Power는 더 커진다)</p><p>interval - Quantitative</p><p>예제 ) $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$ 에서 x가 nominal 데이터라면 더비 변수를 활용하기 때문에 x의 케이스가 4 인 경우 $\beta_1 = 0$ 임을 검정하는 것은 $\beta_1=\beta_2=\beta_3 = 0$ 을 검정하는 것과 동일하게 된다. 따라서 f statistic은 자유도가 (3, n-4) 인 분포에서 나오게 된다.</p><p>반면 x가 oridnal 데이터라면 이미 변수 안에 크기 관계가 정해져있으므로, $\beta_1 = 0$ 을 검정하는 것이므로 f statistic은 자유도가 (1, n-2) 인 분포에서 나오게 된다.</p><p><br /></p><h3 id="12-distributions-for-categorical-data">1.2 Distributions for Categorical Data</h3><ol><li>binomial —- binary obs 가 독립이 아니라면 hypergeometric<li>Multinomial —- $E(n_j) = n \pi_j$ , $var(n_j)=n \pi_j ( 1- \pi_j)$ , $cov(n_j, n_k) = -n \pi_j \pi_k$<li>Poisson —- binomial 에서 n이 매우 크고 $\pi$ 가 매우 작을 때 포아송 근사 가능. 포아송분포는 평균값이 증가면서 정규근사 됨.</ol><p><br /></p><h4 id="124-overdispersion">1.2.4 Overdispersion</h4><p>정의 : 특정 분포를 가정하고 관측을 했을 때, 매우 특이한 이상치들이 나올 수 있다.</p><p>상황 :</p><ol><li>특정 상황의 변화에 따라 분포의 모수가 바뀔 수 있음(베이지안 관점).<br /> e.g. $Poi(\theta)$ 에서 $var(Y) = E[var(Y \mid \mu)] + var[E(Y \mid \mu)] = E(\mu) + var(\mu) &gt; \theta $<li>분포가 서로 다른 모수를 가진 binomial 분포의 mixture 분포</ol><p>해결 : Quasi-Likelyhood and GLM (chapter 4.7)</p><p>일반적으로 GLM에서 score function을 구하면 다음과 같다</p><p>exponential dispersion family</p>\[f(y ; \theta_i, \phi) = exp(\frac {y_i\theta_i - b(\theta_i)} {a(\phi)} + c(y_i,\phi))\]<p>에 대해</p>\[Score \ f = \sum_{i=1} ^N \frac{(y_i - \mu_i)x_{ij}}{var(y_i)} \frac{\partial \mu_i}{\partial \eta_i} = 0 \ \ -- (*)\]<p>(단, $g(\mu_i) = \eta_i$)</p><p>반면 Quasi-likelihood estimate 은 특정 분포가 아닌 평균과 분산의 관계만을 가정한다. (예를 들자면 포아송분포의 평균은 분산과 동일한데, 이때 그 관계를 $\nu(\mu_i) = \mu_i$ 이런식으로 정의하는게 끝)</p><p>그럼 평균과 분산만의 관계로 어떻게 추정이 가능할까? ===&gt; 위의 (*) 함수를 사용한다! (exponential family 가 아니어도 이 식을 사용한다)</p>\[\sum_{i=1} ^N \frac{(y_i - \mu_i)x_{ij}}{\nu(\mu_i)} \frac{\partial \mu_i}{\partial \eta_i} = 0\]<p>를 만족하는 모수값을 추정값으로 삼는다. 이 때 평균과 분산의 관계에 의해서 식이 아래와 같이 변형되고 포아송분포에서는 quasi-likelihoood 를 사용한 추정량이 MLE와 동일한 것을 알 수 있다.</p>\[\sum_{i=1} ^N \frac{(y_i - \mu_i)x_{ij}}{\mu_i} \frac{\partial \mu_i}{\partial \eta_i} = 0\]<p>추가적으로, QL estimateor 는 GLM에서 찾은 MLE의 점근적 분산과 동일하다. $var(score \ f) = I_n(\beta) = var(X^TDV^{-1}(y-\mu)) = X^TWX$</p><p><br /></p><p>이제 실제로 overdispersion을 quasi-likelihood를 사용해서 다뤄보자.</p>\[\nu(\mu_i) = \phi \mu_i\]<p>라고 가정하면, $\phi &gt; 1 $ 일 때 overdispersion을 다룰 수 있다.</p><p>즉 분산이 $\nu(\mu_i) = \phi \nu^* (\mu_i) $ 일 때</p>\[\chi^2 = \sum_{i=1} ^N \frac{(y_i - \hat \mu_i)}{\nu^*(\hat \mu_i)}\]<p>에 대해 $E(\chi^2 / \phi) \approx N-p$ 이므로 $\hat \phi = \chi^2 / (N-p)$ 로 추정 가능하다. 이를 활용해서 일반적인 GLM 에서 찾은 분산값에 $\hat \phi$ 를 곱해주면 overdispersion을 다룰 수 있다.</p><p><br /></p><h4 id="125-connection-between-poisson-and-multinomial-distributions">1.2.5 Connection between Poisson and Multinomial Distributions</h4><p>iid 포아송분포의 합은 당연히 포아송분포를 따르게 되는데, 만약 그 합의 개수가 N으로 정해져있다면?</p><p>$N= \sum Y_i $ 가 given이면 $Y_i$는 절대 N을 넘을 수 없고 이때는 더이상 Y 가 포아송분포가 아니게 된다.</p><p>pf)</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 70% 70%'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/img/cda1_1.png" width="70%" height="70%" title="1" alt="relative" /></p><p><br /></p><h4 id="126-the-chi-squared-distribution">1.2.6 The Chi-Squared Distribution</h4><p>카이제곱분포는 데이터들의 분포가 아니라 sampling distribution 이다.</p><p>자유도가 증가하면 normal로 근사</p><div class="language-R highlighter-rouge"><div class="code-header"> <span text-data=" R "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre><td class="rouge-code"><pre><span class="n">par</span><span class="p">(</span><span class="n">mfrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">4</span><span class="p">))</span><span class="w">
</span><span class="n">n</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">5</span><span class="p">,</span><span class="m">30</span><span class="p">,</span><span class="m">50</span><span class="p">,</span><span class="m">100</span><span class="p">)</span><span class="w">


</span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">4</span><span class="p">){</span><span class="w">
  </span><span class="n">chi.square</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">()</span><span class="w">
  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">j</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">rnorm</span><span class="p">(</span><span class="n">n</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w">
    </span><span class="n">chi.square</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">x</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="w">
  </span><span class="p">}</span><span class="w">
  </span><span class="n">hist</span><span class="p">(</span><span class="n">chi.square</span><span class="p">,</span><span class="n">probability</span><span class="o">=</span><span class="nb">T</span><span class="p">,</span><span class="w"> </span><span class="n">main</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">paste0</span><span class="p">(</span><span class="s1">'df='</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span><span class="w">
  </span><span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="n">from</span><span class="o">=</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">to</span><span class="o">=</span><span class="m">10</span><span class="o">*</span><span class="n">n</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w"> </span><span class="n">by</span><span class="o">=</span><span class="m">0.1</span><span class="p">)</span><span class="w">
  </span><span class="n">fy</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dchisq</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="w">
  </span><span class="n">lines</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="n">fy</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s1">'red'</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">

</span></pre></table></code></div></div><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 70% 70%'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/img/cda1_2.png" width="70%" height="70%" title="1" alt="relative" /></p><p><br /></p><h3 id="13-statistical-inference-for-categorical-data">1.3 Statistical Inference for Categorical Data</h3><h4 id="131-likelihood-functions-and-maximum-likelihood-estimation">1.3.1 Likelihood Functions and Maximum Likelihood Estimation</h4><p>MLE 는 regularity condition 하에서 asymtotically consistent and asymtotically efficient</p><p>(MLE 정의? likelihood function을 최대로 하는 추정값. 관찰된 데이터가 가장 잘 일어나게끔 하는 파라미터 추청값)</p><p>그렇기 때문에 Likelihood function이 더욱 curvature 할수록 estimator의 분산은 더욱 줄어든다.( 더욱 curvature 할수록 mle 값에서 멀어질수록 곡선이 빠르게 감소한다 )</p><p><br /></p><h4 id="132-likelihood-function-and-ml-estimate-for-binomial-parameter">1.3.2 Likelihood Function and ML Estimate for Binomial Parameter</h4><p>binomial distribution에 대한 mle를 구하면 $\hat \pi = y/n$ , $E(\hat \pi)$ 와 $var(\hat \pi)$ 를 구할 수 있음.</p><p>또한 $\hat \pi$ 에 대한 asymtotic variance를 fisher information을 통해 구할 수 있는데, 이는 $var(\hat \pi)$ 와 같다.</p><p><br /></p><h4 id="133-wald---likelihood-ratio---score-test-traid">1.3.3 Wald - Likelihood Ratio - Score Test Traid</h4><p>우선 여기서 제시하는 3가지 방법이 모두 likelihood function을 활용해서 추정을 하는 것임을 숙지해야 한다.</p><p>null hypothesis : $\beta = \beta_0$</p><h5 id="wald"><strong>Wald</strong></h5><p>standard error 를 사용하는 방법. 가장 기본 form은 다음과 같은데, fisher information이 모수에 대한 f 이므로 모수 추정값을 plug-in</p>\[Z = (\hat \beta - \beta_0)/SE, \ \ \ where \ \ \ SE = 1/\sqrt{\mathbf(I(\hat\beta )}\]<p>Z 통계량은 샘플이 커질 수록 Normal로 근사하고, 양측검정일 경우 Z의 제곱값은 카이제곱분포를 따르므로 카이제곱 분포를 활용한 검정이 가능하다.</p><p>Multivariate으로 확장하면</p>\[(\hat \beta - \beta_0)^T[cov(\hat \beta)]^{-1}(\hat \beta - \beta_0)\]<h5 id="lr"><strong>LR</strong></h5>\[-2 log \Lambda = -2 log (L_0/L_1) = 2(l_1 - l_0)\]<p>이 통계량은 귀무가설하에 카이제곱분포로 근사하고, 자유도는 전체모수공간에서 파라미터 dimension 과 귀무가설하에서 파라미터 dimension의 차이이다.</p><h5 id="score"><strong>Score</strong></h5><p>score function을 사용하기 때문에 score test라고 불린다. 다른말로는 Lagrange multiplier test</p><p>score function $\dot l(\beta)$에 대해서</p>\[\dot l(\beta)^T [\mathbf{I}(\beta)]^{-1} \dot l(\beta)\]<p>형태를 생각하자. 그런 다음 귀무가설 하에서 모수값에 대한 MLE인 $\beta_0$ 값을 위 식에 plug-in</p><p>즉</p>\[\dot l(\beta_0)^T [\mathbf{I}(\beta_0)]^{-1} \dot l(\beta_0)\]<p><br /></p><h4 id="134-constructing-confidence-intervals-by-inverting-tests">1.3.4 Constructing Confidence Intervals by Inverting Tests</h4><p>위에서 제시한 세가지 추정 방법을 통해 각기 다른 CI를 찾아낼 수 있다. 특히 Normal 가정하에서 regression setting이면 추정량에 대한 CI는 모두 동일</p><p>$100(1-\alpha)%$ Confidence interval <br /></p><h5 id="wald-1"><strong>Wald</strong></h5><p>\(|\hat \beta - \beta_0|/SE &lt; Z_{\alpha /2}\)</p><h5 id="lr-1"><strong>LR</strong></h5><p>\(2(l_1 - l_0) &lt; \chi^2 (\alpha)\)</p><p>신뢰구간을 구하기 복잡함</p><h5 id="score-1"><strong>Score</strong></h5><p>이건 특별한 조건 하에서만 가능.</p><p>종합하면…. 만약 sample size N 이 충분히 크지 않아 Normal 가정이 성립하지 않는다면 신뢰구간 또한 올바르지 않을 것이다. 또한 parameter의 개수가 너무 많으면 MLE 추정량의 정확성이 떨어지기 때문에 이를 고려할 필요가 있다.</p><p><br /></p><h3 id="14-statistical-inference-for-binomial-parameters">1.4 Statistical Inference for Binomial Parameters</h3><p>binomial setting에서 wald와 score를 비교해보면 모수값에 대한 SE가 형태는 동일하되, wald 는 추정값을, score는 귀무가설 값을 plug-in 하기 때문에 귀무가설하에서의 추청량을 활용한 극한분포는 score의 경우에 normal 근사가 더욱 잘될 것이다. 특히 참값이 귀무가설과 유사할 경우는 더욱 그러할 것.</p><h4 id="144-exact-small-sample-inference-and-the-mid-p-value">1.4.4 Exact Small-Sample Inference and the Mid P-Value</h4><p>computation power로 인해서, normal 근사를 하지 않고도 Exact p-value를 구할 수 있다.</p><p>다만 이산형자료의 경우 정확하게 p-value가 0.05가 되지 않기 때문에 때로는 보수적인 검정을 하게된다. 즉 유의수준보다 더 작은 값을 갖는 p-value를 기준으로 기각역을 설정하게 된다. 표본이 작은 경우 이산형 자료에 맞는 p-value 값을 주기 위해서 아래의 p-value를 활용할 수 있다.</p>\[mid \ \ P-value \ = \frac {P(T=t_0)}{2} + P(T&gt;t_0)\]<p>이 값을 활용하면 이산형에서도 p-value = 0.5를 만들 수 있다 (즉 홀수개의 자료에서도 p-value 값을 합리적으로 구할 수 있다.)</p><p><br /></p><h3 id="15-statistical-inference-for-multinomial-parameters">1.5 Statistical Inference for Multinomial Parameters</h3><h4 id="151-estimation-of-multinomial-parameters">1.5.1 Estimation of multinomial parameters</h4><p>라그랑지안을 사용해서 증명이 가능하다.</p>\[\hat \pi = n_j / n\]<h4 id="152-pearson-chi-squared-test-of-a-specified-multinomial">1.5.2 Pearson Chi-Squared Test of a Specified Multinomial</h4><p>pearson chi-squared test는 categorical data 분석에 아주 큰 영향을 주었다.</p><p>귀무가설 $H_0 : \pi_j = \pi_{j0} $ ,=1,2,3,…,c 에 대한 통계량은 다음과 같다. $\mu_j = n\pi_{j0}$ 일 때</p>\[\chi^2 = \sum_j \frac{(n_j - \mu_j)^2}{\mu_j} \ \ \sim \chi^2 (c-1) \ \ under \ H_0\]<p>multinomial 에서 score statistic과 일치한다</p><h4 id="153-likelihood-ratio-chi-squared-test-of-a-specified-multinomial">1.5.3 Likelihood-Ratio Chi-squared test of a specified multinomial</h4><p>귀무가설 $H_0 : \pi_j = \pi_{j0} $ ,=1,2,3,…,c 에 대한 통계량은 다음과 같다.</p>\[G^2 = 2log\Lambda = 2 \sum_j n_j log(n_j/n \pi_{j0}) \ \ \sim \chi^2 (c-1) \ \ under \ H_0\]<h4 id="155-testing-with-estimated-expected-frequencies">1.5.5 Testing with Estimated Expected Frequencies</h4><p>만약 귀무가설 $H_0 : \pi_j = \pi_{j0} $ 에 대해 $\pi_{j0} = \pi_{j0}(\theta)$ 라고 하자. 즉 귀무가설이 어떤 추정량의 함수형태로 나온다.(1.5.6 참고) MLE의 plug-in 성질에 의해 $\hat \mu_j = n\pi_{j0}(\hat \theta)$ 이고 이를 활용해 $\chi^2$ 이나 $G^2$ 값을 구한 다. 이 때 자유도는 $\theta$의 차원이 p 일 때 (c-1)-p 이다.</p><h4 id="157-chi-squared-theoretical-justification">1.5.7 Chi-Squared Theoretical Justification</h4><p>CLT 에 의해</p>\[\sqrt{n}(\hat \pi - \pi _0) \Rightarrow^d N(0, \Sigma_0)\]<p><br /></p><h3 id="16-bayesian-inference-for-binomial-and-multinomial-parameters">1.6 Bayesian Inference for Binomial and Multinomial Parameters</h3><p>베이지안은 CI 대신에 poeterior interval or credible interval을 사용한다. 이때는 tail 양 끝 값의 확률이 동일하도록 신뢰구간을 설정.(예를 들어 95% 신뢰수준이면 2.5 ~ 97.5로)</p><p>unimodal의 경우 <em>highest posterior density(HPD)</em> 을 사용하는데 이는 구간의 길이를 가능한 짧게 만드는 신뢰구간이다.</p></div><div class="post-tail-wrapper text-muted"><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/categorical-data/" class="post-tag no-text-decoration" >categorical data</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=CDA - chap1 - For Statistics&url=https://hwankam.github.io/posts/CDA1/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=CDA - chap1 - For Statistics&u=https://hwankam.github.io/posts/CDA1/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=CDA - chap1 - For Statistics&url=https://hwankam.github.io/posts/CDA1/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/PRML6/">PRML-6</a><li><a href="/posts/PRML5/">PRML-5</a><li><a href="/posts/PRML4/">PRML-4</a><li><a href="/posts/PRML3/">PRML-3</a><li><a href="/posts/CASI9/">Computer Age Statistical Inference - chap 9</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/datascience/">datascience</a> <a class="post-tag" href="/tags/datamining/">datamining</a> <a class="post-tag" href="/tags/machinelearning/">machinelearning</a> <a class="post-tag" href="/tags/islr/">ISLR</a> <a class="post-tag" href="/tags/statistical-method/">statistical method</a> <a class="post-tag" href="/tags/efron/">Efron</a> <a class="post-tag" href="/tags/prml/">PRML</a> <a class="post-tag" href="/tags/machine-learning/">machine learning</a> <a class="post-tag" href="/tags/machine-learning/">Machine learning</a> <a class="post-tag" href="/tags/categorical-data/">categorical data</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/CDA2/"><div class="card-body"> <span class="timeago small" >Oct 27, 2021<i class="unloaded">2021-10-27T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>CDA - chap2</h3><div class="text-muted small"><p> Describing Contingency Tables 2.1 Probaility structure for contingency tables 2.1.1 Contingency Tables I rows for categories of X J columns for categories of Y ==&gt; IJ possible combinations o...</p></div></div></a></div><div class="card"> <a href="/posts/ROC_AUC/"><div class="card-body"> <span class="timeago small" >Oct 31, 2021<i class="unloaded">2021-10-31T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>CDA - ROC/AUC</h3><div class="text-muted small"><p> 이 글은 범주형 데이터분석과 관련한 아티클을 읽고 정리한 글이다. 아티클 주소는 (https://towardsdatascience.com/understanding-the-roc-curve-and-auc-dd4f9a192ecb) 이다. ROC ( Receiver Operating Characterisitc ) curve binary classifi...</p></div></div></a></div><div class="card"> <a href="/posts/PRML6/"><div class="card-body"> <span class="timeago small" >Mar 1<i class="unloaded">2022-03-01T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>PRML-6</h3><div class="text-muted small"><p> 6. Kernel Methods [서론] there is a class of pattern recognition techniques ==&gt; training data points are kept, memory based method ex ) nearest neighborhood 방법은 training data와 가장 유사한 label을 te...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/CASI3/" class="btn btn-outline-primary" prompt="Older"><p>Computer Age Statistical Inference - chap 3</p></a> <a href="/posts/Online_COV_paper_4/" class="btn btn-outline-primary" prompt="Newer"><p>Online Covariance Estimation - paper - 4</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://twitter.com/username">your_full_name</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/datascience/">datascience</a> <a class="post-tag" href="/tags/datamining/">datamining</a> <a class="post-tag" href="/tags/machinelearning/">machinelearning</a> <a class="post-tag" href="/tags/islr/">ISLR</a> <a class="post-tag" href="/tags/statistical-method/">statistical method</a> <a class="post-tag" href="/tags/efron/">Efron</a> <a class="post-tag" href="/tags/prml/">PRML</a> <a class="post-tag" href="/tags/machine-learning/">machine learning</a> <a class="post-tag" href="/tags/machine-learning/">Machine learning</a> <a class="post-tag" href="/tags/categorical-data/">categorical data</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}}); </script> MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
