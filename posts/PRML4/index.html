<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="PRML-4" /><meta property="og:locale" content="en" /><meta name="description" content="4. Linear Models for Classification" /><meta property="og:description" content="4. Linear Models for Classification" /><link rel="canonical" href="https://hwankam.github.io/posts/PRML4/" /><meta property="og:url" content="https://hwankam.github.io/posts/PRML4/" /><meta property="og:site_name" content="For Statistics" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-01-19T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="PRML-4" /><meta name="twitter:site" content="@twitter_username" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"description":"4. Linear Models for Classification","url":"https://hwankam.github.io/posts/PRML4/","headline":"PRML-4","dateModified":"2022-01-27T23:39:26+09:00","datePublished":"2022-01-19T00:00:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://hwankam.github.io/posts/PRML4/"},"@type":"BlogPosting","@context":"https://schema.org"}</script><title>PRML-4 | For Statistics</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="For Statistics"><meta name="application-name" content="For Statistics"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> </a></div><div class="site-title mt-3"> <a href="/">For Statistics</a></div><div class="site-subtitle font-italic">kam's world</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/Hwankam" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['example','doamin.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>PRML-4</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>PRML-4</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> your_full_name </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Wed, Jan 19, 2022, 12:00 AM +0900" >Jan 19<i class="unloaded">2022-01-19T00:00:00+09:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Thu, Jan 27, 2022, 11:39 PM +0900" >Jan 27<i class="unloaded">2022-01-27T23:39:26+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3571 words">19 min read</span></div></div><div class="post-content"><h2 id="4-linear-models-for-classification">4. Linear Models for Classification</h2><p>이번 chapter는 “Input space를 K개의 Class로 나누는 것”이 핵심이다. 이때 나눠지는 영역은 decision region, 나누는 boundary를 decision boundary 혹은 decision surface라 한다. 특히 이번 chapter에서 중요한 것은 linear model이다. 즉 분류를 위한 decision surface가 linear function이며 D-dimensional input space를 (D-1)-dimensional hyperplane으로 나누는 것을 의미한다.</p><p><br /></p><p>linear model을 활용한 classification에서 주목하는 세 가지 접근방식은 다음과 같다.</p><font color="blue"> three different approaches to the classification </font><ol><li>Discriminant function ( X를 어떤 class에 할당할지 결정 )</ol><p>inference stage ( set conditional prob dist $p(C_k|\mathcal{x})$ ) $\&amp;$ decision stage (use dist to make decision ) 이렇게 두 가지 step으로 나눠서 classification을 할 때 condition prob dist를 어떻게 정할 것인지에 따라 또 나뉜다.</p><ol><li><p>parametric model ( 최적 parameter 찾기 )</p><li><p>generative approach ( Bayes’ rule -&gt; posterior)</p></ol><p><br /></p><p>그러나 classification에서는 linear model의 결과값이 discrete class label과 관련 있어야 하기 때문에 비선형함수인 activation function을 사용한다. (<strong>generalized</strong> linear model )</p><p>더 나아가, input space $\mathcal{X}$를 basis function $\phi(\mathcal{x})$을 활용해 확장하고 있는데 이를 통해 좀 더 유연한 모델을 만들어 낼 수 있을 것이다.</p><p><br /></p><h3 id="41-discriminant-functions">4.1 Discriminant Functions</h3><p>Discriminant 라는 것은 말 그대로 구분식! 즉, input $\mathcal{X}$를 어떤 Class에 넣을 것인지 결정해준다! 여기서는 특별히 linear discriminants 에 한정해서 설명하도록 하겠다.</p><p>“linear discriminants = decision surfaces are hyperplanes”</p><p><br /></p><h4 id="411-two-classes">4.1.1 Two Classes</h4><font size="4"> [Simplest representation of a linear discriminant]</font> \[y(\boldsymbol{x}) = \boldsymbol{w}^T\boldsymbol{x} \ + \ w_0\]<ul><li><p>$y(\boldsymbol{x})=0$ 은 decision boundary.</p><li>$\boldsymbol{w}$ 는 weight vector, decision boundary에 직교하는 벡터, decision boundary 방향 결정<li>$\ w_0$ 는 bias -&gt; negative bias는 threshold.</ul><font color="blue"> negative bias 가 threshold 인 이유 * $y(\boldsymbol{x}) &gt;0 $ 이면 Class 1 으로 판정하기 때문 * $\boldsymbol{x}$가 decision boundary에 위치할 때, 원점에서 decision boundary 까지 거리는 $$ \frac {\boldsymbol{w}^T\boldsymbol{x}}{||\boldsymbol{w}||} = \frac{-w_0}{||\boldsymbol{w}||} $$ ​ $\because$ x' 을 원점에서 가장 가까운 decision boundary 위의 점이라고 하면, boundary의 직교 벡터 $\boldsymbol{w}$ 을 활용할 때, 다음이 성립한다 $$ \begin{align} &amp;\mathcal{x}' = \alpha \boldsymbol{w} \ \Rightarrow \boldsymbol{w}^T \cdot \alpha \boldsymbol{w} + w_0 = 0 \ \Rightarrow \alpha = \frac {-w_0}{||\boldsymbol{w}||^2} \\ &amp; ||\mathcal{x}'|| = ||\alpha \boldsymbol{w}|| = \frac{-w_0}{||\boldsymbol{w}||} \end{align} $$ </font><p>binary class 이므로 boundary decision 을 기준으로 $y(\boldsymbol{x})$ 의 부호가 결정되고, 부호값을 통해 class를 판정하게 된다. 따라서 input X가 boundary를 기준으로 얼마나 떨어져 있는지에 대한 수직거리와 방향을 아는 것이 중요한데 이를 $\gamma \cdot \frac {\boldsymbol{w}}{||\boldsymbol{w}||}$라고 하자. 그 결과 아래와 같은 그림을 얻을 수 있다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 70% 70%'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/img/prml-4-1.png" width="70%" height="70%" title="1" alt="relative" /></p>\[\boldsymbol{x} = \boldsymbol{x}_{\bot} + \gamma \cdot \frac {\boldsymbol{w}}{||\boldsymbol{w}||}\\ \gamma = \frac{y(\boldsymbol{x})}{||\boldsymbol{w}||}\]<p>특히 $\gamma$의 부호 값이 중요하므로, 위와 같이 구하는 것 같다.</p><p>추가적으로 식을 간편하게 적기 위해dummy input을 활용해 bias를 파라미터로 함께 표현할 수 있다. $\boldsymbol{\tilde w} = (w_0, \boldsymbol{w})$ 에 대해, $y(\boldsymbol{x}) = \boldsymbol{\tilde w}^T \boldsymbol{\tilde x}$ 로 표기하면 hyperplane( boundary)는 원점을 통과하는 D - dimensional hyperplane 이다.</p><p><br /></p><h4 id="412-multiple-classes">4.1.2 Multiple Classes</h4><p>binary class에서 확장해서 multiple class 인 경우를 생각해보자.</p><p>기존 binary class에서 하던 방식을 그대로 사용할 경우 올바른 분류가 어려울 수 있는데 이유는 아래 그림을 보면 잘 나타난다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 70% 70%'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/img/prml-4-2.png" width="70%" height="70%" title="1" alt="relative" /></p><p>이를 피하기 위해서는 아래와 같이 단순하게 K-class discriminant 를 설정할 수 있다.</p>\[\begin{align} &amp;y_k(\boldsymbol{x}) = \boldsymbol{w}_k^T\boldsymbol{x} \ + \ w_{k0} \\ &amp;\textrm{assign x to} \ \ C_k \ \ \ \ \textrm{if} \ \ y_k(\boldsymbol{x}) &gt; y_j(\boldsymbol{x}) \ \textrm{for all } j \neq k \end{align}\]<p>binary class 와 유사하게 생각한다면 class k 와 class j 사이의 boundary는 $y_k(\boldsymbol{x}) = y_j(\boldsymbol{x})$ 를 만족해야하므로 아래와 같다.</p>\[(\boldsymbol{w}_k - \boldsymbol{w_j})^T \boldsymbol{x} + (w_{k0} - w_{j0}) = 0\]<p>또한 linear discriminant 에 의해 나눠진 region은 convex set 이다. (증명 생략)</p><p><br /></p><h4 id="413-least-squares-for-classification">4.1.3 Least squares for classification</h4><p>linear discriminant의 parameter 값을 찾기 위한 세 가지 방법을 제시한다.</p><p>첫번째 방법은 Least square 이다.</p><p>정규성을 가정한 선형회귀분석에서 가장 많이 쓰이는 방법으로 quadratic error를 사용하므로 예측값은 $E(\boldsymbol{t}|x)$이다</p><p>그러나 선형모델 자체가 flexibility가 부족하기 때문에 binary class인 경우에도 $E(\boldsymbol{t}|x)$ 값이 1을 초과할 수 있다.</p><p>multiple class에서도 Least square를 사용하기 위해 행렬을 사용해 식을 확장해보자.</p><p>개별 linear discriminant function $ y_k(\boldsymbol{x}) = \boldsymbol{w}<em>k^T \boldsymbol{x} \ + \ w</em>{k0} $ 에 대해 다음과 같이 나타낸다.</p>\[y(\boldsymbol{x}) = \boldsymbol{\tilde W}^T\boldsymbol{\tilde x}\]<p>이 때 $\boldsymbol{\tilde W}$ 는 k(class)개의 column을 가지고 있으며 열벡터가 $\boldsymbol{\tilde w}<em>k = (w</em>{k0}, \boldsymbol{w}_k ^T)^T$ 인 행렬이다.</p><p>Least square에 의해 $\boldsymbol{\hat {\tilde W}} = (\boldsymbol{\tilde X}^T\boldsymbol{\tilde X} )^{-1}\boldsymbol{\tilde X}^T \boldsymbol{T} = \boldsymbol{\tilde X}^{\dagger} \boldsymbol{T} \Rightarrow y(\boldsymbol{x}) = \boldsymbol{\hat {\tilde W} }^T \boldsymbol{\tilde x}$</p><p>이로부터 얻은 벡터 $y(\boldsymbol{x})$ 에 대해서 그 원소들 중 가장 큰 값의 class로 예측하게 된다. (제약식을 통해 y(x)의 sum이 1이 되게 하고 벡터의 각 원소들이 0과 1 사이가 되도록 하면 model output을 probabilistic 하게 해석할 수 있다.)</p><p>Least square는 closed form으로 해를 구할 수 있는 만큼 매우 간단하고 좋은 방법이지만 outlier에 민감하게 반응하며 가우시안을 가정한 경우에는 LS와 ML 방식의 결과가 동일한데, gaussian이 아닌 여러가지 분포들 대해서는 LS를 사용할 때 바람직한 결과를 얻지 못할 우려가 있다는 것도 알아둬야 한다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 70% 70%'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/img/prml-4-4.png" width="70%" height="70%" title="1" alt="relative" /></p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 70% 70%'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/img/prml-4-5.png" width="70%" height="70%" title="1" alt="relative" /></p><p><br /></p><h4 id="414-fishers-linear-discriminant">4.1.4 Fisher’s linear discriminant</h4><p>linear discriminant의 parameter 값을 찾기 위한 두 번째 방법은 Fisher’s linear discriminant 이다. 이 방법의 핵심은 바로 차원 축소에 있다.</p><p>X : D-dimension $\Rightarrow $ one-dimension : $y = \boldsymbol{w}^T \boldsymbol{x}$</p><p>차원 축소는 본질적으로 정보의 손실을 막을 수 없으며, 본래 D 차원의 공간이 1차원(line)으로 축소되며 overlapping이 발생할 수밖에 없다. 이를 보완하기 위해 집단 간 차이는 최대로 하되, 집단 내부의 분산은 최소화되도록 하는 projection을 선택해야 한다.</p><font color="green"> * 집단 간 차이 최대 적절한 weight vector $\boldsymbol{w}$를 잘 선택해서 class separation을 최대로 하는 projection을 선택한다면 정보의 손실을 어느정도 보완할 수 있다. 즉, 볼드체의 m 이 분류된 집단의 평균값이 라 할 때 $$ m_2 - m_1 = \boldsymbol{w}^T(\boldsymbol{m}_2 - \boldsymbol{m}_1) \ \ \ \ \ \ \textrm{where} \ m_k = \boldsymbol{w}^T \boldsymbol{m_k} \ \ \ \ \textrm{(mean of projected data from class)} $$ 값을 최대화하는 길이 1의 벡터인 $\boldsymbol{w}$ 를 찾으면 된다 (길이를 1로 둠으로써 $m_2-m_1$이 무한정 커지진 않도록 제약을 준다) Lagrange multiplier를 사용하면 다음과 같은 결과를 얻는다 $$ \boldsymbol{w} \propto (\boldsymbol{m}_2 - \boldsymbol{m}_1) $$ * 집단 내 분산 최소 within class variance of transformed data from class $C_k$는 다음과 같이 정의한다. $$ s_k^2 = \sum_{n \in C_k} (y_n - m_k)^2 \ \ \ \ \textrm{where} \ y_n = \boldsymbol{w}^T\boldsymbol{x_n} $$ 이 때 binary classification 의 경우 total within class variance는 $s_1^2 + s_2^2$ </font><p><br /></p><p>이를 만족하는 projection vector w를 찾기 위해 Fisher는 다음과 같은 기준을 제시했다.</p>\[\begin{align} &amp;\mathcal{J}(\boldsymbol{w}) = \frac{(m_2 - m_1)^2}{s_1^2 + s_2^2} = \frac{\boldsymbol{w}^T \boldsymbol{S}_B \boldsymbol{w}}{\boldsymbol{w}^T \boldsymbol{S}_W\boldsymbol{w}} \\ \\ &amp;S_B = (\boldsymbol{m}_2 - \boldsymbol{m}_1)(\boldsymbol{m}_2 - \boldsymbol{m}_1)^T\\ &amp;S_W = \sum (\boldsymbol{x}_n - \boldsymbol{m}_1)(\boldsymbol{x}_n - \boldsymbol{m}_1)^T + \sum (\boldsymbol{x}_n - \boldsymbol{m}_2)(\boldsymbol{x}_n - \boldsymbol{m}_2)^T \end{align}\]<p>이 때 J 를 w 로 미분하면 $\mathcal{J}(\boldsymbol{w})$는 $(\boldsymbol{w}^T \boldsymbol{S}_B \boldsymbol{w})\boldsymbol{S}_W\boldsymbol{w} = (\boldsymbol{w}^T \boldsymbol{S}_W\boldsymbol{w})\boldsymbol{S}_B \boldsymbol{w}$ 를 만족할 때 최대가 됨을 알 수 있다. $S_B$ 식으로부터 $S_B\boldsymbol{w}$ 는 $\boldsymbol{m}_2 - \boldsymbol{m}_1$ 와 비례함을 알 수 있으므로 $\boldsymbol{w} \propto S_W^{-1}(\boldsymbol{m}_2 - \boldsymbol{m}_1)$ 이다. 이를 만족하는 $\boldsymbol{w}$를 Fisher linear discriminant 라고 한다. (사실 discriminant 라기 보다는 projection direction을 의미한다고 말하는게 더욱 맞다)</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 70% 70%'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/img/prml-4-6.png" width="70%" height="70%" title="1" alt="relative" /></p><p><br /></p><h4 id="415-relation-to-least-squares">4.1.5 Relation to least squares</h4><ul><li><p>Least -Square : target value를 최대한 정확하게 맞출 수 있을지(error를 최소화)</p><li><p>Fisher-criterion : target의 class를 정확하게 분류할 수 있을지(maximum class separation)</p></ul><p>그러나 1 - of -K coding (K class 중 가장 높은 확률값을 가지는 class 를 선택)이 아닌 다른 target coding scheme을 사용할 때, Least - square 와 Fisher solution은 동일하게 생각할 수 있다. 아래는 그 방법을 나타낸 것이다.</p><font size="4"> [different target coding scheme for binary] </font><p>target for class $C_1$ to be $N/N_1$, target for class $C_2$ to be $- N/N_2$</p><p>라고 두면 sum of squares error function은 다음과 같다</p>\[E = \frac{1}{2} \sum_{n=1}^N (\boldsymbol{w}^T \boldsymbol{x}_n + w_0 - t_n ) ^2\]<p>이를 $\boldsymbol{w}$ 와 $w_0$로 미분하면</p>\[w_0 = -\boldsymbol{w}^T \boldsymbol{m} \\ (S_W + \frac{N_1 N_2}{N}S_B)\boldsymbol{w} = N(\boldsymbol{m}_1 - \boldsymbol{m}_2)\]<p>이므로 $S_B \boldsymbol{w}$가 $\boldsymbol{m}_2 - \boldsymbol{m}_1$의 방향을 나타내는 것을 고려할 때, 결국 Least-square에서도 Fisher - criterion에서 얻은 $\boldsymbol{w} \propto S_W^{-1}(\boldsymbol{m}_2 - \boldsymbol{m}_1)$ 식을 얻게 됨을 알 수 있다.</p><p><br /></p><h4 id="416-fishers-discriminant-for-multiple-classes">4.1.6 Fisher’s discriminant for multiple classes</h4><p>앞서 Fisher-criterion을 설명할 때는 binary class를 기반으로 input space 차원 축소를 1차원으로 했다면 multiple class 의 경우 inpute space 차원 축소를 D’ 차원으로 해 볼 것이다. multiple class 이므로 weight vector $\boldsymbol{w}$ 를 column으로 하는 행렬 $\boldsymbol{W}$를 생각하자. 즉 모델은 아래와 같다.</p>\[y =\boldsymbol{W}^T \boldsymbol{x}\]<p>within-class covariance matrix to the case of K classes</p>\[S_W = \sum ^K S_k \ \ \textrm{where} \ \ S_k = \sum (\boldsymbol{x}_n - \boldsymbol{m}_k)(\boldsymbol{x}_n - \boldsymbol{m}_k)^T \ \ \&amp; \ \ \boldsymbol{m}_k = \frac{1}{N_k}\sum_{n \in C_k}\boldsymbol{x_n}\]<p>Total covariance matrix</p>\[S_T = \sum_{n=1}^N (\boldsymbol{x}_n - \boldsymbol{m})(\boldsymbol{x}_n - \boldsymbol{m})^T\]<p>Between class covariance</p>\[S_B = \sum ^K N_k(\boldsymbol{m}_k - \boldsymbol{m})(\boldsymbol{m}_k - \boldsymbol{m})^T\]<p>세가지 공분산 행렬은 $S_T = S_B + S_W$ 관계를 만족한다.</p><p>D’ 차원으로 projection한 데이터에 대해서도 유사한 공분산 행렬(소문자 s 사용)을 정의할 수 있다.</p><p>binary class와 유사하게 Fisher-criterion $\mathcal{J}(\boldsymbol{W}) = Tr(s_W^{-1}s_B)$ 로 구할 수 있고 projection 이전의 데이터를 사용할 때 $\mathcal{J}(\boldsymbol{w}) = (\boldsymbol{W}^T \boldsymbol{S}_W \boldsymbol{W})^{-1} \boldsymbol{W}^T \boldsymbol{S}_B\boldsymbol{W}$ 이다.</p><p>즉 weight vlaue ($\boldsymbol{W}$)는 $S_W^{-1}S_B$ 의 eigenvector에 의해 결정되고 ??</p><p><br /></p><h4 id="417-the-perceptron-algorithm">4.1.7 The perceptron algorithm</h4><p>…</p><p><br /></p><h3 id="44-the-laplace-approximation">4.4 The Laplace Approximation</h3><p>Bayesian treatment of logistic regression 에서는 $p(C_k | \boldsymbol{x})$가 더이상 Gaussian이 아니므로 적분이 어렵다. 따라서 Laplace apporximation을 통해서 확률밀도에 대한 Gaussian approximation을 하려고 한다.</p><p>알고 싶은 확률밀도 $p(z)$ 에 대해 $f(z)$를 활용해 근사한다고 하자.</p>\[p(z) = \frac {1}{Z}f(z)\]<p>step 1 : $p(z)$ 의 mode 값 $z_0$을 찾는다</p><p>step 2 : $ln f(z)$를 mode인 $z_0$에 대해 Talyor expansion 한다</p>\[ln f(z) \ \simeq \ ln f(z_0) - \frac{1}{2}(z - z_0)^TA(z - z_0) \\ f(z) \ \simeq \ f(z_0)exp\{ - \frac{1}{2}(z - z_0)^T A (z- z_0) \} \\ \textrm{where} \ \ A = - \nabla \nabla ln f(z) |_{z=z_0}\]<p>step 3 : Gaussian 에 근거해서 normalized dist q(z) 로 근사한다</p>\[q(z) = (\frac{|A|^{1/2}}{(2\pi)^{M/2}}) exp\{ - \frac{1}{2}(z - z_0)^T A (z- z_0) \}\]<p>(단, precision matrix인 A는 positive definite 이어야 한다.)</p><p>데이터가 많아지면 Gaussian으로의 근사가 더욱 효과적일 것이다(CLT)</p><p>그러나 1. multimodal 인 경우 위의 Laplace apporximation은 한계가 있다. 2. 또한, Gaussian을 사용해서 근사를 하기 때문에 real variable로 정의된 함수에 대해서만 근사가 가능하다(즉 양수인 경우 로그변환이 필요). 3. 무엇보다 distribution에만 집중하여 global properties를 간과해선 안된다.(10장 variational inference and regression 부분에서 이를 자세히 배울 것)</p><p><br /></p><h4 id="441-model-comparison-and-bic">4.4.1 Model comparison and BIC</h4><p>$p(z)$를 근사하기 위해서는 normalization constant Z에 대해서도 알아야한다. Gaussian approximation $f(z)$에 대해서 다음이 성립한다.</p>\[\begin{align} Z &amp; = \int f(z) dz \\ &amp; \simeq f(z_0)\int exp\{ - \frac{1}{2}(z - z_0)^T A (z- z_0) \} dz \\ &amp; = f(z_0) \frac{(2\pi)^{M/2}}{|A|^{1/2}} \textrm{----- * } \end{align}\]<p>또한 모델의 집합 ${ \mathcal{M}_i }$ 에 대해 각 모델의 parameter를 ${ \theta_i }$ 라고 할 때 model evidence $p(\mathcal{D} | \mathcal{M}_i)$ 는 다음과 같이 쓸 수 있다.</p>\[p(\mathcal{D}) = \int p(\mathcal{D}|\theta) p(\theta) d\theta = \int f(\theta) d\theta\]<p>normalization constant 근사식(*)을 활용할 때, 위 식을 다시 써보자.</p>\[p(\mathcal{D}) = f(\theta_{MAP})\frac{(2\pi)^{M/2}}{|A|^{1/2}} = p(\mathcal{D}|\theta_{MAP}) p(\theta_{MAP}) \frac{(2\pi)^{M/2}}{|A|^{1/2}} \\ ln \ P(\mathcal{D}) \simeq ln \ p(\mathcal{D}|\theta_{MAP}) + \ln p(\theta_{MAP}) + \frac{M}{2} ln \ 2\pi - \frac{1}{2} ln |A|\]<p>첫 번째 term은 log-likelihood 이고 두 번째 term 부터는 model complexity term이다. 그리고 matrix A는 posterior $p(C_k | \boldsymbol{x})$에 대한 Hessian이다.</p><p>이 때, Gaussian prior의 분산을 매우 크게 하면 상수처럼 취급될 수 있으므로 positive definte matrix A 에 대해 다음이 성립한다.</p>\[ln \ P(\mathcal{D}) \simeq ln \ p(\mathcal{D}|\theta_{MAP}) - \frac{M}{2} ln \ N + C\]<p>즉 이는 model selection에서 지표로 사용하는 BIC와 동일한 form임을 알 수 있다.(사실 $-2 ln \ p(\mathcal{D}$) 가 BIC 와 form이 완벽히 일치한다.)</p><p><br /></p><h3 id="45-bayesian-logisitic-regression">4.5 Bayesian Logisitic Regression</h3><p>앞서 설명한대로 sigmoid 함수에 대한 적분이 쉽지 않기 때문에, logistic regression 에 대한 Bayesian inference를 위해 새로운 접근법이 필요하다 $Rightarrow$ Laplace approximation</p><p><br /></p><h4 id="451-laplace-approximation">4.5.1 Laplace approximation</h4><p>posterior에 대한 Gaussian representation을 행할 것이므로 prior를 Gaussain으로 생각해보자.</p>\[p(w) = \mathcal{N}(w|m_0, S_0)\]<p>이때 posterior는 다음과 같다.</p>\[\begin{align} &amp;p(w|\boldsymbol{t}) \propto p(w)p(\boldsymbol{t}|w) \\ &amp;ln \ p(w|\boldsymbol{t}) = -\frac{1}{2}(w-m_0)^T S_0 ^{-1}(w-m_0) + \sum {t_n ln \ y_n + (1-t_n) ln \ (1-y_n)} + C \\ &amp;\textrm{where} \ \ \ \ p(\boldsymbol{t}|w) = \prod y_n^{t_n} \{1- y_n \}^{1-t_n} \end{align}\]<p>Laplace approximation의 논리와 동일하게 MAP 값으로 Taylor expansion 하면 posterior 에 대한 Gaussian 근사는 다음과 같다</p>\[q(w) = \mathcal{N}(w|w_{MAP}, S_N) \\ \textrm{where} \ \ S_N = - \nabla \nabla ln \ p(w| \boldsymbol{t})\]<p><br /></p><h4 id="452-predictive-distribution">4.5.2 Predictive distribution</h4><p>이제 predictive distribution을 생각해보자. 새로운 feature vector $\phi(x)$ 가 주어졌을 때 class $\mathcal{C_1}$ 에 대한 predictive distribution은 다음과 같이 정의 및 근사 된다.</p>\[p(\mathcal{C}_1|\phi, \boldsymbol{t}) = \int p(\mathcal{C}_1|\phi, w) \ p(w|\boldsymbol{t}) dw \simeq \int \sigma(w^T \phi) \ q(w) dw\]<p>이때 Gaussian의 marginal dist 또한 Gaussian이므로 $w$ 를 marginalized out 시킨 $p(a) = \int \delta(a - w^T\phi) q(w) dw$ 를 활용해서 아래와 같이 적을 수 있다.</p>\[p(\mathcal{C}_1|\boldsymbol{t}) \simeq \int \sigma(a) p(a) \ da = \int \sigma(a) \mathcal{N}(a|\mu_a, \sigma_a^2) \ da\]<p>첫번째 근사는 미분방정식을 활용해서, 두번째 등식은 marginal dist $p(a)$가 Gaussian의 marginal임을 활용해서 전개한 것이다.</p><p>위 식은 Gaussian과 sigmoid의 convolution을 나타낸 것이므로 계산하기가 까다로우나, sigomid function 대신 probit function을 사용하면 analytic한 결과를 얻을 수 있다.</p>\[p(\mathcal{C}_1|\boldsymbol{t}) \simeq \int \Phi(\lambda a) \ \mathcal{N}(a|\mu_a, \sigma_a^2) \ da \simeq \sigma(\kappa(\sigma_a^2)\mu_a)\]<p>“Marginalization of the logistic sigmoid model under a Gaussian approximation to the posterior dist will be illustrated in the context of variational inference in Fig 10.13”</p><p><br /></p></div><div class="post-tail-wrapper text-muted"><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/prml/" class="post-tag no-text-decoration" >PRML</a> <a href="/tags/machine-learning/" class="post-tag no-text-decoration" >Machine learning</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=PRML-4 - For Statistics&url=https://hwankam.github.io/posts/PRML4/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=PRML-4 - For Statistics&u=https://hwankam.github.io/posts/PRML4/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=PRML-4 - For Statistics&url=https://hwankam.github.io/posts/PRML4/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/PRML6/">PRML-6</a><li><a href="/posts/PRML5/">PRML-5</a><li><a href="/posts/PRML4/">PRML-4</a><li><a href="/posts/PRML3/">PRML-3</a><li><a href="/posts/CASI9/">Computer Age Statistical Inference - chap 9</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/datascience/">datascience</a> <a class="post-tag" href="/tags/datamining/">datamining</a> <a class="post-tag" href="/tags/machinelearning/">machinelearning</a> <a class="post-tag" href="/tags/islr/">ISLR</a> <a class="post-tag" href="/tags/statistical-method/">statistical method</a> <a class="post-tag" href="/tags/efron/">Efron</a> <a class="post-tag" href="/tags/prml/">PRML</a> <a class="post-tag" href="/tags/machine-learning/">machine learning</a> <a class="post-tag" href="/tags/machine-learning/">Machine learning</a> <a class="post-tag" href="/tags/categorical-data/">categorical data</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/PRML3/"><div class="card-body"> <span class="timeago small" >Nov 7, 2021<i class="unloaded">2021-11-07T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>PRML-3</h3><div class="text-muted small"><p> 3. Linear Models for Regression linear model : parameter에 대한 linear function ( input variables의 nonlinear function이 있다하더라도 이들의 결합이 파라미터 관점에서 선형결합이면 linear model이라 칭한다 ) 이 장에서는 선형모형과 이를 통한 학습 및 예측...</p></div></div></a></div><div class="card"> <a href="/posts/PRML5/"><div class="card-body"> <span class="timeago small" >Feb 9<i class="unloaded">2022-02-09T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>PRML-5</h3><div class="text-muted small"><p> 5. Neural Networks chap 3과 4에서는 linear combination of fixed bases function에 대해 배웠다. 그러나 high dimension에서는 과적합의 문제가 발생하는 한계가 있었다. 이를 해결하기 위해 데이터에 맞게 basis function을 바꾸는 것은 어떠할까? SVM은 데이터에 맞게 hyper...</p></div></div></a></div><div class="card"> <a href="/posts/PRML6/"><div class="card-body"> <span class="timeago small" >Mar 1<i class="unloaded">2022-03-01T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>PRML-6</h3><div class="text-muted small"><p> 6. Kernel Methods [서론] there is a class of pattern recognition techniques ==&gt; training data points are kept, memory based method ex ) nearest neighborhood 방법은 training data와 가장 유사한 label을 te...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/CASI15/" class="btn btn-outline-primary" prompt="Older"><p>Computer Age Statistical Inference - chap 15</p></a> <a href="/posts/PRML5/" class="btn btn-outline-primary" prompt="Newer"><p>PRML-5</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://twitter.com/username">your_full_name</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/datascience/">datascience</a> <a class="post-tag" href="/tags/datamining/">datamining</a> <a class="post-tag" href="/tags/machinelearning/">machinelearning</a> <a class="post-tag" href="/tags/islr/">ISLR</a> <a class="post-tag" href="/tags/statistical-method/">statistical method</a> <a class="post-tag" href="/tags/efron/">Efron</a> <a class="post-tag" href="/tags/prml/">PRML</a> <a class="post-tag" href="/tags/machine-learning/">machine learning</a> <a class="post-tag" href="/tags/machine-learning/">Machine learning</a> <a class="post-tag" href="/tags/categorical-data/">categorical data</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}}); </script> MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
