<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.2.1" /><meta property="og:title" content="PRML-6" /><meta property="og:locale" content="en" /><meta name="description" content="6. Kernel Methods" /><meta property="og:description" content="6. Kernel Methods" /><link rel="canonical" href="https://hwankam.github.io/posts/PRML6/" /><meta property="og:url" content="https://hwankam.github.io/posts/PRML6/" /><meta property="og:site_name" content="For Statistics" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-03-01T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="PRML-6" /><meta name="twitter:site" content="@twitter_username" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"description":"6. Kernel Methods","url":"https://hwankam.github.io/posts/PRML6/","headline":"PRML-6","dateModified":"2022-03-17T21:57:15+09:00","datePublished":"2022-03-01T00:00:00+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://hwankam.github.io/posts/PRML6/"},"@type":"BlogPosting","@context":"https://schema.org"}</script><title>PRML-6 | For Statistics</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="For Statistics"><meta name="application-name" content="For Statistics"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> </a></div><div class="site-title mt-3"> <a href="/">For Statistics</a></div><div class="site-subtitle font-italic">kam's world</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/Hwankam" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['example','doamin.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>PRML-6</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>PRML-6</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> your_full_name </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Tue, Mar 1, 2022, 12:00 AM +0900" >Mar 1<i class="unloaded">2022-03-01T00:00:00+09:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Thu, Mar 17, 2022, 9:57 PM +0900" >Mar 17<i class="unloaded">2022-03-17T21:57:15+09:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3855 words">21 min read</span></div></div><div class="post-content"><h2 id="6-kernel-methods">6. Kernel Methods</h2><p>[서론]</p><p>there is a class of pattern recognition techniques</p><p>==&gt; training data points are kept, memory based method</p><p>ex ) nearest neighborhood 방법은 training data와 가장 유사한 label을 test data에서 선택함. 즉 여기서 training data 가 storing 된다는 것은 모델 내부에서 파라미터를 정하는 학습이 아니라는 뜻. 즉, 모델을 달라고한다면 선형회귀모델 같은 경우에는 파라미터 계수를 주면 되겠지만, NN의 경우에는 데이터 자체를 줘야한다.</p><h5 id="kerenl-function">kerenl function</h5><p>$\ \ $ : $k(x, x’) = \phi(x)^T \phi(x’)$</p><p>특징 : symmetric</p><p>종류</p><ul><li><p>identity kernel(linear kernel) : $k(x, x’) = x^T x’$</p><li><p>stationary kernel : $k(x, x’) = k(x - x’) $ , 오직 차이에만 의존하므로 translation invarinace 를 갖는다.</p><li><p>homogeneous kernel(radial basis kernel) : $k(x, x’) = k(||x - x’||) $ , 오직 거리의 크기에만 의존</p></ul><p>large margin classifier로 머신러닝에서의 kernel function 중요성이 재조명됨</p><p>“extenstion of kernels to handle symbolic objects” : 다양한 자료를 kernel을 통해 분석할 수 있게 됨</p><p><br /></p><h3 id="61-dual-representations">6.1 Dual Representations</h3><p>Many linear models can be reformulated in terms of a dual representation =&gt; kernel function</p><p>예측함수를 $w^t \phi(x)$ 라고 할 때, regularized sum of squares error function을 고려하면 이를 최소화하는 벡터 w는 $\phi(x)$ 의 linear combination으로 표현 가능하다.</p><p>즉 dual formulation을 통해서 최소제곱의 문제를 kernel function을 통해 표현 가능하다.</p><p>gram matrix $ K = \boldsymbol{\Phi} \boldsymbol{\Phi}^T $ 에 대해서 error function을 다음과 같이 적을 수 있다</p>\[\begin{align} &amp;J(a) = \frac{1}{2} a ^T \boldsymbol{K}^T\boldsymbol{K}a - a^T \boldsymbol{K} \boldsymbol{t} + \frac{1}{2} \boldsymbol{t}^t\boldsymbol{t} + \frac{\lambda}{2} a^T \boldsymbol{K} a \\ &amp;\textrm{where} \ \ \ \ w = \boldsymbol{\Phi}^T a \ \ \ \ \&amp; \ \ \ \ a_n = \frac {-1}{\lambda} \{ w^T \phi(x_n) - t_n \} \end{align}\]<p>error function의 gradient 를 0으로 하는 a를 구해 prediction을 구하면</p>\[y(x) = w^T \boldsymbol{\phi}(x) = a^T \boldsymbol{\Phi} \ \boldsymbol{\phi}(x)\]<p>즉, w를 $\phi$의 linear combination으로 구하게 되면, 최소제곱의 문제를 커널을 통해 해결할 수 있게 되는 것이다.</p><p>파라미터 w 에서 이를 변환한 a 를 고려하게 되면 일반적으로 차원은 더 커지게 된다. 그러나 kernel function $k(x, x’)$을 통해서 온전히 식을 표시할 수 있으며 feature vector $\phi(x)$가 무엇인지 명확하게 제시할 필요도 없어진다. =&gt; high dimension을 다룰 수 있다.</p><p><br /></p><h3 id="62-constructing-kernels">6.2 Constructing Kernels</h3><p>construct valid kernel function이 중요하다</p><p>첫번째 방법으로는 mapping $\phi(x)$를 설정한 이후에 내적을 통해 kernel function을 찾는 것이 있고, 두번째 방법으로는 kernel fucntion을 directly 찾는 것이 있다. 두번째 방법에서는 내가 정한 kernel function이 정말 유효한 kernel function인지 찾는 것이 중요하다. 이때 gram matrix(각 원소가 kernel $\phi(x)$의 내적) positive semi - definite이라는 조건만 있으면 kernel function이 되는 필요충분조건을 만족한다.</p><p>따라서 기본 형태의 kernel fucntion을 찾은 뒤 building block의 형태로 kernel을 만들어나간다.</p><p>gaussian kernel : $k(x, x’) = exp(-|| x - x’ || ^2 / 2\sigma^2) = exp(-x^tx / 2\sigma^2) exp(x^tx/\sigma^2) exp(-(x’)^tx’ / 2\sigma^2 )$</p><p>gaussian kernel은 결국 linear kernel인 $k(x, x’) = x^t x’$를 block으로 해서 만든 것이므로 kernel function이 됨을 알 수 있다.</p><p><br /></p><p>kernel 종류</p><ul><li><p>discriminant model과 generative model 섞기 : similarity measure</p><p>use generative model to define a kernel and then use this kernel in a discriminative approach.</p><p>ex ) Fisher kerenl (관련 논문 : http://www.vision.caltech.edu/publications/holubWellingPerona-FisherICCV05.pdf)</p><p>$\dot l(\theta) \sim \mathcal{N}(0 , n I(\theta)) $ 과 연관되어 있지 않을까</p><li><p>sigmoidal kernel 실제로 많이 이용되는 커널. SVM을 neural network와 연결짓는 kernel</p><p>“in the limit of an infinite number of basis functions, a Bayesian neural network with an appropriate prior reduces to a Gaussian process” <br /></p></ul><h3 id="63-radial-basis-function-networks">6.3 Radial Basis Function Networks</h3><p>앞 장에서부터 basis function 을 활용한 linear regression 과 classification 에 대해 이야기했다. 대표적인 basis function으로는 radial basis function이 있다. radial basis function은 Euclidean distance를 기반으로 한 basis function이다.</p><p>[Interpolation - RBF 배경]</p><p>본래 radial basis function은 exact function interpolation을 목적으로 만들어졌다. 일례로, input과 target을 정확하게 fitting 하는 smooth function을 만들기 위해서 radial basis function의 linear combination을 만드는 과정을 생각해볼 수 있다.</p>\[f(x) = \sum_{n=1} w_n h(||x-x_n||)\]<p>여기서 $w_n$은 least square를 통해 얻을 수 있다.</p><p>그러나 exact interpolation은 overfitting의 문제가 있다.</p><p>input이 noisy 한 경우 interpolation을 고려하는 경우에도 Euclidean distance에 기반한 basis function을 고려할 수 있다.</p>\[E = \frac{1}{2} \sum_{n=1} \int \{y(x_n + \xi) - t_n \}^2 v(\xi) d \xi\]<p>이 때 optimization은</p>\[y(x_n) = \sum t_n h(x-x_n) \ \ \ \ \ \ \textrm{where} \ \ \ h(x- x_n) = \frac {v(x-x_n)}{\sum v(x - x_n)}\]<p><br /></p><p>이때 basis function h 는 normalized 되어있으므로, 이를 통해 모든 basis function이 작은 값을 갖게 되는 경우를 방지할 수 있다. 여기서 쓰인 basis function을 Nadaraya-Watson model이라고 한다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 70% 70%'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/img/prml-6-3.png" width="70%" height="70%" title="1" alt="relative" /></p><p><br /></p><p>[Kernel density estimation - RBF 확장]</p><p>normalized radial basis function의 확장으로, regression 문제에서의 kernel density estimation이 있다. 여기서는 computing cost를 절감하기 위한 방안으로써, data point 보다 더 작은 개수의 basis function을 setting 하는 법이 있음을 말하고 있다.</p><p>일반적으로 basis function의 개수와 center에 대한 location $\mu_i$ 는 data point에 의해 정해지며 그 형태는 사전적이다. 또한 계수 $w_i$ 는 최소제곱을 통해 정해진다. 이 때에 basis function의 개수를 data point 개수보다 줄이기 위해서는 basis function의 center를 순차적으로 정하되, sum-of-square error를 최대한으로 줄이는 data point에 상응하는 center로 선택하는 sequential selection process가 있다. 이 방식은 통상적으로 orthogonal least square 라고 불린다.</p><h6 id="orthogonal-least-square">orthogonal least square</h6><p>참고논문 : orthogonal least square regression : a unifed apporoach for data modeling</p><p>sparse kernel data modeling을 위해 kernel matrix(feature)를 QR 분해해서 orthogonoalize</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 70% 70%'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/img/prml-6-1.png" width="70%" height="70%" title="1" alt="relative" /></p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 70% 70%'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/img/prml-6-2.png" width="70%" height="70%" title="1" alt="relative" /></p><h4 id="631-nadaraya-watson-model">6.3.1 Nadaraya-Watson model</h4><p>(3.62)에서는 equivalent kernel $k(x, x’) = \beta \phi(x)^T S_N \phi(x’)$ 를 제시하며 predictive distribution mean이 training data의 target $t_n$의 선형결합으로 나타내어짐을 보였다.</p><p>이제 kernel density estimation의 관점에서 이를 살펴보자.</p><p>joint distribution $p(x,t)$ 를 추정하기 위해 Parzen density estimator를 활용해보자.</p>\[p(x,t) = \frac {1}{N} \sum_{n=1} ^N f(x - x_n, t- t_n)\]<p>여기서 Paren density estimator는 kernel density estimation을 위한 방법 중 하나이다. 아래는 이에 대한 간단한 설명이다.</p><font color="green"> (참고 링크) [https://jayhey.github.io/novelty%20detection/2017/11/08/Novelty_detection_Kernel/] 1장에서도 짧게 배웠듯이, $P(x) \simeq \frac {K}{N V}$ 로 적을 수 있고, V를 고정시키고 K를 구하는 것은 kernel density estimator와 같이 생각할 수 있다.(반대로 K를 고정시키고 V를 찾는 것은 K-nearest neighborhood 라고 할 수 있다) 이를 활용해 d-dimension에서 $V = h^d$ 라고 할 때 x, t 를 기준으로 각 차원으로 h/2 안에 존재하는 데이터의 개수를 세어 밀도를 추정한다고 할 때 $$ p(x, t) =\frac {1}{N h^d} \sum_{n=1} ^N f(\frac{x - x_n}{h}, \frac{t- t_n}{h}) $$ 위의 식으로 밀도추정이 가능하다. </font><p>이제 target variable 에 대한 conditional expectation을 구하는 것에 대해 생각해보자.</p>\[\begin{align} y(x) &amp; = E[t|x] = \int t p(t|x) dt \\ &amp; = \frac{\int t p(x,t) dt }{\int p(x,t) dt} \\ &amp; = \frac {\sum_n \int t f(x-x_n, t- t_n) dt }{\sum _m\int f(x-x_m, t-t_m) dt} \ \ \ \ \cdots \textrm{using Parzen density estimation } \end{align}\]<p>component f에 대한 zero mean을 가정하고, $g(x) = \int f(x,t) dt$ 라고 정의하면</p>\[\begin{align} y(x) &amp; = \frac {\sum_n g(x-x_n) t_n}{\sum_m g(x-x_m)} \\ &amp; = \sum k(x,x_n) t_n \end{align}\]<p>위 식의 마지막 결과를 보면 앞에서 Bayesian regresssion에서 보았던 predictive distribution mean과 비슷한 형태를 가지고 있음을 알 수 있다.</p><p><br /></p><h3 id="64-gaussian-process">6.4 Gaussian Process</h3><p>kernel을 사용해 probablistic discriminative model로 확장해보자.(6.1은 non-probablistic model) 이 장에서는 kernel이 Bayesian setting과 어떻게 연관되는지 보여준다.</p><p><br /></p><h4 id="641-linear-regression-revisited">6.4.1 Linear regression revisited</h4><p>[linear model에서 gaussian process가 어떻게 정의되는지]</p><p>다시 linear regression 문제로 돌아가서, predictive distirbution을 도출해내는 것을 생각해보자. (predictive dist 는 다음 절에서 나오는 듯?)</p><p>basis function의 선형결합으로 구성된 모델을 생각해보자.</p>\[y(x,w) = w^T \phi(x)\]<p>이때 w 의 Gaussian prior를 다음과 같이 정의한다.</p>\[p(w) = \mathcal{N}(w |0, \alpha^{-1} I)\]<p>training data point x 에 대한 joint distribution은 다음과 같다.</p>\[\boldsymbol{Y} = \boldsymbol{\Phi} w\]<p>$\boldsymbol{Y}$ 는 w 에 대한 linear combination이므로, 그 자체로 이미 Gaussian 이다. 그리고 평균과 분산은 다음과 같다.</p>\[\begin{align} &amp; E[\boldsymbol{Y}] = \Phi E[w] = 0 \\ &amp; cov[\boldsymbol{Y}] = E[\boldsymbol{Y} \boldsymbol{Y}^T] = \Phi E[w w^T] \Phi^T = \frac{1}{\alpha} \Phi \Phi^T \\ &amp; \ \ \ \ \ \ \ \ \ \ \ \ = K \textrm{(gram matrix)} \end{align}\]<p>이 모델(linear model)은 Gaussian process 의 특별한 모델로서 일반적으로 Gaussian process 는 joint distribution $\boldsymbol{Y}(x)$ 로 정의된다.</p><p>Gaussian 이므로 평균과 분산은 second order statistic (second moment) 가 중요하고, w에 대한 prior의 평균을 0으로 잡을 때는, covariance matrix 인 gram matrix가 중요하다. Gaussian process 에서도 kernel을 직접 정할 수 있으며 아래 그림은 GP에서 각각 Gaussian kernel과 exponential kernel을 설정해 densitiy estimation을 수행한 결과이다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 70% 70%'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/img/prml-6-4.png" width="70%" height="70%" title="1" alt="relative" /></p><p><br /></p><h4 id="642-gaussian-processes-for-regression">6.4.2 Gaussian processes for regression</h4><p>Gaussian process 문제를 regression setting에 적용하기 위해서는 noise 개념을 도입해야 한다.</p>\[t_n = y(x_n) + \epsilon_n\]<p>Gaussian 분포를 갖는 noise process를 생각하면</p>\[p(t_n | y_n ) = \mathcal{N}(t_n | y(x_n), \beta ^{-1} )\]<p>noise는 모두 독립이므로, $y(x_n)$의 값이 모두 조건으로 주어졌을 때 target value t에 대한 joint distribution은</p>\[p(\boldsymbol{t} | \boldsymbol{y} ) = \mathcal{N}(\boldsymbol{t} | \boldsymbol{y}, \beta ^{-1} \boldsymbol{I}_N )\]<p>그리고 $\boldsymbol{y}$가 Gaussian Process 를 따른다고하면 정의에 의해 marginal distribution 또한 Gaussian을 따른다.</p>\[p(\boldsymbol{y}) = \mathcal{N}(\boldsymbol{y} | 0, \boldsymbol{K})\]<p>여기서 K는 gram matrix이며 이는 각 원소가 kernel의 내적으로 정의된 함수이다.</p><p>이때 t에 대한 marginal distribution을 구하면</p>\[p(\boldsymbol{t}) = \int p(\boldsymbol{t}|\boldsymbol{y}) p(\boldsymbol{y}) d\boldsymbol{y} = \mathcal{N} ( \boldsymbol{t} | 0, \boldsymbol{C} )\]<p>Gaussian process regression에서 가장 자주 쓰이는 kernel function은 exponential of quadratic form으로 아래와 같다.</p>\[k(x_n, x_m) = \theta_0 exp \{ - \frac{\theta_1}{2} ||x_n - x_m || ^2\} + \theta_2 + \theta_3 x_n^Tx_m\]<p>여기서 주목할 만한 점은 last term이 input variable x에 대한 linear function이라는 것이다.</p><p>이제 regression에 초점을 맞춰보자. 결국 하고 싶은 것은 prediction이므로 new input $x_{N+1}$에 대한 target $t_{N+1}$ 을 예측하고 싶다. 즉 $p(t_{N+1} | \boldsymbol{t}_N)$ 을 구하고 싶다.</p><p>N+1개의 target에 대한 joint distribution은 위에서 구한 marginal distribution을 활용해서 아래 결과를 알 수 있다.</p>\[p(\boldsymbol{t}_{N+1}) = \mathcal{N} ( \boldsymbol{t}_{N+1} | 0, \boldsymbol{C}_{N+1} )\]<p>이때 $\boldsymbol{C}_{N+1}$에 대한 partition matrix를 다음과 같이 정의한다고 하자.</p>\[C= ( \begin{array}{c c} \boldsymbol{C}_N &amp; k \\ k^T &amp; c \end{array} )\]<p>이 때의 predictive distribution의 평균과 분산은 아래와 같다.</p>\[\begin{align} &amp; m(x_{N+1}) = k^T \boldsymbol{C}_N ^{-1} \boldsymbol{t} \\ &amp; \sigma^2(x_{N+1}) = c - k^T\boldsymbol{C}_N ^{-1} k \end{align}\]<p>식에서도 알 수 있듯, predictive distribution은 Gaussian이며 평균과 분산이 new input value $x_{N+1}$ 에 의존한다. (또한 kernel에 의존한다) 그래서 kernel $k(x,x’)$만 잘 정의된다면 3.3.2장에서 배운 linear regression의 predictive distribution에 대한 gaussian process 관점을 살펴볼 수 있게 되는 것이다.(3.3.2에서 배운 predictive distribution의 평균, 분산의 형태와 매우 비슷하다)</p><p>정리하면 predictive distribution에 대한 두가지 관점이 존재한다.</p><ol><li><p>linear regression을 사용한 parameter space viewpoint</p><p>$y = X \beta + \epsilon $ 에서 $\beta$의 추정이 중요한 경우</p><li><p>Gaussian process를 활용한 function space viewpoint</p><p>$y = f(x_n) + \epsilon$ 에서 GP $f(x_n)$이 중요한 경우</p></ol><p>이제 Gaussian process의 한계를 살펴보자. 우선 GP는 역행렬 계산이 들어가기 때문에 연산량이 매우 크다 만약 kernel function을 바로 define해서 gram matrix를 구하면 복잡도는 $O(N^3)$일 것이다. 그러나 kernel function으로 바로 gram matrix를 define하는 방법 이외에도, mapping function을 통해 $\Phi$를 define한 뒤, gram matrix를 구하면 mapping function의 차원(차수) M에 의해 연산량은 $O(M^3)$ 이다.</p><p>이를 통해 알 수 있는 점은 무한차원의 mapping function을 사용한다고 할 때 이를 활용한 kernel f을 바로 define할 수 있다면 데이터의 개수에 의존하는 연산량만으로 GP를 계산할 수 있다. 그래서 이 책에서 제시되는 장점으로 “그럼에도 불구하고 GP를 사용하면 무한차원의 basis function을 covariance function(kernel function)을 통해 나타낼 수 있다는 장점이 있다.” 라는 말이 언급되는 것이다.</p><p><br /></p><h4 id="643-learning-the-hyperparameters">6.4.3 Learning the hyperparameters</h4><p>Gaussian process를 통한 예측은 결국 공분산함수에 영향을 받게 된다(공분산함수에 kernel이 들어가있기 때문). 특히, 공분산함수를 parametric하게 만들어 데이터에 따라 여러가지 변형을 줄 수 있다. 이러한 방법으로 책에서는 두 가지 방법을 소개한다.</p><ul><li><p>가장 단순한 방법으로 Maximum Likelihood Estimator로 hyperparameter $\theta$를 점추정하는 방법이 있다.</p><p>본래 커널은 연구자가 정하는 것이므로 kernel function $k(x,x’)$ 안에 들어있는 theta는 hyperparameter이다. 그러나 empirical Bayes 관점(prior마저도 data를 통해 ML방식으로 세팅하는 것)과 유사하게 ML방식으로 hyperparameter를 세팅할 수 있다.</p><p>multivariate Gussian distribution의 standard form을 활용해서 로그가능도를 표현하면</p>\[ln \ p(\boldsymbol{t} | \theta) = - \frac{1}{2} ln \ |C_N| - \frac {1}{2} \boldsymbol{t}^T C_N ^{-1} \boldsymbol{t} - \frac {N}{2} ln(2\pi)\]<p>$ln \ p(\boldsymbol{t} | \theta)$ 는 일반적으로 nonconvex function이므로 multiple maxima를 가진다.</p><li><p>두 번째 방법으로는 prior를 설정하고 log posterior를 극대화하는 방식으로 hyperparamter를 설정한다.(MAP)</p></ul><p>Gaussian process 로부터 predictive distribution을 구하면 평균과 분산이 모두 input x에 의존하게 됨을 위에서 보였다. 특히 분산은 noise에 의해서도 영향을 받는다는 것을 식을 통해 알 수 있는데, 몇몇 문제에서는 noise가 데이터에 의존하는 경우를 볼 수 있다. 이 경우 second Gaussian process $\beta(x)$ 를 모델링 할 수 있다.</p><p><br /></p><h4 id="644-automatic-relevnance-determination">6.4.4 Automatic relevnance determination</h4><p>Maximum likelihood 방식을 이용해서 parameter를 최적화하는 것을 통해 다양한 input들의 상대적 중요도를 판정할 수 있다. Gaussian process 에서 이러한 automatic relevance determination 방법을 통해서 변수의 중요도를 판정할 수 있다. (7.2.2 참고)</p><p>이차원의 input space $\boldsymbol{x}= (x_1, x_2) $ 에 대해 kernel function을 아래와 같이 정의하자.</p>\[k(\boldsymbol{x}, \boldsymbol{x}') = \theta_0 exp \{ - \frac{1}{2} \sum _{i=1} ^2 \eta_i (x_i - x_i ') ^2 \}\]<p>실제로 prior $\theta$에 대한 그림을 그려보면 parameter $\eta_i$ 가 작을수록 함수가 input $x_i$에 덜 민감해짐을 알 수 있다.</p><p>따라서, parameter $\eta_i$를 optimize 했을 때, 만약 그 값이 작다면 input $x_i$는 predictive distribution에서 중요도가 낮은 변수일 것이다.</p><p>아래 그림은 $\eta_i$ 값에 따라 prior가 input에 반응하는 민감도 차이를 나타낸 그림이다.</p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 70% 70%'%3E%3C/svg%3E" data-proofer-ignore data-src="/assets/img/prml-6-5.png" width="70%" height="70%" title="1" alt="relative" /></p><p>특히, ARD(Automatic Relevance Determination)는 exponential quadratic kernel 에서 잘 이용된다.</p>\[k(\boldsymbol{x}_n, \boldsymbol{x}_m) = \theta_0 exp \{ - \frac{1}{2} \sum _{i=1} ^D \eta_i (x_{ni} - x_{mi}) ^2 \} + \theta_2 + \theta_3 \sum _{i=1} ^D x_{ni}x_{mi}\]<p><br /></p></div><div class="post-tail-wrapper text-muted"><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/prml/" class="post-tag no-text-decoration" >PRML</a> <a href="/tags/machine-learning/" class="post-tag no-text-decoration" >Machine learning</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=PRML-6 - For Statistics&url=https://hwankam.github.io/posts/PRML6/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=PRML-6 - For Statistics&u=https://hwankam.github.io/posts/PRML6/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=PRML-6 - For Statistics&url=https://hwankam.github.io/posts/PRML6/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/PRML6/">PRML-6</a><li><a href="/posts/PRML5/">PRML-5</a><li><a href="/posts/PRML4/">PRML-4</a><li><a href="/posts/PRML3/">PRML-3</a><li><a href="/posts/CASI9/">Computer Age Statistical Inference - chap 9</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/datascience/">datascience</a> <a class="post-tag" href="/tags/datamining/">datamining</a> <a class="post-tag" href="/tags/machinelearning/">machinelearning</a> <a class="post-tag" href="/tags/islr/">ISLR</a> <a class="post-tag" href="/tags/statistical-method/">statistical method</a> <a class="post-tag" href="/tags/efron/">Efron</a> <a class="post-tag" href="/tags/prml/">PRML</a> <a class="post-tag" href="/tags/machine-learning/">machine learning</a> <a class="post-tag" href="/tags/machine-learning/">Machine learning</a> <a class="post-tag" href="/tags/categorical-data/">categorical data</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/PRML3/"><div class="card-body"> <span class="timeago small" >Nov 7, 2021<i class="unloaded">2021-11-07T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>PRML-3</h3><div class="text-muted small"><p> 3. Linear Models for Regression linear model : parameter에 대한 linear function ( input variables의 nonlinear function이 있다하더라도 이들의 결합이 파라미터 관점에서 선형결합이면 linear model이라 칭한다 ) 이 장에서는 선형모형과 이를 통한 학습 및 예측...</p></div></div></a></div><div class="card"> <a href="/posts/PRML4/"><div class="card-body"> <span class="timeago small" >Jan 19<i class="unloaded">2022-01-19T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>PRML-4</h3><div class="text-muted small"><p> 4. Linear Models for Classification 이번 chapter는 “Input space를 K개의 Class로 나누는 것”이 핵심이다. 이때 나눠지는 영역은 decision region, 나누는 boundary를 decision boundary 혹은 decision surface라 한다. 특히 이번 chapter에서 중요한 것은 ...</p></div></div></a></div><div class="card"> <a href="/posts/PRML5/"><div class="card-body"> <span class="timeago small" >Feb 9<i class="unloaded">2022-02-09T00:00:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>PRML-5</h3><div class="text-muted small"><p> 5. Neural Networks chap 3과 4에서는 linear combination of fixed bases function에 대해 배웠다. 그러나 high dimension에서는 과적합의 문제가 발생하는 한계가 있었다. 이를 해결하기 위해 데이터에 맞게 basis function을 바꾸는 것은 어떠할까? SVM은 데이터에 맞게 hyper...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/PRML5/" class="btn btn-outline-primary" prompt="Older"><p>PRML-5</p></a> <span class="btn btn-outline-primary disabled" prompt="Newer"><p>-</p></span></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2022 <a href="https://twitter.com/username">your_full_name</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/datascience/">datascience</a> <a class="post-tag" href="/tags/datamining/">datamining</a> <a class="post-tag" href="/tags/machinelearning/">machinelearning</a> <a class="post-tag" href="/tags/islr/">ISLR</a> <a class="post-tag" href="/tags/statistical-method/">statistical method</a> <a class="post-tag" href="/tags/efron/">Efron</a> <a class="post-tag" href="/tags/prml/">PRML</a> <a class="post-tag" href="/tags/machine-learning/">machine learning</a> <a class="post-tag" href="/tags/machine-learning/">Machine learning</a> <a class="post-tag" href="/tags/categorical-data/">categorical data</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}}); </script> MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) { alert("Math Processing Error: "+message[1]); }); MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) { alert("Math Processing Error: "+message[1]); }); </script> <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
