---
layout : post
title : PRML-6
subtitle : PRML
date : 2022-03-01
#categories:
tags : [PRML, Machine learning]
toc_sticky : true
use_math : true
comments: true
--- 

## 6. Kernel Methods

[서론]

there is a class of pattern recognition techniques

==> training data points are kept, memory based method

ex ) nearest neighborhood 방법은 training data와 가장 유사한 label을 test data에서 선택함. 즉 여기서 training data 가 storing 된다는 것은 모델 내부에서 파라미터를 정하는 학습이 아니라는 뜻. 즉, 모델을 달라고한다면 선형회귀모델 같은 경우에는 파라미터 계수를 주면 되겠지만, NN의 경우에는 데이터 자체를 줘야한다.

##### kerenl function 

$\ \ \$ : $k(x, x') = \phi(x)^T \phi(x')$

특징 : symmetric

종류 

* identity kernel(linear kernel) : $k(x, x') = x^T x'$

* stationary kernel : $k(x, x') = k(x - x') $ , 오직 차이에만 의존하므로 translation invarinace 를 갖는다.

* homogeneous kernel(radial basis kernel) 
  :
  $k(x, x') = k(||x - x'||) $ , 오직 거리의 크기에만 의존

large margin classifier로 머신러닝에서의 kernel function 중요성이 재조명됨

"extenstion of kernels to handle symbolic objects" : 다양한 자료를 kernel을 통해 분석할 수 있게 됨


<br>


### 6.1 Dual Representations

Many linear models can be reformulated in terms of a dual representation => kernel function

예측함수를 $w^t \phi(x)$ 라고 할 때, regularized sum of squares error function을 고려하면 이를 최소화하는 벡터 w는 $\phi(x)$ 의 linear combination으로 표현 가능하다. 

즉 dual formulation을 통해서 최소제곱의 문제를 kernel function을 통해 표현 가능하다. 

gram matrix $ K = \boldsymbol{\Phi} \boldsymbol{\Phi}^T $  에 대해서 error function을 다음과 같이 적을 수 있다

$$
\begin{align} 

&J(a) = \frac{1}{2} a ^T \boldsymbol{K}^T\boldsymbol{K}a - a^T \boldsymbol{K} \boldsymbol{t} + \frac{1}{2} \boldsymbol{t}^t\boldsymbol{t} + \frac{\lambda}{2} a^T \boldsymbol{K} a
\\
&\textrm{where} \ \ \ \ w =  \boldsymbol{\Phi}^T a \ \ \ \ \& \ \ \ \ a_n = \frac {-1}{\lambda} \{ w^T \phi(x_n) - t_n \}

\end{align}
$$

error function의 gradient 를 0으로 하는 a를 구해 prediction을 구하면

$$
y(x) = w^T \boldsymbol{\phi}(x) = a^T \boldsymbol{\Phi} \ \boldsymbol{\phi}(x)
$$

즉, w를 $\phi$의 linear combination으로 구하게 되면, 최소제곱의 문제를 커널을 통해 해결할 수 있게 되는 것이다. 

파라미터 w 에서 이를 변환한 a 를 고려하게 되면 일반적으로 차원은 더 커지게 된다. 그러나 kernel function $k(x, x')$을 통해서 온전히 식을 표시할 수 있으며 feature vector 
$\phi(x)$가 무엇인지 명확하게 제시할 필요도 없어진다. => high dimension을 다룰 수 있다.  

<br>


### 6.2 Constructing Kernels

construct valid kernel function이 중요하다

첫번째 방법으로는 mapping $\phi(x)$를 설정한 이후에 내적을 통해 kernel function을 찾는 것이 있고, 두번째 방법으로는 kernel fucntion을 directly 찾는 것이 있다. 두번째 방법에서는 내가 정한 kernel function이 정말 유효한 kernel function인지 찾는 것이 중요하다. 이때 gram matrix(각 원소가 kernel $\phi(x)$의 내적) positive semi - definite이라는 조건만 있으면 kernel function이 되는 필요충분조건을 만족한다. 

따라서 기본 형태의 kernel fucntion을 찾은 뒤 building block의 형태로 kernel을 만들어나간다. 

gaussian kernel : 
$k(x, x') = exp(-|| x - x' || ^2 / 2\sigma^2) = exp(-x^tx / 2\sigma^2) exp(x^tx/\sigma^2) exp(-(x')^tx' / 2\sigma^2 )$

gaussian kernel은 결국 linear kernel인 $k(x, x') = x^t x'$를 block으로 해서 만든 것이므로 kernel function이 됨을 알 수 있다. 


<br>

kernel 종류

* discriminant model과 generative model 섞기 : similarity measure

  use generative model to define a kernel and then use this kernel in a discriminative approach.
  
  ex ) Fisher kerenl (관련 논문 : http://www.vision.caltech.edu/publications/holubWellingPerona-FisherICCV05.pdf)

  $\dot l(\theta) \sim  \mathcal{N}(0 , n I(\theta)) $ 과 연관되어 있지 않을까

* sigmoidal kernel
  실제로 많이 이용되는 커널. SVM을 neural network와 연결짓는 kernel 

  "in the limit of an infinite number of basis functions, a Bayesian neural network with an appropriate prior reduces to a Gaussian process"
<br>


### 6.3 Radial Basis Function Networks

앞 장에서부터 basis function 을 활용한 linear regression 과 classification 에 대해 이야기했다. 대표적인 basis function으로는 radial basis function이 있다. radial basis function은 Euclidean distance를 기반으로 한 basis function이다. 

[Interpolation - RBF 배경]

본래 radial basis function은 exact function interpolation을 목적으로 만들어졌다. 일례로, input과 target을 정확하게 fitting 하는 smooth function을 만들기 위해서 radial basis function의 linear combination을 만드는 과정을 생각해볼 수 있다.

$$
f(x) = \sum_{n=1} w_n h(||x-x_n||)
$$

여기서 $w_n$은 least square를 통해 얻을 수 있다.

그러나 exact interpolation은 overfitting의 문제가 있다. 


input이 noisy 한 경우 interpolation을 고려하는 경우에도 Euclidean distance에 기반한 basis function을 고려할 수 있다. 

$$
E = \frac{1}{2} \sum_{n=1} \int \{y(x_n + \xi) - t_n   \}^2 v(\xi) d \xi 
$$

이 때 optimization은

$$
y(x_n) = \sum t_n h(x-x_n) \ \ \ \ \ \ \textrm{where} \ \ \ h(x- x_n) = \frac {v(x-x_n)}{\sum v(x - x_n)}
$$

<br>

이때 basis function h 는 normalized 되어있으므로, 이를 통해 모든 basis function이 작은 값을 갖게 되는 경우를 방지할 수 있다. 여기서 쓰인 basis function을 Nadaraya-Watson model이라고 한다.  


<img src='{{"/assets/img/prml-6-3.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>


<br>

[Kernel density estimation - RBF 확장]

normalized radial basis function의 확장으로, regression 문제에서의 kernel density estimation이 있다. 
여기서는 computing cost를 절감하기 위한 방안으로써, data point 보다 더 작은 개수의 basis function을 setting 하는 법이 있음을 말하고 있다.


일반적으로 basis function의 개수와 center에 대한 location $\mu_i$ 는 data point에 의해 정해지며
그 형태는 사전적이다. 또한 계수 $w_i$ 는 최소제곱을 통해 정해진다. 이 때에 basis function의 개수를 data point 개수보다 줄이기 위해서는 basis function의 center를 순차적으로 정하되, sum-of-square error를 최대한으로 줄이는 data point에 상응하는 center로 선택하는 sequential selection process가 있다. 이 방식은 통상적으로 orthogonal least square 라고 불린다.

###### orthogonal least square 

참고논문 : orthogonal least square regression : a unifed apporoach for data modeling

sparse kernel data modeling을 위해 kernel matrix(feature)를 QR 분해해서 orthogonoalize


<img src='{{"/assets/img/prml-6-1.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>


<img src='{{"/assets/img/prml-6-2.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

#### 6.3.1 Nadaraya-Watson model

(3.62)에서는 equivalent kernel $k(x, x') = \beta \phi(x)^T S_N \phi(x')$ 를 제시하며 predictive distribution mean이 training data의 target
$t_n$의 선형결합으로 나타내어짐을 보였다. 

이제 kernel density estimation의 관점에서 이를 살펴보자.

joint distribution $p(x,t)$ 를 추정하기 위해 Parzen density estimator를 활용해보자. 

$$
p(x,t) = \frac {1}{N} \sum_{n=1} ^N f(x - x_n, t- t_n)
$$


여기서 Paren density estimator는 kernel density estimation을 위한 방법 중 하나이다. 아래는 이에 대한 간단한 설명이다.


<font color="green">

(참고 링크)

[https://jayhey.github.io/novelty%20detection/2017/11/08/Novelty_detection_Kernel/]

  
  

1장에서도 짧게 배웠듯이, $P(x) \simeq \frac {K}{N V}$ 로 적을 수 있고, V를 고정시키고 K를 구하는 것은 kernel density estimator와 같이 생각할 수 있다.(반대로 K를 고정시키고 V를 찾는 것은 K-nearest neighborhood 라고 할 수 있다) 이를 활용해 d-dimension에서
$V = h^d$ 라고 할 때 x, t 를 기준으로 각 차원으로 h/2 안에 존재하는 데이터의 개수를 세어 밀도를 추정한다고 할 때 

$$
p(x, t) =\frac {1}{N h^d} \sum_{n=1} ^N f(\frac{x - x_n}{h}, \frac{t- t_n}{h})
$$

위의 식으로 밀도추정이 가능하다. 

</font>


이제 target variable 에 대한 conditional expectation을 구하는 것에 대해 생각해보자. 

$$
\begin{align}
y(x) & = E[t|x] = \int t p(t|x) dt \\

& = \frac{\int t p(x,t) dt }{\int p(x,t) dt} \\

& = \frac {\sum_n \int t f(x-x_n, t- t_n) dt }{\sum _m\int f(x-x_m, t-t_m) dt} \ \ \ \ \cdots \textrm{using Parzen density estimation }
\end{align}
$$


component f에 대한 zero mean을 가정하고, $g(x) = \int f(x,t) dt$ 라고 정의하면

$$
\begin{align}
y(x) & = \frac {\sum_n g(x-x_n) t_n}{\sum_m g(x-x_m)} \\
& = \sum k(x,x_n) t_n
\end{align}
$$

위 식의 마지막 결과를 보면 앞에서 Bayesian regresssion에서 보았던 predictive distribution mean과 비슷한 형태를 가지고 있음을 알 수 있다. 


<br>


### 6.4 Gaussian Process

kernel을 사용해 probablistic discriminative model로 확장해보자.(6.1은 non-probablistic model) 이 장에서는 kernel이 Bayesian setting과 어떻게 연관되는지 보여준다. 


<br>


#### 6.4.1 Linear regression revisited

[linear model에서 gaussian process가 어떻게 정의되는지]

다시 linear regression 문제로 돌아가서, predictive distirbution을 도출해내는 것을 생각해보자. (predictive dist 는 다음 절에서 나오는 듯?)

basis function의 선형결합으로 구성된 모델을 생각해보자.

$$
y(x,w) = w^T \phi(x)
$$

이때 w 의 Gaussian prior를 다음과 같이 정의한다.

$$
p(w) = \mathcal{N}(w |0, \alpha^{-1} I)
$$

training data point x 에 대한 joint distribution은 다음과 같다.

$$
\boldsymbol{Y} = \boldsymbol{\Phi} w
$$


$\boldsymbol{Y}$ 는 w 에 대한 linear combination이므로, 그 자체로 이미 Gaussian 이다. 그리고 평균과 분산은 다음과 같다.

$$
\begin{align}
& E[\boldsymbol{Y}] = \Phi E[w] = 0 \\

& cov[\boldsymbol{Y}] = E[\boldsymbol{Y} \boldsymbol{Y}^T] = \Phi E[w w^T] \Phi^T = \frac{1}{\alpha} \Phi \Phi^T \\ & \ \ \ \ \ \ \ \ \ \ \ \ =  K \textrm{(gram matrix)}

\end{align}
$$

이 모델(linear model)은 Gaussian process 의 특별한 모델로서 일반적으로 Gaussian process 는 joint distribution $\boldsymbol{Y}(x)$ 로 정의된다. 

Gaussian 이므로 평균과 분산은 second order statistic (second moment) 가 중요하고, w에 대한 prior의 평균을 0으로 잡을 때는, covariance matrix 인 gram matrix가 중요하다. Gaussian process 에서도 kernel을 직접 정할 수 있으며 아래 그림은 GP에서 각각 Gaussian kernel과 exponential kernel을 설정해 densitiy estimation을 수행한 결과이다.


<img src='{{"/assets/img/prml-6-4.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>


<br>

#### 6.4.2 Gaussian processes for regression


Gaussian process 문제를 regression setting에 적용하기 위해서는 noise 개념을 도입해야 한다. 

$$
t_n = y(x_n) + \epsilon_n
$$

Gaussian 분포를 갖는 noise process를 생각하면 

$$
p(t_n | y_n ) = \mathcal{N}(t_n | y(x_n), \beta ^{-1} )
$$

noise는 모두 독립이므로, $y(x_n)$의 값이 모두 조건으로 주어졌을 때 target value t에 대한 joint distribution은

$$
p(\boldsymbol{t} | \boldsymbol{y} ) = \mathcal{N}(\boldsymbol{t} | \boldsymbol{y}, \beta ^{-1} \boldsymbol{I}_N )
$$

그리고 $\boldsymbol{y}$가 Gaussian Process 를 따른다고하면 정의에 의해 marginal distribution 또한 Gaussian을 따른다.

$$
p(\boldsymbol{y}) = \mathcal{N}(\boldsymbol{y} | 0, \boldsymbol{K})
$$

여기서 K는 gram matrix이며 이는 각 원소가 kernel의 내적으로 정의된 함수이다. 

이때 t에 대한 marginal distribution을 구하면 

$$
p(\boldsymbol{t}) = \int p(\boldsymbol{t}|\boldsymbol{y}) p(\boldsymbol{y}) d\boldsymbol{y} = \mathcal{N} ( \boldsymbol{t} | 0, \boldsymbol{C} )
$$

Gaussian process regression에서 가장 자주 쓰이는 kernel function은 exponential of quadratic form으로 아래와 같다.

$$
k(x_n, x_m) = \theta_0 exp \{ - \frac{\theta_1}{2} ||x_n - x_m || ^2\} + \theta_2 + \theta_3 x_n^Tx_m
$$

여기서 주목할 만한 점은 linear term이 input variable x에 대한 linear function이라는 것이다.

이제 regression에 초점을 맞춰보자. 결국 하고 싶은 것은 prediction이므로 new input $x_{N+1}$에 대한 target 
$t_{N+1}$ 을 예측하고 싶다. 즉 
$p(t_{N+1} | \boldsymbol{t}_N)$ 을 구하고 싶다. 

N+1개의 target에 대한 joint distribution은 위에서 구한 marginal distribution을 활용해서 아래 결과를 알 수 있다.

$$
p(\boldsymbol{t}_{N+1}) = \mathcal{N} ( \boldsymbol{t}_{N+1} | 0, \boldsymbol{C}_{N+1} )
$$

이때 $\boldsymbol{C}_{N+1}$에 대한 partition matrix를 다음과 같이 정의한다고 하자.

$$


C=
(
\begin{array}{c c}
\boldsymbol{C}_N & k \\

k^T & c
\end{array}

)
$$

이 때의 predictive distribution의 평균과 분산은 아래와 같다.

$$
\begin{align}
& m(x_{N+1}) = k^T \boldsymbol{C}_N ^{-1} \boldsymbol{t} \\
& \sigma^2(x_{N+1}) = c - k^T\boldsymbol{C}_N ^{-1} k
\end{align}
$$


식에서도 알 수 있듯, predictive distribution은 Gaussian이며 평균과 분산이 new input value $x_{N+1}$ 에 의존한다. (또한 kernel에 의존한다) 그래서 kernel
$k(x,x')$만 잘 정의된다면 3.3.2장에서 배운 linear regression의 predictive distribution에 대한 gaussian process 관점을 살펴볼 수 있게 되는 것이다. 

정리하면 predictive distribution에 대한 두가지 관점이 존재한다. 

1. linear regression을 사용한 parameter space viewpoint

    $y = X \beta + \epsilon $ 에서 
    $\beta$의 추정이 중요한 경우

2. Gaussian process를 활용한 function space viewpoint

    $y = f(x_n) + \epsilon$ 에서 GP
    $f(x_n)$이 중요한 경우


이제 Gaussian process의 한계를 살펴보자. 우선 GP는 역행렬 계산이 들어가기 때문에 연산량이 매우 크다 -- $O(N^3)$

만약 basis function에 의해 x의 공간이 확장(?)되면 연산량은 기하급수적으로 늘어날 것이다. 그럼에도 불구하고 GP를 사용하면 공분산함수를 basis function을 활용해 나타낼 수 있다는 장점이 있다.

<br>

#### 6.4.3 Learning the hyperparameters

Gaussian process를 통한 예측은 결국 공분산함수에 영향을 받게 된다. 특히, 공분산함수를 parametric하게 만들어 데이터에 따라 여러가지 변형을 줄 수 있다. 이러한 방법으로 책에서는 두 가지 방법을 소개한다.

* 가장 단순한 방법으로 MLE의 방식으로 hyperparameter 를 점추정하는 방법이 있다. 

  multivariate Gussian distribution의 standard form을 활용해서 로그가능도를 표현하면 
  
  $$
  ln \ p(\boldsymbol{t} | \theta) = - \frac{1}{2} ln \ |C_N| - \frac {1}{2} \boldsymbol{t}^T C_N ^{-1} \boldsymbol{t} - \frac {N}{2} ln(2\pi)
  $$

  $\ln p(\boldsymbol{t} | \theta)$
  는 일반적으로 nonconvex function이므로 multiple maxima를 가진다. 

* 두 번째 방법으로는 prior를 설정하고 log posterior를 극대화하는 방식으로 hyperparamter를 설정한다.(MAP)

Gaussian process 로부터 predictive distribution을 구하면 평균과 분산이 모두 input x에 의존하게 됨을 위에서 보였다. 특히 분산은 noise에 의해서도 영향을 받는다는 것을 식을 통해 알 수 있는데, 몇몇 문제에서는 noise가 데이터에 의존하는 경우를 볼 수 있다. 이 경우 second Gaussian process $\beta(x)$ 를 모델링 할 수 있다.

<br>


#### 6.4.4 Automatic relevnance determination

Maximum likelihood 방식을 이용해서 parameter를 최적화하는 것을 통해 다양한 input들의 상대적 중요도를 판정할 수 있다. Gaussian process 에서 이러한 automatic relevance determination 방법을 통해서 변수의 중요도를 판정할 수 있다. (7.2.2 참고)

이차원의 input space $\boldsymbol{x}= (x_1, x_2) $ 에 대해 kernel function을 아래와 같이 정의하자.

$$
k(\boldsymbol{x}, \boldsymbol{x}') = \theta_0 exp \{ - \frac{1}{2} \sum _{i=1} ^2 \eta_i (x_i - x_i ') ^2 \}
$$

실제로 prior $\theta$에 대한 그림을 그려보면
parameter $\eta_i$ 가 작을수록 함수가 input
$x_i$에 덜 민감해짐을 알 수 있다.

따라서, parameter $\eta_i$를 optimize 했을 때, 만약 그 값이 작다면
input $x_i$는 predictive distribution에서 중요도가 낮은 변수일 것이다. 

아래 그림은 $\eta_i$ 값에 따라 prior가 input에 반응하는 민감도 차이를 나타낸 그림이다.

<img src='{{"/assets/img/prml-6-5.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

특히, ARD는 exponential quadratic kernel 에서 잘 이용된다.

$$
k(\boldsymbol{x}_n, \boldsymbol{x}_m) = \theta_0 exp \{ - \frac{1}{2} \sum _{i=1} ^D \eta_i (x_{ni} - x_{mi}) ^2 \} + \theta_2 + \theta_3 \sum _{i=1} ^D x_{ni}x_{mi}
$$

<br>
