---
layout : post
title : ISLR chap 7
subtitle : statistical learning - 7장
date : 2021-08-05
#categories:
tags : [datascience, datamining, machinelearning, ISLR]
toc_sticky : true
use_math : true
comments: true
---

### 7. Moving Beyond Linearity

relax the linearity assumption while still attempting to maintain as much interpretability as possible



#### 7.1 Polynomial Regression

변수들의 제곱항이나 세제곱항을 추가한 뒤에 기존의 linear regression이나 logistic regression을 똑같이 적합시키면 된다. 차원이 높아질수록 매우 비선형적인 그래프가 만들어질것이고 이 경우 심각한 오버피팅의 위험이 있기 때문에 보통은 3차원 혹은 4차원 정도에서 적합한다.

 

#### 7.2 Step Functions ( piecewise constant regression )

global structure을 피하고 step function을 사용해서 X 변수를  quantitative => qualitative로  바꾼다.

cutting 방법은 그림과 같이 cutpoint를 설정 후 categorical 변수로 바꾸는 것이다. dummy화 시키는 것

<img src='{{"/assets/img/islr7-1.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

각 bin을 predictor로 사용해서 회귀식에 적합

$
y_i = \beta_0 + \beta_1C_1(x_i) + \beta_2C_2(x_i) + ...+\beta_kC_k(x_i) + 
\epsilon_i
$

단, breakpoint를 잘못 설정하면 변수간의 증감이 드러나지 않을 수도 있다.


#### 7.3 Basis Functions

polynomial regression 혹은 piecewise-constant regression 은 basis function approach의 일종이다. basis function을 b는 연구자에 의해 사전에 정해진 것으로 이를 활용해서 predictor를 변화시켜 적합

$
y_i = \beta_0 + \beta_1b_1(x_i) + \beta_2b_2(x_i) + ...+\beta_kb_k(x_i) + 
\epsilon_i
$

polynomial function, step function, wavelets, Fourier series 등이 basis function으로 사용 가능
regression spline 또한 basis function을 사용한 회귀이다. 



#### 7.4 Regression Splines

flexible class of basis functions



##### 7.4.1 Piecewise Polynomials

X 변수의 각 영역(piecewise)에 낮은 차수의 각기 다른 polynomial을 적합시키는 것.

회귀 계수가 변하는 부분, 즉 cutpoint를 knot라고 부른다.

만약 single knot를 가진 piecewise cubic polynomial을 적합시키고자 한다면
$$
y_i = \begin{cases} \beta_{01} + \beta_{11}x_i + \beta_{21}x_i^2 & if \ \ x_i < c \\ \beta_{02} + \beta_{12}x_i + \beta_{22}x_i^2 & if\ \  x_i \geq c \end{cases}
$$


결국 앞에서 나온 step function은 knot를 통해 나눠진 구간에, 적합시키는 polynomial의 차수가 0인 것이고 simple 한 polynomial은 knot가 0개인  Piecewise Polynomials를 의미하는 것이다.



단점? 이라면 piecewise polynomial들이 불연속일 수 있고, knots의 개수가 늘어나면 모델 전체의 계수도 많아지는 것이므로 자유도가 늘어난다. 



##### 7.4.2 Constraints and Splines

Constraints가 필요한 이유? knot에 의해 나눠진 각 section 별 함수가 불연속일 수 있다. 이는 데이터 자체가 가진 본연의 이유가 아니라면 매우 어색한 것.

continuity와 smoothness를 주기 위해 constraints가 필요하다. 단 이러한 제약을 위해서 어느정도의 자유도는 감소된다.

예를들어 cubic spline은 third ordered piecewise polynomial에 3개의 제약(continuity, continuity of the first derivative, continuity of the second derivate)을 주면서 continuity와 smoothness를 달성

linear spline은 각 적합 함수가 선형이며 knot에서 연속성을 갖는 spline을 말한다.(smoothness는 달성하지 못함)



##### 7.4.3 The Spline Basis Representation

basis model을 활용해서 복잡한 spline을 표현해보자

cubic spline의 경우(knots는 K개)
$$
y_i = \beta_0 + \beta_1 b_1(x_i) + \beta_2b_2(x_i) + ... + \beta _{K+3}b_{K+3}(x_i) + \epsilon_i
$$

((cubic spline의 경우 자유도가 (knot 개수 K + 4)이다. 그 이유는 제약이 없을 때 자유도는 4K 인데 knot 하나당 제약으로 인해 자유도가 3씩 줄어 4(K+1) - 3K 가 최종 자유도가 되기 때문이다.))

basis model을 cubic polynomial와 truncated power basis function으로 표현해보자.

truncated power basis function이 다음과 같이 정의될 때

$
h(x,\xi) = (x-\xi)_+^3 = \begin{cases} (x-\xi)^3 & \ \ if  \ \ x > \xi \\ 0 & \ \ otherwise \end{cases}
$

basis function을 
$$
X, X^2, X^3, h(X,\xi_1), h(X,\xi_2), ...,h(X,\xi_K)
$$
로 잡으면 cubic spline을 basis function을 사용해서 표현하게 되는 것이다. 

그러나 이 경우에 predictor 값의 양 극단 부분에서는 분산이 매우 커지게 되는 경우가 발생한다. 이를 방지하기 위해서 'natural spline'이 등장하는데 양 극단 데이터에 대한 추가적인 가정을 부여하는 것이다. 단, 조건은 함수가 양 극단에서 linear 해야한다는 것. natural spline으로 인해 자유도는 기존의 cubic spline에 비해 4만큼 빠짐 

<img src='{{"/assets/img/islr7-2.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>



##### 7.4.4 Choosing the Number and Locations of the Knots

(1) location

함수가 급격하게 변할것 같은 부분을 연구자가 선택?

자유도를 연구자가 정한 뒤에 소프트웨어에 의해 uniform 하게 선택?



(2) number

가장 좋아보이는 curve를 그리는 knot의 개수를 선택?

CV의 결과 RSS가 가장 낮은 K를 선택

 

##### 7.4.5 Comparison to Polynomial Regression

polynomial은 flexible fit을 위해 degree(자유도, 차수)를 높여야 하는데 spline은 차수를 정해놓고, knot를 바꿔가면서 flexibility를 달성할 수 있다. 또한 함수 전체가 아닌 일부의 필요한 부분에 flexibility를 높이니까 훨씬 적합이 잘 된다. 추정량도 안정적이다. 특히 flexibility를 과하게 주면 데이터의 양 극단 부분에서 적합이 잘 안되고 분산이 매우 커지는데, natural spline의 경우 이 또한 방지 가능하다. 

<img src='{{"/assets/img/islr7-3.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>



#### 7.5 Smoothing Splines



##### 7.5.1 An Overview of Smoothing Splines

spline regression은 knot와 basis function을 결정해서 LSE로 계수를 추정하는 것이었다면 smoothing spline은 좋은 fitting을 위해 RSS를 줄이는 것에 기반한다. 단지 RSS만을 줄이는 것이 아니라 smoothness를 고려한다.  즉 아래의 식을 최소로 하는 g function을 찾고자 한다. 

$
\sum (y_i - g(x_i))^2 + \lambda\int g''(t)^2dt
$

이는 smoothness를 주기 위해서 penalty 항을 부여하는 것이다. 이를 좀 더 자세히 설명하면 
$$
g''(x)
$$
함수는 roughness에 대한 measure로 기울기의 변화율이다. 이를 적분해서 
$$
g'(x)
$$
에 대한 total change를 평가한 것이 
$$
\int g''(t)^2
$$
이 되는 것이다. 



또한,

 Ridge, Lasso와 비슷하게 tuning parameter lambda를 가지고 bias - variance trade-off 를 조절할 수 있다. 즉 natural cubic spline의 shrunken version이 smoothing spline 이라 할 수 있다. 

##### 7.5.2 Choosing the Smoothing Parameter lambda


모든 unique 값인 x_i에 대해 knot를 잡고 natural cubic spline을 하면 그것이 smoothing spline이 된다.(왜냐면 rss를 줄이고자 하는것이니까 모든 x_i에 대해 knot를 주는 것과 같다!) 그러나 모든 데이터에 대해 knot을 주면 자유도가 커진다. 여기서 lambda값을 통해 자유도를 control 하는 것이다. 

smoothing spline에서는 knot의 개수나 위치를 따로 구할 필요가 없다. 계산에 의해 알아서 해주니까. 단, 여기서는 tuninig parameter인 lambda 값을 정해야 한다. lambda값은 커지면 모델의 flexibility가 줄어드는데 이 값과 반대의 역할을 가지는 parameter로 effective degree of freedom
$$
df_\lambda
$$
 가 있다.  smoothing spline에서는 모든 데이터에 대해 knot를 주고 lambda로 variance를 조절하기 때문에 effective degree of freedom으로 자유도(분산과 관련있는 것)를 판단한다. 

이 값을 구하는 공식은 아래와 같다. smoothing spline을 적합한 fitted value를 벡터로 표현한 것이 
$$
\hat g_\lambda = S_\lambda y
$$
라고 할 때 
$$
df_\lambda = trace(S_\lambda)
$$


적당한 lambda 값은 어떻게 구할 것인가? CV를 이용.

특히 LOOCV 같은 경우 계산이 쉽다.

==>
$$
RSS_{cv}(\lambda) = \sum(y_i - \hat g_\lambda^{(-i)}(x_i))^2 = \sum [\frac {y_i - \hat g_\lambda(x_i)}{1-\{ \ S_\lambda \}_{ii}}]^2
$$




#### 7.6 Local Regression

알고리즘은 다음과 같다

<img src='{{"/assets/img/islr7-4.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

각 값에 대한 예측치를 주변 값을 이용해 하나하나 찾아가는 게 특징이다. (KNN과 유사)

KNN에서 K 값을 어떻게 정할지가 중요한 것 처럼 여기서도 s 값을 어떻게 정할지 결정해야 하는데 일반적으로는 CV를 통해서 정한다. s의 값이 작으면 fitting이 보다  local에 맞춰질 것이다(wiggly)

거리에 따라 weight를 주고 난 뒤에는 weighted least square를 통해 예측값을 정한다. 

또한 여러가지 변수를 종합적으로 보고 거리에 포함시킬지 말지 정할 수도 있는데 변수 개수가 너무 많은 경우, training에 필요한 변수가 적어 오버피팅 발생 가능.



#### 7.7 Generalized Additive Models

가법성(additivity)을 유지하면서 선형 모델을  비선형 모델으로 확장. 반응변수가 양적 혹은 질적 변수인 경우 모두 사용 가능.

##### 7.7.1 GAMs for Regression Problems

7.1-7.6에서는 하나의 변수에 대해 fitting function을 다르게 주는 법을 공부했다.

GAMs 의 특징은 "we can use these(7.1-7.6) methods as building blocks for fitting an additive model"

$
y_i = \beta_0 + \sum_j f_j(x_{ij}) + \epsilon_i
$

즉 각 변수마다 spline, local regression, polynomial 등등을 사용한 뒤 이를 가법모형으로 만들어 합치는 방식으로 모형을 fitting 하는 것.

###### pros and cons

장점 :

(1) 모델을 쌓을 때 비선형 모델을 추가할 수 있기 때문에 선형모델이라는 제약에서 쉽게 벗어날 수 있고, 따로 변수를 변환해주는 등의 노력이 필요 없다.

(2) 비선형 모델이 들어가기 때문에 fitting이 더 잘될 것

(3) 가법 모형이기 때문에 나머지 변수를 고정한 채로 하나의 변수의 영향력을 알 수 있다. 그러니까 모델의 해석력이 좋을 것.

(4) smoothness도 포함 가능



단점:

(1)  이름에서도 알 수 있듯, '가법'이라는 것에 모형을 한정시킨다. 따라서 변수간의 interaction이 포함되지 못한다. 그러나 애초에 변수 자체에 interaction을 포함시키고 이를 가법으로 쌓으면 앞의 문제를 상쇄 할 수 있다. 



GAMs는 linear 모형과 더 flexible한 모형인 RF, Boosting의 compromise



##### 7.7.2 GAMs for Classification Problems

로지스틱 회귀를 예로 들면

$
log(\frac {p(X)}{1-p(X)}) = \beta_0 + \sum_j f_j(x_{j}) 
$

와 같이 적합된 모델을 쌓아서 분류한다. 
