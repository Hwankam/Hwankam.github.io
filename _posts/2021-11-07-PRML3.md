---
layout : post
title : PRML-3.1
subtitle : PRML
date : 2021-11-07
#categories:
tags : [PRML, Machine learning]
toc_sticky : true
use_math : true
comments: true
--- 

## 3. Linear Models for Regression

linear model : parameter에 대한 linear function ( input variables의 nonlinear function이 있다하더라도 이들의 결합이 파라미터 관점에서 선형결합이면 linear model이라 칭한다 )

이 장에서는 선형모형과 이를 통한 학습 및 예측에 대해 설명하게 될 것이다.

선형모델은 단순하며 모형의 해석적 측면에서 뛰어나지만, high dimension으로 넘어가게 되면 모델 사용의 한계가 있다는 점을 먼저 밝힌다.


<br>


### 3.1 Linear Basis Function Models

input variables에 대해서도 선형함수라면 모형 자체가 너무 제한될 것이다. 따라서 basis function을 사용해서 조금 더 복잡한 선형모델을 만들어보자.

Basis function $\phi_j(x)$에 대해 다음과 같은 선형모델을 만들 수 있을 것이다.

$$
y(\mathbf x,\mathbf w) = \sum_{j=0}^{M-1} w_j \phi_j(x) = \mathbf w^T \mathbf \phi(x)
$$


여기서 basis fuction의 종류는 여러가지가 있다.  
* power를 사용한 $\phi_j(x) = x^j$ 
* 가우시안 basis $\phi_j(x) = exp\{ - \frac{(x-\mu_j)^2}{2s^2}\}$ 
* sigmoid basis $\phi_j(x) =\sigma (\frac {x-\mu_j} {s})$  

등이 있다.

<img src='{{"/assets/img/prml-3-11.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>



그러나 이 챕터에서는 특정 basis에 국한하지 않고, 일반적인 basis function에 적용되는 성질에 대해 말할 것이다.


##### Wavelets 

basis function으로 wavelets이 나오는데, wavelets transform을 한다면 함수를 특정 구간으로 localized 혹은 orthogonalized 할 수 있다. Input value들이 주변의 input value와 서로 연결되어있는 temporal sequnece나 image 상의 pixel 일 때 사용될 수 있을 것이다. 

<img src='{{"/assets/img/prml-3-12.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

<br>

#### 3.1.1 Maximum likelihood and least squares

notation이 지금까지 내가 공부하면서 쓰던 것과 조금 다르긴 하지만 내용은 동일함.

Gaussian noise assumption 하에서 선형모델을 아래와 같이 적을 수 있다

$$
p(t \mid \mathbf x, \mathbf w, \beta) = N(t \mid y(\mathbf x, \mathbf w),\beta^{-1})
$$

gaussian assumption이므로 모델은 unimodal일 것이다. 


likelihood function은 

$$
p(\mathbf t \mid \mathbf X, \mathbf w, \beta) = \prod _{n=1} N(t_n \mid y(\mathbf x_n, \mathbf w),\beta^{-1}) = \prod _{n=1} N(t_n \mid \mathbf w ^T \phi(\mathbf x_n),\beta^{-1})
$$


이를 극대화하는 파라미터 $\mathbf w$ 추정값은 (ML)

$$
\mathbf w_{ML} = (\Phi^T \Phi)^{-1} \Phi^T \mathbf t 
$$

이를 극대화하는 precision 파라미터 $\beta$ 추정값은 (ML)

$$
\frac {1} {\beta_{ML}} = \frac {1}{N} \sum \{t_n -  \mathbf w_{ML} ^T \phi(\mathbf x_n)  \}^2
$$

이를 극대화하는 bias term인 $w_0$의 추정값은 (ML)

$$
\sum _{n=1} \{t_n - w_0 - \sum _{j=1}w_j\phi_j(\mathbf x_n)   \}^2
$$

을 $w_0$로 미분한 값을 0으로 만드는 값이므로 

$$
\hat w_0 = \frac{1} {N} \sum _{n=1} t_n  - \sum _{j=1}  w_j \sum _{n=1} \frac{1} {N} \phi_j(\mathbf x_n) 
$$

이다. 즉 bias term의 추정값은 target value의 평균값과 basis function의 평균값에 대한 weighted sum의 차이를 의미한다.


<br>


#### 3.1.2 Geometry of least squares

아래 그림이 모든 것을 말해주고 있다. 

<img src='{{"/assets/img/prml-3-13.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

벡터 $  \mathbf t $ 에 대해 M개의 basis function이 span하는 공간 
$S$으로의 사영을 
$\mathbf y =   \hat {\mathbf w ^T} \phi(X) $ 라고 하자. 
즉 $\mathbf y$ 는 
공간 $S$의 기저벡터의 선형결합으로 이뤄지며, 
$\mathbf t$와의 squared Euclidean 거리가 가장 가까운 벡터이다. 

따라서, $\mathbf t$의 추정값을 구할 때는 
결국 $\mathbf w_{ML} = (\Phi^T \Phi)^{-1} \Phi^T \mathbf t $ 를 구해야하는데, 
실제 데이터 분석에서는 $(\Phi^T \Phi)$ 가 singular 인 경우가 부지기수다(high dimension). 이 때에는 SVD를 사용하거나 Ridge, Lasso와 같이 regularization을 통해 해결할 수 있다. 

<br>


notation의 편의를 위해 $\phi$ 대신에 
$\mathbf{X}$ 를 사용하겠다.

<br>

1. (thin) Singular Value Decomposition의 경우


만약 design matrix X 가 데이터의 개수가 변수에 비해 충분히 많은 (즉 n>p) 경우에는 thin SVD를 사용해서 아래와 같은 결과를 얻을 수 있을 것이다.

$$
X^TX = (VDU^T)(UDV^T) \Rightarrow \hat \beta = (X^TX)^{-1}X^Ty = VD^{-1}U^Ty
$$

그러나 design matrix X 가 high dimension이거나 mulicollinear인 경우에는 $ \mathbf X^T \mathbf X$ 가 signular가 될 것이다. 이때는 SVD를 통해 Moore-Penrose pseudoinverse matrix를 구하는 방식으로 문제를 해결할 수 있다.

$X=UDV^T$ 일 때 X의 무어 펜로즈 역행렬 
$X^- = VD^-U^T$ , 
$D=diag(d_1^{-1},d_2^{-1},...)$  을 사용해서 


$$
(X^TX)^- = V(D^2)^- V^T \Rightarrow \hat \beta = (X^TX)^{-}X^Ty = X^-y
$$

참고 : *무어 펜로즈 역행렬의 성질*

+ 무어펜로즈 역행렬은 SVD를 통해 만들어지므로 유일하다
+ 무어펜로즈 역행렬은 $XX^-X = X$를 만족하는 
$X^-$이다.
+ $(X^TX)^{-1}X^T = \mathbf K $ 는 
$X$의 generalized inverse 이다.  

<br>

2. Ridge의 경우

$$
\hat \beta^R = argmin_{\beta} \{ loss + \lambda || \beta||_2^2 \} = (X^tX + \lambda I)^{-1}X^Ty = (I + \lambda(X^TX)^{-1})^{-1}\hat\beta
$$



<br>


#### 3.1.3 Sequential learning 

online algorithm의 한 종류로, 실시간으로 파라미터의 추정량을 업데이트 할 수 있다. 데이터가 너무 큰 경우 인위적으로 데이터를 batch 별로 나눠 파라미터를 업데이트하는 방식을 사용할 수도 있을 것이다. sequential learning을 위해서는 Stochastic Gradient Descent를 사용한다. 즉

$$
\mathbf w^{(\tau+1)} = \mathbf w^{(\tau)} - \eta \nabla E_n  
$$

여기서 $E_n$이란 특정 n번째 데이터에 대한 error를 구한 것으로, SGD에서는 이전의 반복에서 추정한 결과값을 따로 기억할 필요가 없고 한번의 적합으로 한번의 업데이트가 일어나기 때문에 계산비용과 메모리 비용이 적다는 장점이 있다. 그러나 업데이트 되는 값의 variability가 클 수 있다는 단점이 있다.(loss function의 형태에 따라 경사하강의 방향이 뒤죽박죽)

교재 내용과는 별개로, 딥러닝에서 optimizer에 대한 설명을 담은 사진을 첨부한다.

<img src='{{"/assets/img/prml-3-14.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>


<br>


#### 3.1.4 Regularized least squares

딥러닝에서 weight decay라는 용어를 자주 볼 수 있다. weight란 통계학 관점에서의 parameter를 의미하는데, 단순히 loss 를 줄이는 것으로 파라미터값을 추정하다보면 특정 가중치(parameter)가 매우 커지면서 모델이 overfitting 하게 되므로 이를 막고자 weight값을 줄이자는 것이다. 통계학에서는 이를 shrinkage라고 하는데, regularization term을 사용해서 overfitting을 방지하고자 한다. 제약식 하에서 loss를 최소화 하는 것은 아래 식을 최소화 하는 것과 동일하다(Lagrange)

$$
\frac {1}{2} \sum \{t_n -  \mathbf w ^T \phi(\mathbf x_n)  \}^2 + \frac {\lambda} {2} \sum |w_j|^q
$$


regularization term을 통해서 파라미터 추정값을 구하면 주어진 제약 하에서의 squared loss를 최소로하는 추정값을 closed form으로 구할 수 있다는 장점이 있다. $\lambda$값을 크게할수록 변수에 대한 규제가 커지고 그 값을 0에 가깝게 만들기 때문에 이 값을 적절히 설정하는 것이 계수를 추정하는 것 만큼이나 중요하다.

특히 q=1 인 경우를 Lasso라고 하는데 이는 sparse model을 만들어 낼 수 있다. 아래그림에서와 같이 L1 제약하에서 파라미터 추정값이 0이 되는 것을 알 수 있다. 

<img src='{{"/assets/img/prml-3-15.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

<br>


#### 3.1.5 Multiple outputs

target variable이 multiple 인 경우에는 어떻게 해야할까?

각 target에 대한 모델을 만들 수도 있겠지만, 동일한 basis function을 사용해서 multiple response와 한번에 적합하는 모델을 생각할 수 있다. (후자가 더 일반적인 접근 방식이다)

target의 dimemsion이 K라고 하자.

기존 모델과의 차이는 basis function에 대한 set $\phi(\mathbf x)$는 기존과 동일하나, 회귀계수 벡터였던 $\mathbf w$ 대신 response variable의 개수(dimension K)를 열 개수로 하는 행렬 $\mathbf W$을 사용한다는 것이다. 가우시안 가정하에서 single obs $\mathbf t_1, \mathbf t_2, \cdots \mathbf t_n$에 대한 모델은 다음과 같다.

K 차원 벡터 $\mathbf y$에 대해

$$
\mathbf y(\mathbf x, \mathbf w) = \mathbf W^T \phi(\mathbf x)
$$

이며 target vector $\mathbf t$ 에 대해

$$
p(\mathbf t \mid \mathbf x, \mathbf W, \beta) = N(\mathbf t \mid \mathbf y(\mathbf x, \mathbf w),\beta^{-1})
$$


이다. 모델의 구성을 행렬과 벡터의 size를 통해 살펴보면 다음과 같다.  

$$
\mathbf T = \begin{bmatrix} t_{1} \ \  t_{2} \ \ t_{3} \ \  \cdots \ \  t_{K}   \end{bmatrix} = \begin{bmatrix} w_1 \  w_{2} \ w_{3} \ \cdots  w_{K}   \end{bmatrix} \begin{bmatrix} \ 1 \ \ \ \  \ \ 1 \ \ \ \ \ 1 \ \ \ \ \cdots \ \   \ \ 1 \\  \phi_{n1} \ \ \  \phi_{n1}  \ \ \ \phi_{n1} \ \cdots  \ \ \phi_{n1}  \\ \vdots \\ \phi_{nK} \ \ \phi_{nK} \ \ \phi_{nK} \ \cdots \phi_{nK}\end{bmatrix}  + \begin{bmatrix} \epsilon_{1} \ \  \epsilon_{2} \ \ \epsilon_{3} \ \  \cdots \ \  \epsilon_{K}   \end{bmatrix} = \mathbf W^T \phi(\mathbf x) + \mathbf \epsilon
$$


로그가능도함수는 

$$
\text{ln} \ p(\mathbf T \mid \mathbf X, \mathbf W, \beta) = \sum_{k=1} \sum _{n=1} \text{ln} \ N(\mathbf t_{nk} \mid y(\mathbf x_n, \mathbf w),\beta^{-1}) = \frac{NK}{2} \text{ln} \ (\frac {\beta}{2\pi}) - \frac{\beta}{2}   \sum _{n=1} ||\mathbf t_n -  \mathbf W ^T \phi(\mathbf x_n)||^2
$$

이렇게 보면 multiple response에서의 계수추정값은 기존의 single response variable에 대한 계수추정값과 같은 form으로 나오게 될 것임이 예상가능하며 각 target variable에 대해 각각의 회귀식을 적합시킨 것에 다를 바 없다.

$$
\mathbf W_{ML} = (\Phi^T \Phi)^{-1} \Phi^T \mathbf T 
$$

회귀계수 $\mathbf w$는 분산에 관계없이 오직 가우시안의 mean 과 연관되어 있으므로 
$\mathbf w$에 대한 추정을 할 때 각 target 변수들을 독립적으로 생각할 수 있다는 뜻.


<br>

### 3.2 The Bias Variance Decomposition


ML or LS 추정은 복잡한 모델에서 데이터셋의 개수가 한계가 있는 경우 overfitting의 문제를 야기할 수 있다. 그러나 무턱대고 모델의 복잡성(complexity / flexibility) 를 줄여 정말한 피팅을 포기할 수 있을까? 
Regularization term을 사용한다한들, $\lambda$ 값은 또 어떻게 정할 것인가?

3장의 목적은 over-fitting을 줄이기 위한 Bayesian 관점의 해결전략을 알아보는 데에 있다. 뒤에서도 설명하겠지만, posterior를 극대화하는 추정량(MAP)는 sum of squared error와 regularization을 동시에 고려해서 error를 최소화 하는 추정량이 된다.

그러나 이 part에서는 좀 더 일반적으로 알려진, frequentist 관점에서 모델 복잡성 문제 이슈를 살피는 bias-variance trade-off 에 관해 설명하도록 하겠다.

Expected squared loss decompostion을 통해 bias와 variance 간의 관계를 살펴보자. 

<br>

##### For Your Information  
어떤 loss 를 사용하는지에 따라, opitmal prediction이 달라짐을 1장에서 확인했다. 예를들어 squared loss f에서는 조건부 기댓값이, absolute loss f에서는 조건부분포의 median이 optimal prediction이 될 것이다. 핵심은 조건부 분포를 찾는 데에 있으므로, regularization 혹은 Baysian approach(Bayes theorem)를 통해 조건부 분포를 구할 수 있을 것이다. 


<br>


##### step 1

$t=X\beta + \epsilon $는 response variable, 
$y(x)$는 추정회귀선, 
$h(x) = X\beta $이라고 할 때 (figure 3.2 참조)


$$
E[L] = \int \{ y(x)-h(x) \}^2 p(x) dx + \int \{ h(x) - t   \}^2 p(x,t)dxdt
$$

와 같이 분해가 가능하다. 

첫번째 term 은 어떤 추정회귀식 $y(x)$ 를 선택했는가에 따라 달라지는 값이며 이를 최소화하는 y(x)를 찾고자 할 것이다. 이상적으로 데이터가 무수히 많다면 정확히 h(x) 와 일치하는 y(x)를 찾아낼 수 있을 것이므로 첫번째 term은 0이 될 것이다. 

반면, 두번째 term 은 $h(x) - t = \epsilon$ 이므로 intrinsic noise를 의미하며 expected loss 의 minimum이다. 즉 두번째 term은 결코 0이 될 수 없다.

이로부터 필연적으로 존재할 수밖에 없는 noise는 차치하고, 줄일 수 있는 첫번째 term을 어떻게 다뤄야할지 생각해볼 필요가 있다.

<br>


##### step 2

frequentist는 데이터셋을 통해 파라미터를 추정하며, 데이터셋이 바뀌면 당연히 추정값이 바뀌게 된다. 전체 데이터셋을 batch 형태로 나누었다고 할 때, 각 batch 마다의 적합 회귀추정식을 $y(x; D)$,
batch 마다 만들어진 추정회귀식의 평균을 $E_D[y(x;D)]$ 라고 하면 다음과 같은 분해가 가능하다.

$$
\begin{align} 
\int \{ y(x)-h(x) \}^2 p(x) dx & = \int \{ y_D(x)-E_D[y(x;D)] \}^2 p(x) dx + \int \{ E_D[y(x;D)] - h(x) \}^2 p(x) dx \\ 
& =\ \ \ \ \ \textrm{Variance of       } \  y_D(x) \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ + \ \ \ \ \ \ (bias)^2  

\end{align}
$$

여기서 분산이란 특정 데이터셋이 얼마나 민감한지를 나타내며 bias 란 추정한 회귀식의 평균값이 실제 참값이라고 여겨지는 h(x)와 어느정도 멀리 떨어져있는지를 알려준다. 

<br>

이제 모든 과정을 종합해보면

$$
\textrm{Expected loss = squared bias + variance + noise}
$$

임을 알 수 있다.

<br>



##### bias-variance trade-off

결국 expected loss 가 위와 같이 분해되기 때문에 bias 와 variacne 간의 trade - off 는 필연적이다.

<img src='{{"/assets/img/prml-3-16.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

위 그림은 regularization term $\lambda$ 값에 따라 bias와 variance가 어떻게 바뀌는지 simulation 한 결과이다. 제약이 강하게 들어갈 때, 분산이 줄어들고 상대적으로 bias 가 커진다. 

decompostion을 통한 관계를 qunatitative하게 나타내면

$$
\begin{align}
& (bias)^2 = \frac {1} {N} \sum_{n=1}^N \{\bar y(x_n) - h(x_n)    \}^2 \\
& Variance =  \frac {1} {N} \sum_{n=1}^N \frac{1}{L} \sum_{l=1}^L  \{y_{(l)}(x_n) - \bar y(x_n) \}^2
\end{align}
$$


<br>



##### limitation

식을 통해서도 알 수 있지만, 위의 decompostion은 data set의 평균에 관한 것이다. 그러나 현실에서는 오직 single data set만을 가진 경우가 허다하다. 이런 이유로 모델 complexity와 over-fitting 문제를 동시에 다룰 수 있는 Baysian approach를 공부해볼 필요가 있다.
