---
layout : post
title : PRML-3.1
subtitle : PRML
date : 2021-10-31
#categories:
tags : [PRML, Machine learning]
toc_sticky : true
use_math : true
comments: true
--- 

## 3. Linear Models for Regression

linear model : parameter에 대한 linear function ( input variables의 nonlinear function이 있다하더라도 이들의 결합이 파라미터 관점에서 선형결합이면 linear model이라 칭한다 )

이 장에서는 선형모형과 이를 통한 학습과 예측에 대해 설명하게 될 것이다.

선형모델은 단순하며 모형의 해석적 측면에서 뛰어나지만, high dimension으로 넘어가게 되면 모델 사용의 한계가 있다.  


<br>


### 3.1 Linear Basis Function Models

input variables에 대해서도 선형함수라면 모형 자체가 너무 제한될 것이다. 따라서 basis function을 사용해서 조금 더 복잡한 선형모델을 만들어보자.

Basis function $\phi_j(x)$에 대해 다음과 같은 선형모델을 만들 수 있을 것이다.

$$
y(\mathbf x,\mathbf w) = \sum_{j=0}^{M-1} w_j \phi_j(x) = \mathbf w^T \mathbf \phi(x)
$$


여기서 basis fuction의 종류는 여러가지가 있다.  
예를 들어 power를 사용한 $\phi_j(x) = x^j$ 
혹은 가우시안 basis $\phi_j(x) = exp \big \{ - \frac{(x-\mu_j)^2}{2s^2}  \big \}$ 나 
sigmoid basis $\phi_j(x) =\sigma (\frac {x-\mu_j} {s})$  

등이 있다.

<img src='{{"/assets/img/prml-3.1-1.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>



그러나 이 챕터에서는 특정 basis에 국한하지 않고, 일반적인 basis function에 적용되는 성질에 대해 말할 것이다.


##### Wavelets 

basis function으로 wavelets이 나오는데, wavelets transform을 한다면 함수를 특정 구간으로 localized 혹은 orthogonalized 할 수 있다. Input value들이 주변의 input value와 서로 연결되어있는 temporal sequnece나 image 상의 pixel 일 때 사용될 수 있을 것이다. 

<img src='{{"/assets/img/prml-3.1-2.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

<br>

#### 3.1.1 Maximum likelihood and least squares

notation이 지금까지 내가 공부하면서 쓰던 것과 조금 다르긴 하지만 내용은 동일함.

Gaussian noise assumption 하에서 선형모델을 아래와 같이 적을 수 있다

$$
p(t \mid \mathbf x, \mathbf w, \beta) = N(t \mid y(\mathbf x, \mathbf w),\beta^{-1})
$$

gaussian assumption이므로 모델은 unimodal일 것이다. 


likelihood function은 

$$
p(\mathbf t \mid \mathbf X, \mathbf w, \beta) = \prod _{n=1} N(t_n \mid y(\mathbf x_n, \mathbf w),\beta^{-1}) = \prod _{n=1} N(t_n \mid \mathbf w ^T \phi(\mathbf x_n),\beta^{-1})
$$


이를 극대화하는 파라미터 $\mathbf w$ 추정값은 

$$
\mathbf w_{ML} = (\Phi^T \Phi)^{-1} \Phi^T \mathbf t 
$$

error term의 분산 추정량은 

$$
\frac {1} {\beta_{ML}} = \frac {1}{N} \sum \{t_n -  \mathbf w_{ML} ^T \phi(\mathbf x_n)  \}^2
$$

bias term인 $w_0$의 추정값은 

$$
\sum _{n=1} \{t_n - w_0 - \sum _{j=1}w_j\phi_j(\mathbf x_n)   \}^2
$$

을 $w_0$로 미분한 값을 0으로 만드는 값이므로 

$$
\hat w_0 = \frac{1} {N} \sum _{n=1} t_n  - \sum _{j=1}  w_j \sum _{n=1} \frac{1} {N} \phi_j(\mathbf x_n) 
$$

이다. 즉 bias term의 추정값은 target value의 평균값과 basis function의 평균값에 대한 weighted sum의 차이를 의미한다.


<br>


#### 3.1.2 Geometry of least squares

<br>


#### 3.1.3 Sequential learning 

<br>


#### 3.1.4 Regularized least squares

<br>


#### 3.1.5 Multiple outputs
