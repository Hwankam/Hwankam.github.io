---
layout : post
title : PRML-3.1
subtitle : PRML
date : 2021-11-07
#categories:
tags : [PRML, Machine learning]
toc_sticky : true
use_math : true
comments: true
--- 

## 3. Linear Models for Regression

linear model : parameter에 대한 linear function ( input variables의 nonlinear function이 있다하더라도 이들의 결합이 파라미터 관점에서 선형결합이면 linear model이라 칭한다 )

이 장에서는 선형모형과 이를 통한 학습과 예측에 대해 설명하게 될 것이다.

선형모델은 단순하며 모형의 해석적 측면에서 뛰어나지만, high dimension으로 넘어가게 되면 모델 사용의 한계가 있다.  


<br>


### 3.1 Linear Basis Function Models

input variables에 대해서도 선형함수라면 모형 자체가 너무 제한될 것이다. 따라서 basis function을 사용해서 조금 더 복잡한 선형모델을 만들어보자.

Basis function $\phi_j(x)$에 대해 다음과 같은 선형모델을 만들 수 있을 것이다.

$$
y(\mathbf x,\mathbf w) = \sum_{j=0}^{M-1} w_j \phi_j(x) = \mathbf w^T \mathbf \phi(x)
$$


여기서 basis fuction의 종류는 여러가지가 있다.  
* power를 사용한 $\phi_j(x) = x^j$ 
* 가우시안 basis $\phi_j(x) = exp\{ - \frac{(x-\mu_j)^2}{2s^2}\}$ 
* sigmoid basis $\phi_j(x) =\sigma (\frac {x-\mu_j} {s})$  

등이 있다.

<img src='{{"/assets/img/prml-3-11.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>



그러나 이 챕터에서는 특정 basis에 국한하지 않고, 일반적인 basis function에 적용되는 성질에 대해 말할 것이다.


##### Wavelets 

basis function으로 wavelets이 나오는데, wavelets transform을 한다면 함수를 특정 구간으로 localized 혹은 orthogonalized 할 수 있다. Input value들이 주변의 input value와 서로 연결되어있는 temporal sequnece나 image 상의 pixel 일 때 사용될 수 있을 것이다. 

<img src='{{"/assets/img/prml-3-12.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

<br>

#### 3.1.1 Maximum likelihood and least squares

notation이 지금까지 내가 공부하면서 쓰던 것과 조금 다르긴 하지만 내용은 동일함.

Gaussian noise assumption 하에서 선형모델을 아래와 같이 적을 수 있다

$$
p(t \mid \mathbf x, \mathbf w, \beta) = N(t \mid y(\mathbf x, \mathbf w),\beta^{-1})
$$

gaussian assumption이므로 모델은 unimodal일 것이다. 


likelihood function은 

$$
p(\mathbf t \mid \mathbf X, \mathbf w, \beta) = \prod _{n=1} N(t_n \mid y(\mathbf x_n, \mathbf w),\beta^{-1}) = \prod _{n=1} N(t_n \mid \mathbf w ^T \phi(\mathbf x_n),\beta^{-1})
$$


이를 극대화하는 파라미터 $\mathbf w$ 추정값은 

$$
\mathbf w_{ML} = (\Phi^T \Phi)^{-1} \Phi^T \mathbf t 
$$

error term의 분산 추정량은 

$$
\frac {1} {\beta_{ML}} = \frac {1}{N} \sum \{t_n -  \mathbf w_{ML} ^T \phi(\mathbf x_n)  \}^2
$$

bias term인 $w_0$의 추정값은 

$$
\sum _{n=1} \{t_n - w_0 - \sum _{j=1}w_j\phi_j(\mathbf x_n)   \}^2
$$

을 $w_0$로 미분한 값을 0으로 만드는 값이므로 

$$
\hat w_0 = \frac{1} {N} \sum _{n=1} t_n  - \sum _{j=1}  w_j \sum _{n=1} \frac{1} {N} \phi_j(\mathbf x_n) 
$$

이다. 즉 bias term의 추정값은 target value의 평균값과 basis function의 평균값에 대한 weighted sum의 차이를 의미한다.


<br>


#### 3.1.2 Geometry of least squares

아래 그림이 모든 것을 말해주고 있다. 

<img src='{{"/assets/img/prml-3-13.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

벡터 $  \mathbf t $ 에 대해 M개의 basis function이 span하는 공간 
$S$으로의 사영을 
$\mathbf y =   \hat {\mathbf w ^T} \phi(X) $ 라고 하자. 
즉 $\mathbf y$ 는 
공간 $S$의 기저벡터의 선형결합으로 이뤄지며, 
$\mathbf t$와의 squared Euclidean 거리가 가장 가까운 벡터이다. 

따라서, $\mathbf t$의 추정값을 구할 때는 
결국 $\mathbf w_{ML} = (\Phi^T \Phi)^{-1} \Phi^T \mathbf t $ 를 구해야하는데, 
실제 데이터 분석에서는 $(\Phi^T \Phi)$ 가 singular 인 경우가 부지기수다(high dimension). 이 때에는 SVD를 사용하거나 Ridge, Lasso와 같이 regularization을 통해 해결할 수 있다. 

<br>

1. (thin) Singular Value Decomposition의 경우


구체적으로는 SVD를 통해 Moore-Penrose pseudoinverse matrix를 구해하는 방식이다. 

만약 design matrix X 가 데이터의 개수가 변수에 비해 충분히 많은 (즉 n>p) 경우에는 thin SVD를 사용해서 아래와 같은 결과를 얻을 수 있을 것이다.

$$
X^TX = (VDU^T)(UDV^T) \Rightarrow \hat \beta = (X^TX)^{-1}X^Ty = VD^{-1}U^Ty
$$

그러나 design matrix X 가 high dimension이거나 mulicollinear인 경우에는 $ \mathbf X^T \mathbf X$ 가 signular가 되고 이때는 

$X=UDV^T$ 일 때 X의 무어 펜로즈 역행렬 
$X^- = VD^-U^T$ , 
$D=diag(d_1^{-1},d_2^{-1},...)$  을 사용해서 


$$
(X^TX)^- = V(D^2)^- V^T \Rightarrow \hat \beta = (X^TX)^{-}X^Ty = X^-y
$$

* 참고

    무어 펜로즈 역행렬의 성질

+ 무어펜로즈 역행렬은 SVD를 통해 만들어지므로 유일하다
+ 무어펜로즈 역행렬은 $XX^-X = X$를 만족하는 
$X^-$이다.
+ $(X^TX)^{-1}X^T = \mathbf K $ 는 
$X$의 generalized inverse 이다.  

<br>

2. Lidge의 경우

$$
\hat \beta^R = argmin_{\beta} \{ loss + \lambda || \beta||_2^2 \} = (X^tX + \lambda I)^{-1}X^Ty = (I + \lambda(X^TX)^{-1})^{-1}\hat\beta
$$



<br>


#### 3.1.3 Sequential learning 

online algorithm의 한 종류로, 실시간으로 파라미터의 추정량을 업데이트 할 수 있다. 데이터가 너무 큰 경우 인위적으로 데이터를 batch 별로 나눠 파라미터를 업데이트하는 방식을 사용할 수도 있을 것이다. sequential learning을 위해서는 Stochastic Gradient Descent를 사용한다. 즉

$$
\mathbf w^{(\tau+1)} = \mathbf w^{(\tau)} - \eta \nabla E_n  
$$

여기서 $E_n$이란 특정 n번째 데이터에 대한 error를 구한 것으로, SGD에서는 이전의 반복에서 추정한 결과값을 따로 기억할 필요가 없고 한번의 적합으로 한번의 업데이트가 일어나기 때문에 계산비용과 메모리 비용이 적다는 장점이 있다. 그러나 업데이트 되는 값의 variability가 클 수 있다는 단점이 있다.(loss function의 형태에 따라 경사하강의 방향이 뒤죽박죽)

교재 내용과는 별개로, 딥러닝에서 optimizer에 대한 설명을 담은 사진을 첨부한다.

<img src='{{"/assets/img/prml-3-14.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>


<br>


#### 3.1.4 Regularized least squares

딥러닝에서 weight decay라는 용어를 자주 볼 수 있다. weight란 통계학 관점에서의 parameter를 의미하는데, 단순히 loss 를 줄이는 것으로 파라미터값을 추정하다보면 특정 가중치(parameter)가 매우 커지면서 모델이 overfitting 하게 되므로 이를 막고자 weight값을 줄이자는 것이다. 통계학에서는 이를 shrinkage라고 하는데, regularization term을 사용해서 overfitting을 방지하고자 한다. 제약식 하에서 loss를 최소화 하는 것은 아래 식을 최소화 하는 것과 동일하다(Lagrange)

$$
\frac {1}{2} \sum \{t_n -  \mathbf w ^T \phi(\mathbf x_n)  \}^2 + \frac {\lambda} {2} \sum |w_j|^q
$$


regularization term을 통해서 파라미터 추정값을 구하면 주어진 제약 하에서의 squared loss를 최소로하는 추정값을 closed form으로 구할 수 있다는 장점이 있다. $\lambda$값을 크게할수록 변수에 대한 규제가 커지고 그 값을 0에 가깝게 만들기 때문에 이 값을 적절히 설정하는 것이 계수를 추정하는 것 만큼이나 중요하다.

특히 q=1 인 경우를 Lasso라고 하는데 이는 sparse model을 만들어 낼 수 있다. 아래그림에서와 같이 L1 제약하에서 파라미터 추정값이 0이 되는 것을 알 수 있다. 

<img src='{{"/assets/img/prml-3-15.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

<br>


#### 3.1.5 Multiple outputs
