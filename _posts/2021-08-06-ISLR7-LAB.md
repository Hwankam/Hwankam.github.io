---
layout : post
title : ISLR
subtitle : statistical learning - 7장 - LAB
date : 2021-08-06
#categories:
tags : [datascience, datamining, machinelearning, ISLR]
toc_sticky : true
use_math : true
comments: true
---

### 7장 코드 정리

~~~R
##Non - linear modeling
library(ISLR)
attach(Wage)

#polynomial
fit = lm(wage~poly(age,4), Wage) #orthogonal polynomial
coef(summary(fit))

"""
                Estimate Std. Error    t value     Pr(>|t|)
(Intercept)    111.70361  0.7287409 153.283015 0.000000e+00
poly(age, 4)1  447.06785 39.9147851  11.200558 1.484604e-28
poly(age, 4)2 -478.31581 39.9147851 -11.983424 2.355831e-32
poly(age, 4)3  125.52169 39.9147851   3.144742 1.678622e-03
poly(age, 4)4  -77.91118 39.9147851  -1.951938 5.103865e-02
"""

fit2 = lm(wage~poly(age,4, raw = T), Wage)
coef(summary(fit2))
"""
                            Estimate   Std. Error   t value     Pr(>|t|)
(Intercept)            -1.841542e+02 6.004038e+01 -3.067172 0.0021802539
poly(age, 4, raw = T)1  2.124552e+01 5.886748e+00  3.609042 0.0003123618
poly(age, 4, raw = T)2 -5.638593e-01 2.061083e-01 -2.735743 0.0062606446
poly(age, 4, raw = T)3  6.810688e-03 3.065931e-03  2.221409 0.0263977518
poly(age, 4, raw = T)4 -3.203830e-05 1.641359e-05 -1.951938 0.0510386498
"""

#orthogonal polynomial은 제곱항 세제곱항 등이 가지는 높은 상관계수를 제거한 변수들이다. 회귀계수 추정값을 보면 orthogonal poly일 때 추정 계수 값이 훨씬 크다. 그러나 단점으로는 orthogonal 변수로 바뀌었기 때문에 모델의 해석력이 떨어진다.

#orthogonal인지 아닌지는 모델의 의미 자체에는 영향을 안준다. 즉 fitted value는 orthogonal 여부에 상관없이 동일하다. 

#raw=TRUE와 같은 polynomial regression은 아래 두가지이다.
fit2a = lm(wage ~ age + I(age^2) + I(age^3) + I(age^4), data=Wage)
coef(fit2a)
"""
  (Intercept)           age      I(age^2)      I(age^3)      I(age^4) 
-1.841542e+02  2.124552e+01 -5.638593e-01  6.810688e-03 -3.203830e-05 
"""

fit2b = lm(wage~cbind(age,age^2,age^3,age^4), data=Wage)
coef(fit2b)


agelims = range(age) #r 에서 range는 범위를 나타낸다.
age.grid = seq(from=agelims[1], to=agelims[2])
preds = predict(fit, newdata = list(age=age.grid), se=TRUE)
se.bands = cbind(preds$fit + 2*preds$se.fit,preds$fit - 2*preds$se.fit )
se.bands #예측값에 대한 신뢰구간 코딩으로 직접 구하기.



# polynomial 의 차수를 결정하기위해 anova를 사용하자
#단 여기서 각 모델은 nested model이어야.

fit.1 = lm(wage~age, data=Wage)
fit.2 = lm(wage~poly(age,2), data=Wage)
fit.3 = lm(wage~poly(age,3), data=Wage)
fit.4 = lm(wage~poly(age,4), data=Wage)
fit.5 = lm(wage~poly(age,5), data=Wage)
anova(fit.1, fit.2, fit.3,fit.4,fit.5)

#anova 메소드를 사용한 결과는 orthogonal poly와 결과가 같다.
#즉 coef(summary(fit)) 또한 동일한 p-value와 동일한 t-statistic ^2 = F-statistic 을 가지고 있다. 

#로지스틱에서도 polynomial을 사용할 수 있다.
fit = glm(I(wage>250) ~ poly(age,4), Wage, family = binomial)
preds = predict(fit, newdata = list(age=age.grid), se=T)
#여기서 나온 preds 값은 logit 값이므로 이를 바꾼다. 
pfit = exp(preds$fit) / (1+exp(preds$fit))
pfit

#위와 같은 논리를 간단한 코드로 나타낼 수 있다. family = binomial 이므로 type=response 만 주면 확률값이 나온다.
preds = predict(fit, newdata = list(age=age.grid), type = "response", se=T)
preds

#로지스틱 신뢰구간 만들기
plot(age,I(wage>250), xlim=agelims, type = "n", ylim=c(0,.2))
points(jitter(age), I((wage>250)/5), cex=.5, pch="l", col="darkgray")
lines(age.grid, pfit, lwd=2, col='blue')
matlines(age.grid,se.bands, lwd=5, col="green")
se.bands

#step function -- use cut() function
table(cut(age,4))
fit = lm(wage ~ cut(age,4), data=Wage)
coef(summary(fit)) #dummy 이므로 4개로 구간 cutting 하면 변수는 3개

"""
                        Estimate Std. Error   t value     Pr(>|t|)
(Intercept)            94.158392   1.476069 63.789970 0.000000e+00
cut(age, 4)(33.5,49]   24.053491   1.829431 13.148074 1.982315e-38
cut(age, 4)(49,64.5]   23.664559   2.067958 11.443444 1.040750e-29
cut(age, 4)(64.5,80.1]  7.640592   4.987424  1.531972 1.256350e-01
"""


## spline regression
library(splines)
fit = lm(wage ~ bs(age, knots = c(25,40,60)), data=Wage) #spline은 basis function을 만드는 bs 명령어를 준다.
pred = predict(fit, newdata=list(age=age.grid), se=T)
plot(age, wage, col='gray')
lines(age.grid, pred$fit , lwd=2)
lines(age.grid, pred$fit + 2*pred$se, lty = "dashed")
lines(age.grid, pred$fit - 2*pred$se, lty = "dashed")

#knots를 data의 uniform quantile로 주고 싶다면
bs(age, df=6) # 왜 6인가? cubic spline에서는knot가 3개 일때 자유도가 7인데 intercept를 제외하고 6이 되는 것.
fit2 = lm(wage~bs(age,df=4), data=Wage)


## smoothing spline -- smooth.spline() 사용
plot(age, wage, xlims=agelims, cex = .5 , col = 'darkgrey')
fit = smooth.spline(age, wage, df=16)
fit2 = smooth.spline(age, wage, cv=TRUE)
fit2$df #6.8 즉 cv를 통해서 rss를 가장 작게하는 smoothness level을 결정.

lines(fit, col='red')
lines(fit2, col='blue')



##local regression -- loess() 사용

plot(age, wage, xlim = agelims, cex=.5, col='darkgrey')
fit = loess(wage~age, span=.2, Wage)
fit2 = loess(wage~age, span=.5, Wage)
lines(age.grid, predict(fit,data.frame(age=age.grid)),col='red')
lines(age.grid, predict(fit2,data.frame(age=age.grid)),col='blue')


## GAMs
# big linear regression model using basis function 인 경우
gam1 = lm(wage ~ bs(year,4) + bs(age,5) + education, Wage)

# smoothing spline을사용한 GAMs -- gam 라이브러리 사용

library(gam)
gam.m3 = gam(wage ~ s(year, 4)+s(age,5)+education, data=Wage)
par(mfrow=c(1,3))
plot(gam.m3, se=TRUE, col="blue")
#par(mfrow=c(1,1))

plot.gam(gam1, se=TRUE, col='red') #plot.gam 은 gam,lm 모두 사용 가능

summary(gam.m3)
preds = predict(gam.m3, newdata = Wage)

#gam을 building 하는 데에 local regression 쓸수도 있음.
gam.io = gam(wage ~ s(year, df=4) + lo(age, span=0.7) + education, data=Wage)
gam.io.i = gam(wage ~ lo(year,age, span=0.5) + education, data=Wage) #localregression 과정에서 interaction term 넣기

library(akima) #two-dimenstional surface 볼 수 있는 package
plot(gam.io.i)

#logistic GAMs
gam.lr = gam(I(wage>250) ~ year + s(age,df=5)+education, family = binomial, data=Wage)
plot(gam.lr, se=T, col='green')

~~~

