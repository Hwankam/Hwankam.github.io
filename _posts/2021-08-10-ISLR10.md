---
layout : post
title : ISLR
subtitle : statistical learning - 10장
date : 2021-08-10
#categories:
tags : [datascience, datamining, machinelearning, ISLR]
toc_sticky : true
use_math : true
comments: true
---

### 10. Unsupervised Learning

supervised learning : X를 가지고 Y를 예측하는 것

unsupervised learning : X 데이터만을 가지고 어떠한 특성을 밝히는 것. visualize... subgroups...

대표적인 비지도학습 : PCA : visualization, pre-processing / clustering : subgroup

 

#### 10.1 The Challenge of Unsupervised Learning

앞서 지도학습에서는 분류 혹은 회귀 라는 분석 목적이 뚜렷했으며, 지도학습 분석의 성능에 대해 평가하는 기준이 명확했다(e.g CV).

그러나 비 지도학습의 경우에는 분석의 목적이 다소 주관적이며 exploratory data analysis(EDA) 의 성격을 가지고 있다. 또한 분석의 결과가 얼마나 신뢰할 수 있고 바람직한지를 평가하는 기준이 없다.(Y 관측치가 존재하지 않는다!!)

그럼에도 불구하고 비지도학습의 중요성은 점점 커져간다. 이는 데이터 자제의 숨겨진 속성들을 추출해냄으로서 더 많은 부가가치를 창출할 수 있기 때문이다.

 

#### 10.2 Principal Components Analysis

ISLR 6.3.1 부분의 PCR 을 참고하자

##### 10.2.1 What Are Principal Components

feature의 개수가 많다면, 정보의 관계를 한눈에 파악하기 힘들다. 대신 이러한 정보들을 추출해서 2차원 데이터로 만들어 시각화 한다면 훨씬 이해하기 편하다. ==> PCA

pca는 관심 정보들을 작은 차원에서 설명하려는 노력. 축소된 차원은 본래의 p 개의 feature들에 대한 조합

first PC of normalized data 는 다음과 같이 조합된다.

$
Z_1 = \phi _{11}X_1 + \phi_{21}X_2 + ... + \phi_{p1}X_p
$

여기서 phi 값은 방향을 의미하고 Z_1이 first PC라면 phi는 가장 분산이 큰 방향을 의미하게 될 것. 그리고 Z 값은 score 값으로 phi에 의해 정해진 방향에 X를 사영시켰을 때 나오는 projected value이다.

(pc loading phi 의 제곱합은 1 : 이런 제약 조건이 없다면 분산을 가장 크게 만든다는 데이터의 조합에 대한 기준이 없어짐.)



pc loading 을  찾는 공식은 아래와 같다

$
max_{\phi_{11},..., \phi_{p1}} \frac {1}{n} \{\ \sum_i^n(\sum_j^p \phi_{j1}x_{ij})^2\} \ \ subject \ \ to \ \ \sum \phi^2_{j1} = 1
$

위 공식을 처음보면 어떻게 나온 것인지 헷갈릴 수 있다. i는 obs, j 는 feature를 의미하며, 데이터가 표준화되어있다는 전제하에 

$
\sum _i x_{ij} = 0
$

이므로 maximization 하고자 하는 값은 결국 분산을 의미한다. 또한 이렇게 분산을 극대화시키는  pc score를 찾는 것은 결국 데이터의 eigen-value 값을 찾는 것과 동일한데 이 내용은 책에 생략되어있다.

두 번째 PC score Z_2 를 찾기 위해서는 분산이 Z_1 다음으로 크게 하면서도 Z_1과는 uncorrelated 되어있는 X들의 linear combination을 찾아야 한다. 이는 loading벡터의 방향이 이전 first PC score의 loading 벡터와 직교하도록 만들면 된다.  즉 PC loading 을 찾기위해 제약조건하에서의 최적화 식에서 추가적으로 직교성을 추가하면 된다.



기하학적으로 생각해보자. 주성분을 계산하고 나면 낮은 차원에서 이를 plot 할 수 있는데, PC score 벡터는 본래 데이터(X)를 방향벡터인 phi로 span 된 공간에 사영시킨 것을 의미한다. 

방향벡터 
$$
\phi 
$$
들은 
$$
X'X
$$
의 eigenvector가 되고(X'X는 표준화된 X에 대한 분산행렬이다) PC score의 분산값은 eigen value가 된다

기하학적 내용에 대해 수식으로 접근해보면

단위벡터 e에 의해 span 된 공간에 X 데이터를 사영하는 것은
$$
Xe
$$
이고 이것의 분산이 최대가 되도록 사영하는 것을 목적으로 하기 때문에
$$
Max_e \ \ [Var(Xe)] \ \ subject \ \ to \ \ \sum e_i^2 = 1
$$
를 풀면
$$
\Sigma e = \lambda e
$$
를 만족할 때 위 식을 최대로 만들 수 있다. 여기서 lambda는 고유값이고 e는 고유 벡터가 된다. 즉 데이터의 분산을 최대로 하는 방향벡터는 X의 분산에 대한 고유벡터이고, PC score의 값은 고유값 lambda가 된다



$$
Var(\Sigma e) = e'\Sigma e \ = e' \lambda\ e = \lambda
$$


<img src='{{"/assets/img/islr10-1.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

PCA는 요인분석과도 연관이 깊은데 이에 대한 내용이 살짝 언급되어 있다.

아래 표를 보자

<img src='{{"/assets/img/islr10-2.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

pc1의 loading vector를 보면 Murder, Assault, Rape 의 비율이 높다. 즉 pc1은 중범죄율이라는 새로운 변수로 해석할 수 있다. 반대로 pc2를 보면 UrbanPop의 비율이 높다. 즉 pc2는 도시화율이라는 변수로 해석될 수 있다. 즉 pc1의 score 값이 높은 obs의 경우 높은 범죄율을 가지는 데이터라 말할 수 있다. 만약 어떤 obs가 각 score의 값이 0 인 경우는 범죄율이나 도시화에 대해서 그냥 평균정도의 상황임을 드러내는 것이라 할 수 있다. 



##### 10.2.2 Another Interpretation of Principal Components

Principal component의 또 다른 해석은 바로 관측치와 가장 가까운 low dimensional linear surface를 제공한다는 것이다. first pc loading vector는 p 차원의 데이터에서 이들과 가장 가까운 line을 의미한다. 여기서 더 나아가 first two pc loading vector는 데이터와 가장 거리가 가까운 평면을 만들어 낸다. 

즉 차원이 매우 클 때, M개의 pc score 값 혹은 pc loading vector 들이 원 데이터를 잘 설명하는 근사치가 될 수 있다.

$
x_{ij} \approx \sum ^M z_{im}\phi_{jm}
$

또한, 

$
M = min(n-1,p)
$

이면 정확히

$
x_{ij} = \sum ^M z_{im}\phi_{jm}
$

가 성립한다.


##### 10.2.3 More on PCA

(1) 분산을 기준으로 데이터를 축소하기 때문에 측정 단위가 동일해야 한다. => 표준화가 꼭 필요하다. 

(2) prinicpal component loading vector는 유일하다. 즉 단위가 바뀌거나 flip되어도 벡터는 동일하다. 또한 score vector 또한 유일하다. 이는 z에 대한 분산이나, -z 에 대한 분산이 동일한 것과 같은 이치이다.

(3) Principal component 중 몇개를 뽑는 것으로 전체 데이터를 축약한다면 데이터의 소실량은 얼마인가? 분산의 비로 나타내보자.

==> proportion of variance explained(PVE) of the m-th principal component

$
\frac {\sum^n(\sum^p \phi_{jm} x_{ij})^2}{\sum^p(\sum^n x_{ij^2})}
$

<img src='{{"/assets/img/islr10-3.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>



(4) 그럼 principal component를 몇개 골라야 할까?

전체데이터를 잘 설명하는 축소된 변수가 작으면 작을수록 좋을 것이다. 

scree plot을 가지고 볼 때 분산 비율이 크게 감소하지 않는 지점 직전까지로 principal component의 개수를 정하면 좋을 것이다. 그러나 이는 각 데이터마다 어떻게 할지 모두 다르므로, 각 principal components 마다 어떠 패턴이 존재하는지 확인해보면 좋을 것이다. 만약 PC regression(지도학습) 을 하는 경우라면 CV 값이 제일 작은 principal component개수를 설정하면 될 것이다. 

##### 10.2.4 Other Uses for Principal Components

차원 축소를 통해서 regression, classification, clustering에 모두 사용될 수 있다. 차원 축소가 잘 된다면 데이터의 nosie를 줄여 보다 올바른 결과가 나올 수도 있다.

#### 10.3 Clustering Methods

군집화를 할 때는 무엇이 같고 무엇이 다른 것인지에 대한 기준이 필요하다. 이는 domain 지식을 요구할 수도 있다. pca와 clustering은 데이터를 단순화 시킨다는 데서 공통점이 있지만, pca는 feature의 dimension을 줄이려는 시도이고, clustering은 obs안에서 subgroup을 찾으려는 시도라는 데서 차이점이 존재한다.

그런데 clustering 과정에서 X matrix를 transpose한다고 하면 obs를 기반으로 해서 feature를 나누는 시도 또한 가능하다. 이는 새롭게 배우는 idea인데 충분히 생각해볼 필요가 있다.



##### 10.3.1 K-Means Clustering

바람직한 K를 clustering 이전에 미리 정해야한다.  또한 각 clusteing은 서로 겹치지 않아야 하며 그 합이 전체 데이터 set을 만들어야 한다

K-Means의 아이디어는 무엇인가? 우선 각 cluster 내부의 데이터들은 작은 분산을 가질 것이라는 것이다. 각 cluster의 within-variance를 W라고 하면 clustering의 문제는 아래의 식을 푸는 것과 동일하게 된다.

$
minimize _{ C_1, C_2,...,C_k} \{\ \sum^K W(C_k)\}
$

W를 정의하는 방법은 여러가지가 있지만 책에서는 pairwise squared Euclidean distance의 총합을 cluster의 원소 개수로 나누는 것을 제시한다

$
W(C_k) = \frac { 1} {|W_k|}\sum_{i,i' \in C_k} \sum _j ^ K (x_{ij} - x_{i'j})^2
$

그런데 이런 방식은 계산량이 매우 많다. K^n 만큼의 경우가 존재.

따라서 local optimum을 찾는 방식의 알고리즘이 존재한다.

<img src='{{"/assets/img/islr10-4.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

알고리즘에서는 K-Means 라는 명칭에 걸맞게 평균을 사용해서 clustering을 하는데 이것이 가능한 이유는 아래와 같다. 앞서 말했던 within-variance를 최소화 하는 식은 다음 식과 동일하다

$
\frac { 1} {|W_k|}\sum_{i,i' \in C_k} \sum _j ^ K (x_{ij} - x_{i'j})^2 = 2\sum_{i\in C_k}\sum_{j=1}^K(x_{ij} - \bar x_{kj})^2
$

즉 분산을 최소화하는 값이 바로 clustering 내부의 평균값과의 차이 합인 것이다.

그러나 여전히 이러한 방법은 global opitmum이 아닌 local optimum이므로 초기값을 다양하게 설정함으로써 clustering이 보다 잘 되게 할 수 있다. 알고리즘의 결과치를 비교할 때에는 within-variance의 총합을 최소로 하는 clustering을 찾는다.

아래는 알고리즘을 그림으로 표현한 것이다

<img src='{{"/assets/img/islr10-5.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

<img src='{{"/assets/img/islr10-6.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>


##### 10.3.2 Hierarchical Clustering

K-Means와는 달리 K 값을 미리 정하지 않아도 된다. 결과값으로 tree-based representation을 준다. 이를 dendrogram이라고 한다.

핵심용어는 bottom - up, agglomerative  clustering, upside-down tree



##### Interpreting a Dendrogram

dendrogram에서 관측지들이 유사할수록 더 빨리(dengrogram의 아랫부분) 결합될 것이다. 

또한 dendrogram은 vertical axis를 기준으로 유사성을 판단하게 되는데, 서로 다른 brach이면 그 값들이 horizontal 하게 떨어져있을 때는 유사성이 차이가 없다. 아래 그림을 참조

<img src='{{"/assets/img/islr10-7.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

그럼 clustering을 어떻게 할 것인가? horizontal cut을 사용. horizontal cut을 어떻게 하는가에 따라 cluster K 값이 달라진다. 아래 그림의 점선이 horizontal cut을 나타내고, 그에 따른 clustering을 보여준다.

<img src='{{"/assets/img/islr10-8.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

heirarchical의 의미는 cutting에 의한 cluster들이 다른 cluster 에 nested 되어 있다는 것이다. 그러나 모든 데이터에서 이런 계층적 구조가 발생하는 것은 아니다. 예를들어 남 / 녀, 한/ 중/ 일 데이터를 성별, 그리고 국적으로 clustering 할 수 있겠지만 어떤 cluster가 다른 cluster 안에 계층적으로 포함되지는 않는다.



##### The Hierarchical Clustering Algorithm

알고리즘은 다소 단순하다. 관측치가 n개 있을 때, 유클리디안 거리를 기준으로 dendrogram의 제일 하단 부분에서 가장 유사한 두 관측치를 결합해서 n-1개의 cluster를 만든다. 마찬가지 방식으로 n-2, n-3, ... 으로 cluster의 개수를 줄여나가고 dendrogram의 제일 윗 부분에서는 하나의 cluster만이 남는다.

이제 또 다른 문제를 살펴보자. 관측치들의 여러개 들어있는 그룹들의 거리는 어떻게 측정할 수 있을까? 이를 위해 linkage 라는 개념이 등장한다. 그 종류로는 Complete, Single, Average, Centroid가 있고 각 likage 마다 dissimilarity를 측정하는 다른 기준이 있다. 일반적으로 complete이나 average linkage는 single linkage 에 비해 훨씬 균형적이다. 또한 Centroid linkage는 inversion의 위험이 있다. 

hierarchical 알고리즘과 linkage에 대한 내용이 책에 나온다.

<img src='{{"/assets/img/islr10-9.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>



##### Choice of Dissimilarity Measure

지금까지는 유클리디안 거리를 기준으로 dissimilarity를 체크했는데 다른 방식의 측도에 대해서 알아보자.

correlation-based distance는 두 obs 간의 상관성을 비교하는 것으로 유클리디안 거리가 크기를 가지고 비교했다면, correlation-based distance는 관측치 profile의 모양을 가지고 dissimilarity를 판단하는 것이다. 어떠한 기준을 선택하는지는 데이터의 형태 혹은 분석의 목적에 따라 달라진다. 예를들어 마케팅 팀에서 소비자들을 대상으로 군집분석을 하는데, 구매 패턴이 비슷한 소비자를 같은 그룹으로 할지, 아니면 소비액이 유사한 소비자를 같은 그룹으로 할지 차이가 있는 것이다. 

또한 변수를 표준화하는 것에 따라 dissimilarity가 달라질 수 있으므로 이데 대해서도 고려해야한다(K-means 도 마찬가지)



##### 10.3.3 Practical Issues in Clustering

clustering 과정에서 고려해야할 부분이 몇가지 있다.

(1) Small Decisions with Big Consequences

1. obs 혹은 feature가 standardize 되어야 한다. 왜? 분산으로 clustering을 하기 때문에 
2. hierarchical clustering 의 경우 - dissimilarity measure / linkage / where to cut the dendrogram
3. K-Means cluster에서 K의 값은?



(2) Validating the Cluster Obtained

clustering이 제대로 된 것일까? 아님 noise 때문에 우연히 cluster 된 것일까? 이를 평가하는 기준은?

p - vlaue로 cluster가 우연에 의한 것인지 아닌지를 평가하는 방법이 있긴한데 이 또한 학자들이 모두 인정한 그런 방식은 아니다.



(3) Other Considerations in Clustering

clustering 은 데이터의 작은 변동에도 크게 바뀐다( robust X)

대다수의 obs가 몇개의 소그룹에 속해있는 경우, 그리고 그 소그룹들이 서로 매우 다른 경우 ==> clustering에 왜곡이 일어날 수 있다. clustering 은 특정 그룹에 데이터를 강제로 분류하기 때문에. 이를 막기위해 mixture model을 사용한 soft K-means 가 등장



(4) 

clustering을 하는 여러가지 방법을 배웠는데(standardize, linkage,...) 이들을 모두 사용해서 반복적으로 나타나는 패턴이 있는지 파악하는 것이 중요. non - robust한 성질을 가지고 있기 때문에, clustering을 절대적인 분류로 생각하지 말고, 연구를 하는데 어느정도의 도움을 얻고 또 출발점이 되는 그런 정도의 용도로 사용하는게 좋다.
