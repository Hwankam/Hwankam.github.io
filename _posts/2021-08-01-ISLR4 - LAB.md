---
layout : post
title : ISLR - LAB4
subtitle : statistical learning - 4장-LAB
date : 2021-08-02
#categories:
tags : [datascience, datamining, machinelearning, ISLR]
toc_sticky : true
use_math : true
comments: true
---

4장에서 배운 내용을 바탕으로 한 R 코드를 정리했습니다.



~~~R
library(ISLR)
names(Smarket)
dim(Smarket)
cor(Smarket[,-9])
attach(Smarket)
plot(Volume)
Smarket$Year


#GLM // family = binomial 이 명령어 추가하는 것 이외에는 동일
glm.fit = glm(Direction ~ Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=Smarket, family=binomial)
summary(glm.fit)

#train data를 가지고 예측하기
glm.prob = predict(glm.fit, type ='response') #response를 type으로 주면 로짓 말고 확률 값이 나온다.
glm.prob[1:10]
contrasts(Direction)
#contrasts를 통해 볼 때 up 이 1 이므로 10개의 데이터는 대부분 up으로 분류될 것.


#0 과 1을 실제값인 down와 up으로 코딩하기
glm.pred = rep("Down", 1250) #down으로 크기가 1250인 벡터 만들어라
glm.pred[glm.prob>.5] = "Up"

#table 함수를 통해 예측값과 실제값을 한눈에 보기
table(glm.pred, Direction)
"""
        Direction
glm.pred Down  Up
    Down  145 141
    Up    457 507
"""

#올바르게 분류될 확률
mean(glm.pred==Direction) #0.52 -> train error rate 0.48은 너무 크다!


#2005년도 class만 따로 추출해보기
train=(Year<2005)
Smarket.2005 = Smarket[!train,]
dim(Smarket.2005)
Direction.2005 = Direction[!train]

glm.fit = glm(Direction ~ Lag1+Lag2+Lag3+Lag4+Lag5+Volume, data=Smarket, family=binomial, subset=train) #subset 인자를 통해 일부 데이터만 분석한다.
glm.prob = predict(glm.fit, Smarket.2005, type = 'response')
glm.pred = rep("Down", 252)
glm.pred[glm.prob>.5]="Up"
table(glm.pred, Direction.2005)
mean(glm.pred==Direction.2005) # 0.48 -> train error rate 0.52 오히려 더 나쁜 성능.

#모델 안에 들어있는 변수들 중 유의하지 않은 변수들 때문에 성능이 안좋을 수 있으므로 이를 제거하고 다시 분석해보면 좋을 것이다. 

##LDA
library(MASS)
lda.fit = lda(Direction ~ Lag1+Lag2, data = Smarket, subset=train)
lda.fit
"""
Call:
lda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)

Prior probabilities of groups:
    Down       Up 
0.491984 0.508016 

Group means:
            Lag1        Lag2
Down  0.04279022  0.03389409
Up   -0.03954635 -0.03132544

Coefficients of linear discriminants:
            LD1
Lag1 -0.6420190
Lag2 -0.5135293
"""
#위에서 Coefficients of linear discriminants 에 나오는 값으로 LDA decision rule이 결정된다
# =>> 여기서는 -0.64 X lag1 -0.51 X lag2

lda.pred = predict(lda.fit, Smarket.2005)
names(lda.pred)
#[1] "class"     "posterior" "x"

lda.class = lda.pred$class
lda.pred$posterior

#posterior 를 기준으로 class를 나눌 때 threshold를 다르게 줄 수 있다.
up = lda.pred$posterior[,1]>0.8
aa = rep("Down", 252)
aa[up]="Up"
table(aa,Direction.2005)

##QDA
#LDA와 과정이 모두 같다. 단, fit 에 대한 summary에 coefficients of discriminat는 나오지 않는다. 즉 decision rule을 알기 어렵다.

##KNN
library(class)
train.x = cbind(Lag1, Lag2)[train,]
test.x = cbind(Lag1, Lag2)[!train,]
train.Direction = Direction[train]
knn.pred = knn(train.x, test.x, train.Direction,k=3)
table(knn.pred, Direction.2005)
mean(knn.pred==Direction.2005)


~~~

