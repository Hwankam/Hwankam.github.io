---
layout : post
title : CDA - chap2
subtitle : categorical data analysis
date : 2021-10-27
#categories:
tags : [categorical data]
toc_sticky : true
use_math : true
comments: true
---

# Describing Contingency Tables

### 2.1 Probaility structure for contingency tables

<br>

#### 2.1.1 Contingency Tables
I rows for categories of X  
J columns for categories of Y  
==> IJ possible combinations of outcomes

<br>

#### 2.1.2 Joint/Marginal/Conditional Distributions for Contingency Talbes


<img src='{{"/assets/img/cda2_5.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>


그림에서 보듯 X와 Y가 모두 변수이면 table안에는 $\pi_{12}$ 와 같이 joint probability가 들어가겠지만, 제시된 상황이 X에 대한 Y의 조건부분포라면 아래와 같이 
$\pi_{1|2}$와 같이 표기 될 것이다. 즉 분할표 안의 확률값은 어떤 상황을 table로 바꾸었는지 파악해야 알 수 있다. 

<br>

#### 2.1.3 Example : Sensitivity and Specificity for Medical Diagnoses  

<br>


sensitivity : p(Diagnosis of Test is Positive $\mid$ person has disease)  
specificity : p(Diagnosis of Test is Negative $\mid$ person doesn't have disease)


<img src='{{"/assets/img/cda2_6.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

$\pi_{1\mid1}$ = sensitivity, 
$\pi_{2\mid2}$ = specificity

위와 관련한 자세한 내용은 다른 글에서 다룰 예정

<br>

#### 2.1.4 Independence of Categorical Variables

$\pi_{ij} = \pi_{i+} \pi_{+j} $

<br>

#### 2.1.5 Poisson, Binomial and Multinomial sampling

포아송 -> 다항분포 -> 초기하분포 로 연결되는 관계를 보여준다  

<br>

* Poisson sampling

table 안의 셀 $Y_{ij}$가 모두 독립인 포아송분포 
Poi($\mu_{ij}$)를 따른다고 생각해보자.  

이 때는 table 안의 각 셀 $n_{ij}$ 에 대한 joint probability는 다음과 같다.  


$P(Y_{ij}= n_{ij}) = \Pi_i \Pi_j exp(-\mu_{ij}) \mu_{ij}^{n_{ij}}/n_{ij}! $

<br>

* Multinomial sampling

각 셀들이 포아송분포를 따르는데, 전체 샘플사이즈 n 이 고정되어 있으면

$P(Y_{ij}= n_{ij}) = [n! / (n_{11}! \cdots n_{IJ}!)] \  \ \Pi_i \Pi_j \pi_{ij}^{n_{ij}}  $


<br>

* Independent Multinomial sampling ( = product multinomial sampling)

전체 샘플사이즈 n 이 고정되어 있으며, 설명변수 X에 대해 반응변수 Y가 서로 다른 설명변수에 대해서는 독립적으로 나타날 때  

<a href="https://www.codecogs.com/eqnedit.php?latex=\inline&space;P(Y_{ij}=&space;n_{ij})&space;=&space;\Pi_i&space;[\frac&space;{n_i&space;!}&space;{\Pi_j&space;(n_{ij}!)}&space;\Pi_j&space;\pi_{j|i}^{n_{ij}}]" target="_blank"><img src="https://latex.codecogs.com/svg.latex?\inline&space;P(Y_{ij}=&space;n_{ij})&space;=&space;\Pi_i&space;[\frac&space;{n_i&space;!}&space;{\Pi_j&space;(n_{ij}!)}&space;\Pi_j&space;\pi_{j|i}^{n_{ij}}]" title="P(Y_{ij}= n_{ij}) = \Pi_i [\frac {n_i !} {\Pi_j (n_{ij}!)} \Pi_j \pi_{j|i}^{n_{ij}}]" /></a>

<br>

* Hypergeometric

전체 샘플사이즈 n이 고정되어 있으며, row 와 column 모두 고정되어 있을 때.

<br>

#### 2.1.6 ~ Example

실험 설계의 방법 (1)


1. Prospective - Clinical trials(연구자가 임의로 대상을 특정 군에 넣은 뒤 어떤 현상이 일어나는지 관찰), Cohort studies(연구 대상자가 특정 군을 선택한 뒤 어떠한 결과가 발생하는지 지켜보는 것) - 설명변수 X를 사전에 control 하기 때문에 marginal sum 이 정해져 있는 경우가 많다  ==> independent multinomial 
2. Cross-sectional - 특정군에서 발생하는 현상을 미리 setting 한 뒤에 sample을 동시에 배치하는 것 - total N 이 정해진다 => multinomial
3. Retrospective - 반응변수 Y를 사전에 control 하고 marginal sum이 정해지는 경우가 많다 => multinomial 

아래 표에서 나오는 예시는 case - control study의 예시인데, 여기서는 암환자와 그렇지 않은 환자를 각각 709명씩 모집하고(즉 margianl is fixed) 그들이 흡연자인지 아닌지 살펴보는 retrospective sampling design 이다.

<img src='{{"/assets/img/cda2_7.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

위와 같은 연구에서는 암환자 중에서 흡연자의 비율만 알 수 있기 때문에, Bayes rule을 사용해서 흡연자 중에서 암이 발생할 확률을 알기 위해서는 전체 암 환자의 비율을 알고 있어야 한다.  




실험 설계의 방법 (2)

1. Experimental Study - clinical study - power of randomization, group balance, control the confounding effect
2. Observational Study - case-control, cohort, cross-sectional - bais 에 대한 위험성, causal connection이 생길 수 있음.

<br>

### 2.2 Comparing two proportions

<br>

#### 2.2.1 Difference of Proportions

i 번째 row에 대해서, $\pi_{1|i}$를
$\pi_i$ 라고 표현할 때 첫번째 열에 대한 확률값을 차이를 표현하면 
$\pi_1 - \pi_2$ 이고 이를 difference 라고 한다. 만일 row 들이 동일한 분포를 가지고 있다면 difference = 0 일 것이다.(당연한 것) 
즉 diffence =0 이면 row에 대해서 독립
<br>

#### 2.2.2 Relative Risk

RR = $\pi_i / \pi_j$

difference는 값의 차이만 나타낼 뿐 상대적 크기를 나타낼 수 없는 한계를 가지고 있는데, 이를 보완해서 RR을 만들었다. 
RR =1 이면 독립

<br>

#### 2.2.3 Odds Ratio



* Odds = 성공의 비율 / 실패의 비율

따라서 오즈값을 알면 역으로 성공의 확률을 알 수 있다.


* Odds ratio = odds1 / odds2

joint distributions with cell probabilities {$\pi_{ij}$} 에 대해서 오즈비는 아래와 같이 다양한 형태로 표현된다

$$
\theta = \frac {\pi_{11}/\pi_{12}} {\pi_{21}/\pi_{22}} = \frac {\pi_{11}\pi_{22}}{\pi_{12}\pi_{21}} = \frac {n_{11}n_{22}}{n_{12} n_{21}}
$$

제일 마지막 term 때문에 cross-product ratio 라고도 불림.

<br>

#### 2.2.4 Properties of the Odds Ratio

1. 모든 셀에 대한 odds ratio 값이 1 이면 X와 Y는 서로 독립이다.
2. 오즈비가 1보다 크다면, i 번째 **성공에 대한 오즈**가 j 번째 **성공에 대한 오즈**에 비해 크다 (성공률이 아니라 성공에 대한 오즈에 대한 비율이 오즈비!!)
3. 오즈비와 RR을 구분하자. 예를 들어 오즈비가 3인 것이 $\pi_i = 3\pi_j$을 의미하는 것은 아니다. 
4. 표를 해석하는 방향에 따라 오즈비는 기존 값의 역수값이 될 수 있다. 
5. $log \theta $ 를 사용하면 독립일 때 로그 오즈비 = 0 ;; 표의 해석방향을 다르게 할 시(행 방향이든 열 방향이든 한쪽으로만), 로그오즈비는 부호가 바뀌게 됨. (sign 만 제외하고는 로그오즈비의 값이 같아지기 때문에 해석하기도 더 편하다.)
6. tabel 자체를 transpose해서 생각하면 오즈비는 동일하다. 이러한 성질때문에 오즈비는 RR과 달리 prospective, retrospective, cross-sectional sampling design에서 모두 가능하다. (예를들어, 위의 흡연과 암환자 테이블에서 보면 흡연에 따른 암에 대한 RR을 구하고 싶어도 후행적 연구에서는 구할 수 없다. 그러나 오즈비는 이와 상관없이 항상 동일하기 때문에 후행적연구에서도 구할 수 있는 것이다)
7. table의 특정 행 또는 특정 열의 셀들에서 동일한 비율로 샘플 수를 늘리면 오즈비는 변하지 않는다. 예를들어 열 방향으로 marginal sum이 각각 50, 50 이었는데 첫번째 열에는 1.5를 곱하고 두번째 열에는 2를 곱해서 각 샘플수를 만들었다고 해도 오즈비는 변하지 않는다. (RR와 difference의 차이)

#### 2.2.7 Relationship between Odds Ratio and Relative Risk

Odds ratio = RR * $(1-\pi_2) / (1- \pi_1)$

즉 $\pi_i$ 가 0에 가까울수록 오즈비와 RR의 값은 유사해질 것이다. 

<img src='{{"/assets/img/cda2_7.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

위의 흡연과 암환자 테이블에서, 만약 흡연 여부와 관련없이 암이 생길 확률이 매우 낮다면 오즈비의 값은 RR과 유사할 것이다.

위 테이블에서 오즈비가 약 3인데, 흡연 했을 때 암이 걸릴 확률은 그렇지 않을 경우에 비해 3배 정도 높다고 할 수 있다.  


<br>

### 2.3 Conditional Association in Stratified 2 X 2 Tables

X가 Y에 미치는 영향을 알고싶은데, 어떤 covariate이 X와 Y 모두에 영향을 주는 경우? 이 때를 X와 Y 관계가 confounding을 나타낸다고 말한다. (relationship between X and Y shows confounding)

이런 효과를 제거하기 위해서는 X의 각 다른 level에 대상들을 랜덤하게 할당함으로써 해결해볼 수 있겠으나, 후행적 연구라면 이 또한 불가하다.

따라서 confounding variable을 통제하면서 X와 Y 간의 association을 분석하는 것이 중요하다.



<br>

#### 2.3.1 Partial Tables

three way contingency table을 생각해보자.

Z의 각 수준을 통제한 채로 X와 Y의 관계를 보고자 하는데 이 경우 실제로 보고자 하는 내용은 two way table이 된다.(예를 들어 Z가 1인 경우의 X, Y table을 보는 것) 이를 partial table이라고 한다. 

만약 이러한 partial table을 합치게 된다면 이는 marginal table between x and y 가 될 것이다. 

partial table에서의 association을 conditional association이라고 하는데 이는 marginal table에서의 association과 다를 수 있다. 아래 예시를 잘 보자.

<br>

#### 2.3.2 Simpson's Paradox

아주 유명한 예제이다.

아래 three way table을 3차원으로 표현한 것이 figure 2.1 이다.

<img src='{{"/assets/img/cda2_3.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

<img src='{{"/assets/img/cda2_4.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

3차원 그림은 victim's race를 covariate으로 보고 각각의 case에 대해 conditional association (percentage) 를 보여주고 있다. 이렇게 보면 피해자들의 인종과 관련 없이 피고의 인종이 흑인일 경우에 사형판결을 받을 확률이 더 높아보인다. 그러나 marginal table을 보면 (즉 피해자의 인종을 완전히 무시하면) 피고의 인종이 백인일 경우 사형판결을 받을 확률이 훨씬 높아보인다 왜 이런 일이 발생하는 것일까?  

이는 victim's race 라는 공변량이 X 혹은 Y에 영향을 주기 때문이다. 특히 이렇게 marginal와 conditional의 association이 다르게 나오려면 공변량이 x 혹은 y 에 미치는 영향이 매우 커야함이 증명되었다. 

<br>

#### 2.3.3 Conditional and Marginal Odds Ratios
simpson's paradox를 설명하는 table에서보면, conditional odds ratio는 victim의 각 인종에서 모두 1보다 작다. 
$\hat \theta_{XY(1)} = 0.43$ 이므로 (1,1) 원소인 백인이 사형판결받는 case의 odds 값이 흑인 case odds의 43% 밖에 되지 않음을 알수있다.

그러나 margianl odds를 비교해보면 그 값이 1을 넘어선다. 

이 역시 conditional association와 marginal association을 잘 구분해야함을 말한다. 

<br>

#### 2.3.4 Marginal independence VS Conditional independence

X and Y are siad to be conditionally independence given Z when they are conditionally independent **at every level of Z**

$$
P(Y = j | X = i , Z = k ) = p(Y = j | Z = k), \ \ \forall i,j
$$


이를 이용하면 conditional independence 가 성립하는 경우

$$
\pi_{ijk} = P(X=i, Z=k)P(Y=j | Z=k) = \pi_{i+k}\pi_{+jk} / \pi_{++k}
$$

임이 증명된다.

또한 앞서 simpson's paradox에서 계속 설명한 것처럼, 여기서도 conditional independence와 marginal indepence는 서로 동일한 개념이 아님을 보일 수 있다.

참고 : marginal independence = $\pi_{ij+} = \pi_{i++}\pi_{+j+}$




<br>

#### 2.3.5 Homogeneous Association

만약 X와 Y의 association이 homogeneous 이면

$$
\theta_{XY(1)} = \theta_{XY(2)} = \theta_{XY(3)} = \cdots =\theta_{XY(k)} 
$$


<br>

### 2.4 Measuring Association in I X J tables


2X2 table에서는 odds ratio를 통해서 변수간의 요약정보를 전달할 수 있었다. 그러나 일반적인 I X J table에서는 odds ratio를 사용하는 경우 일부 변수간의 관계는 고려되지 못한다.

이를 어떻게 해결할 수 있을까?

<br>

#### 2.4.1 Odds ratios in IXJ tables

첫 번째 방법으로, odds ratio의 set을 모두 살펴보는 것이다.  

여기서 알 수 있는 오즈비의 성질을 살펴보자  

I X J table에서는 $\binom{I}{2} \binom{J}{2}$ 개의 odds ratios 가 존재한다.  

local odds ratio를 아래와 같이 정의하면 

$$
\theta_{ij} = \frac {\pi_{i,j} \pi_{i+1,j+1}}{\pi_{i,j+1} \pi_{i+1,j}}
$$

local odds ratio는 (I-1)(J-1) 개 존재한다. 이를 활용해서 원하는 변량 간의 오즈비를 곱셈을 통해 구할 수 있다.

예를 들어 $\alpha_{ij}$ 를 X의 i level 와 I 번째 level,  Y의 j 번째 level와 J 번째 level 간의 오즈비라고 할 때

$$
\alpha_{ij} = \frac {\pi_{i,j} \pi_{I,J}}{\pi_{i,J} \pi_{I,j}} = \theta_{i,j}\theta_{i+1,j}\cdots\theta_{I-1,J-2}\theta_{I-1,J-1}
$$

로 표현할 수 있다. 이로부터, 오즈비를 만드는 방법은 유일하지 않음을 알 수 있다.

또한 변수가 독립이라면 (I-1)(J-1) 개의 오즈비는 모두 1이 나올 것이다.
<br>

#### 2.4.2 Association Factors

두 번째 방법으로는 모두 독립일 때를 가정할 때의 기대값과 실제 데이터값을 비교해보는 것이다.  

*association factor* 를 아래와 같이 정의한다

$$
\pi_{i,j} / (\pi_{i+}\pi_{+j})
$$

이 값이 1이면 level 간 독립이 성립하는 것을 의미하며 양 극단 값인 0 와 $min(1/\pi_{i+}, 1/\pi_{j+})$ 로 갈수록 독립 관계에서 멀어진다 할 수 있다.


<br>

#### 2.4.3 Summary Measures of Association

association을 보기 위해서 새로운 summary index 를 설정할 수도 있다.

아래와 같은 식을 생각할 수 있을 것이다.

$$
\frac {V(Y)-E[V(Y|X)]}{V(Y)}
$$

여기서 $V(Y)$는 marginal distribution인 
${\pi_{+j}}$ 의 분포에 대한 분산을 나타낸다.

명목형 반응변수에 대해서 variation 에 대한 measure로 $V(Y) = \sum _j \pi_{+j} log \pi_{+j}$ 를 설정하면
이는 entropy를 나타내게 되고, cross entropy인 

$$
\frac{\sum _j \pi_{+j}log \pi_{+j} - \sum_i \pi_{i+} \sum _j \pi_{j|i} log \pi_{j|i}}{\sum _j \pi_{+j}log \pi_{+j}} = - \frac {\sum_i\sum_j\pi_{i,j}log(\pi_{i,j}/\pi_{i+}\pi_{+j})}{\sum _j \pi_{+j}log \pi_{+j}}
$$

값을 *uncertainty coefficient* 라고 한다.  
(분할표의 특성상; 이 값이 1이라면 조건부 확률인 $\pi_{j|i}$에 대해서 
$ \forall i, \exists j, \pi_{j|i}=1$ ;; 이 값이 1이면 X와 Y는 서로 독립이다라고 말할 수 있다)

<br>

#### 2.4.4 Ordinal Trends

위에서는 nominal data에 대한 association을 살펴보았다. 그럼 ordinal 데이터는 어떻게 보아야 할까?  

ordinal 데이터에 대해서는 concordant 와 discordant 를 살펴볼 수 있다. 

*concordant* : x 의 rank가 높아질 때 , y 의 rank도 높아짐을 의미 $\Leftrightarrow$ *discordant*

아래 table에서 total number of concordant를 C ; total number of discordant를 D라고 하면

<img src='{{"/assets/img/cda2_1.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

C = 34(174+304+75+172) + 53(304+172) + 80(75+172) + 174(172)  
D = 88(80+174+29+75) + 53(80+29) + 304(29+75) + 174(29)

C > D 인 것을 알 수 있으므로 나이가 많을수록 직무 만족도가 크다는 것을 알 수 있다.


concordant와 discordant의 확률값은 

<img src='{{"/assets/img/cda2_2.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

pair로 계산하기 때문에 2가 붙은 것을 알 수 있다.

<br>


#### 2.4.5 Ordinal Measure of association : Gamma

위에서 구한 total number of concordant and discordant를 가지고 gamma값을 구할 수 있다

$$
\gamma = \frac {\Pi_c - \Pi_d} {\Pi_c + \Pi_d}
$$

이 gamma 값은 상관계수와 유사하다.
1. -1와 1 사이의 값을 가지고 있다.
2. ordering이 반대로 된 ordinal 데이터의 경우 gamma값의 부호가 바뀔 것이다.  
3. 상관계수는 perfect linear 이면 절댓값 1을 갖지만 gamma는 monotonicity 가 성립하면 절댓값이 1이다. 
4. 독립이면 gamma =0 인데 역은 성립하지 않는다.

이 개념을 연속형 변수로 확장하면 상관관계에 대한 비모수 추론인 Kendall's tau value를 얻게 된다. 

<br>

#### 2.4.8 Correlation for underlying Normality

ordinal variable의 경우, 각 셀에 특정 값을 준다던가 혹은 중위수를 선택해서 pearson correlation formula 를 사용할 수도 있을 것이다. 

<br>
