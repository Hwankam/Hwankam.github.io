---
layout : post
title : Computer Age Statistical Inference - chap 2
subtitle : statistical method 2
date : 2021-10-20
#categories:
tags : [datascience, statistical method, Efron]
toc_sticky : true
use_math : true
comments: true
---

# Frequentist Inference

estimator : function  
estimate : estimator의 realization (single number)

즉 estimate은 실제로 구현된 값들이고 이 값의 error가 어느정도인지 알고 싶을 땐 estimator를 보아야한다. (그럼 estimator는? data로부터!)

Frequnetist says **the accuracy of an observed estimate $ \hat \theta $ is the probabilistic accuracy of $\hat \Theta$ as an estimator of $\theta$**

<br>

### Frequentist in practice

1. plug - in

2. Taylor series approximations  
e.g. $ \bar X^2 = \bar x + 2 \bar x (\bar X - \bar x) $      ->    $ se(\bar X ^2) = 2 |\bar x| se(\bar X) $

3. parametric family and Maximun likelihood theory

4. simulation and the boosting  
i.e. value $\hat \Theta^{(k)} = t(X^{(k)})$  simulated from $\hat F$, $\hat \Theta^{(k)}$ 들의 표준편차로 모 표준편차를 추정

5. Pivotal statistics  
i.e. Normal 가정하에 파라미터 값에 상관없이 t statistic 이 만들어지는 것.  
e.g. 95% 신뢰구간에 대한 정의 : 신뢰구간이 모수값을 포함하는 것에 대해 95% confident  




<br>

### Frequentist Optimality

- parametric model  
MLE는 minimum (asymptotic) standard error 관점에서 optimum estimate 이다.  
정보량 부등식 (CRLB)에 의하면 $Var_{\theta}(\hat \eta_n ) \geq (\frac {\partial} {\partial \theta}E_{\theta}(\hat \eta_n )^T[n \mathbf(I)(\theta)]^{-1} (\frac {\partial} {\partial \theta}E_{\theta}(\hat \eta_n ) $ 인데, MLE 는 점근적으로 unbiased 이고 점근적으로 최소분산이다. 

- hypothesis testing  
Neyman Pearson lemma에 의해 단순가설에서

$$
t_c(x) = \Big \{ \ \ \ \ \begin{align} &1 \ \ if  \ \ log \frac {f_1(x)} {f_0(x)} \geq c \\ &0 \ \ if  \ \ log \frac {f_1(x)} {f_0(x)} < c \end{align}
$$ 

는 $E_{\theta_0}(t_c(x)) = \alpha $ 일 때 power를 최대로 하는 검정이 가능하다. 


(일반적인 가설에서는 전역최강력 검정을 구하게 되는데, 이때는 귀무가설하의 검정함수 기댓값의 max 가 type 1 error 보다 작게 맞춘 상태에서 power를 최대로)

<br>

figure 2.2 >

c 값을 0.75 로 주었을 때의 type 1 error 와 type 2 error를 보여준다. Neyman Pearson lemma를 따르지 않는 검정은 모두 검은 색 선 기준으로 더 위쪽에 존재하게 된다.  
