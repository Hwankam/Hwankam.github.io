---
layout : post
title : PRML-4
subtitle : PRML
date : 2022-01-19
#categories:
tags : [PRML, Machine learning]
toc_sticky : true
use_math : true
comments: true
--- 


## 4. Linear Models for Classification

이번 chapter는 "Input space를 K개의 Class로 나누는 것"이 핵심이다. 이때 나눠지는 영역은 decision region, 나누는 boundary를 decision boundary 혹은 decision surface라 한다. 특히 이번 chapter에서 중요한 것은 linear model이다. 즉 분류를 위한 decision surface가 linear function이며 D-dimensional input space를 (D-1)-dimensional hyperplane으로 나누는 것을 의미한다. 

<br>

linear model을 활용한 classification에서 주목하는 세 가지 접근방식은 다음과 같다. 

<font color="blue"> three different approaches to the classification </font>


1. Discriminant function ( X를 어떤 class에 할당할지 결정 )

inference stage ( set conditional prob dist $p(C_k|\mathcal{x})$ ) 
$\&$ 
decision stage (use dist to make decision ) 이렇게 두 가지 step으로 나눠서 classification을 할 때 condition prob dist를 어떻게 정할 것인지에 따라 또 나뉜다. 

2. parametric model ( 최적 parameter 찾기 )

3. generative approach ( Bayes' rule -> posterior)

<br>


그러나 classification에서는 linear model의 결과값이 discrete class label과 관련 있어야 하기 때문에 비선형함수인 activation function을 사용한다. (**generalized** linear model )

더 나아가, input space $\mathcal{X}$를 
basis function $\phi(\mathcal{x})$을 활용해 확장하고 있는데 이를 통해 좀 더 유연한 모델을 만들어 낼 수 있을 것이다.


<br>

### 4.1 Discriminant Functions

Discriminant 라는 것은 말 그대로 구분식! 즉, input $\mathcal{X}$를 어떤 Class에 넣을 것인지 결정해준다! 여기서는 특별히 linear discriminants 에 한정해서 설명하도록 하겠다. 



"linear discriminants = decision surfaces are hyperplanes"


<br>

#### 4.1.1 Two Classes


<font size=4> [Simplest representation of a linear discriminant]</font>

$$
y(\boldsymbol{x}) = \boldsymbol{w}^T\boldsymbol{x} \ + \ w_0
$$

* $y(\boldsymbol{x})=0$ 은 decision boundary. 

* $\boldsymbol{w}$ 는 weight vector, decision boundary에 직교하는 벡터, decision boundary 방향 결정
* $\ w_0$ 는 bias ->  negative bias는 threshold. 


<font color = "blue"> negative bias 가 threshold 인 이유

* $y(\boldsymbol{x}) >0 $ 이면 Class 1 으로 판정하기 때문 

* $\boldsymbol{x}$가 decision boundary에 위치할 때, 원점에서 decision boundary 까지 거리는

$$
\frac {\boldsymbol{w}^T\boldsymbol{x}}{||\boldsymbol{w}||} = \frac{-w_0}{||\boldsymbol{w}||}
$$

​	$\because$ x' 을 원점에서 가장 가까운 decision boundary 위의 점이라고 하면, boundary의 직교 벡터 $\boldsymbol{w}$ 을 활용할 때, 다음이 성립한다

$$
\begin{align}
&\mathcal{x}' = \alpha \boldsymbol{w} \ \Rightarrow \boldsymbol{w}^T \cdot \alpha \boldsymbol{w} + w_0 = 0 \ \Rightarrow \alpha = \frac {-w_0}{||\boldsymbol{w}||^2} \\
& ||\mathcal{x}'|| = ||\alpha \boldsymbol{w}|| = \frac{-w_0}{||\boldsymbol{w}||}
\end{align}
$$

</font>

binary class 이므로 boundary decision 을 기준으로 $y(\boldsymbol{x})$ 의 부호가 결정되고, 부호값을 통해 class를 판정하게 된다. 
따라서 input X가 boundary를 기준으로 얼마나 떨어져 있는지에 대한 수직거리와 방향을 아는 것이 중요한데 이를 $\gamma \cdot \frac {\boldsymbol{w}}{||\boldsymbol{w}||}$라고 하자. 그 결과 아래와 같은 그림을 얻을 수 있다.


<img src='{{"/assets/img/prml-4-1.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>


$$
\boldsymbol{x} = \boldsymbol{x}_{\bot} + \gamma \cdot \frac {\boldsymbol{w}}{||\boldsymbol{w}||}\\
\gamma = \frac{y(\boldsymbol{x})}{||\boldsymbol{w}||}
$$

특히 $\gamma$의 부호 값이 중요하므로, 위와 같이 구하는 것 같다. 



추가적으로 식을 간편하게 적기 위해dummy input을 활용해 bias를 파라미터로 함께 표현할 수 있다. $\boldsymbol{\tilde w} = (w_0, \boldsymbol{w})$ 에 대해, 
$y(\boldsymbol{x}) = \boldsymbol{\tilde w}^T \boldsymbol{\tilde x}$ 로 표기하면 hyperplane( boundary)는 원점을 통과하는 D - dimensional hyperplane 이다. 



<br>



#### 4.1.2 Multiple Classes

binary class에서 확장해서 multiple class 인 경우를 생각해보자.

기존 binary class에서 하던 방식을 그대로 사용할 경우 올바른 분류가 어려울 수 있는데 이유는 아래 그림을 보면 잘 나타난다.



<img src='{{"/assets/img/prml-4-2.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>




이를 피하기 위해서는 아래와 같이 단순하게 K-class discriminant 를 설정할 수 있다.

$$
\begin{align}
&y_k(\boldsymbol{x}) = \boldsymbol{w}_k^T\boldsymbol{x} \ + \ w_{k0} \\
&\textrm{assign x to} \ \ C_k \ \ \ \ \textrm{if} \ \ y_k(\boldsymbol{x}) > y_j(\boldsymbol{x}) \ \textrm{for all } j \neq k
\end{align}
$$


binary class 와 유사하게 생각한다면 class k 와 class j 사이의 boundary는 $y_k(\boldsymbol{x}) = y_j(\boldsymbol{x})$ 를 만족해야하므로 아래와 같다.

$$
(\boldsymbol{w}_k - \boldsymbol{w_j})^T \boldsymbol{x} + (w_{k0} - w_{j0}) = 0
$$



또한 linear discriminant 에 의해 나눠진 region은 convex set 이다. (증명 생략)

<br>



#### 4.1.3 Least squares for classification

linear discriminant의 parameter 값을 찾기 위한 세 가지 방법을 제시한다. 

첫번째 방법은 Least square 이다.

정규성을 가정한 선형회귀분석에서 가장 많이 쓰이는 방법으로 quadratic error를 사용하므로 예측값은 
$E(\boldsymbol{t}|x)$이다

그러나 선형모델 자체가 flexibility가 부족하기 때문에 binary class인 경우에도 
$E(\boldsymbol{t}|x)$ 값이 1을 초과할 수 있다. 


multiple class에서도 Least square를 사용하기 위해 행렬을 사용해 식을 확장해보자. 

개별 linear discriminant function 
$ y_k(\boldsymbol{x}) = \boldsymbol{w}_k^T \boldsymbol{x} \ + \ w_{k0} $ 에 대해 다음과 같이 나타낸다.

$$
y(\boldsymbol{x}) = \boldsymbol{\tilde W}^T\boldsymbol{\tilde x}
$$

이 때 $\boldsymbol{\tilde W}$ 는 k(class)개의 column을 가지고 있으며 
열벡터가 $\boldsymbol{\tilde w}_k = (w_{k0}, \boldsymbol{w}_k ^T)^T$ 인 행렬이다.

Least square에 의해 $\boldsymbol{\hat {\tilde W}} = (\boldsymbol{\tilde X}^T\boldsymbol{\tilde X} )^{-1}\boldsymbol{\tilde X}^T \boldsymbol{T} = \boldsymbol{\tilde X}^{\dagger} \boldsymbol{T} \Rightarrow y(\boldsymbol{x}) = \boldsymbol{\hat {\tilde W} }^T \boldsymbol{\tilde x}$

이로부터 얻은 벡터 $y(\boldsymbol{x})$ 에 대해서 그 원소들 중 가장 큰 값의 class로 예측하게 된다. (제약식을 통해 y(x)의 sum이 1이 되게 하고 벡터의 각 원소들이 0과 1 사이가 되도록 하면 model output을 probabilistic 하게 해석할 수 있다.)

Least square는 closed form으로 해를 구할 수 있는 만큼 매우 간단하고 좋은 방법이지만 outlier에 민감하게 반응하며 가우시안을 가정한 경우에는 LS와 ML 방식의 결과가 동일한데, gaussian이 아닌 여러가지 분포들 대해서는 LS를 사용할 때 바람직한 결과를 얻지 못할 우려가 있다는 것도 알아둬야 한다. 

<img src='{{"/assets/img/prml-4-4.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

<img src='{{"/assets/img/prml-4-5.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

<br>

#### 4.1.4 Fisher's linear discriminant

linear discriminant의 parameter 값을 찾기 위한 두 번째 방법은 Fisher's linear discriminant 이다. 이 방법의 핵심은 바로 차원 축소에 있다. 

X : D-dimension $\Rightarrow $ 
one-dimension  : 
$y = \boldsymbol{w}^T \boldsymbol{x}$

차원 축소는 본질적으로 정보의 손실을 막을 수 없으며, 본래 D 차원의 공간이 1차원(line)으로 축소되며 overlapping이 발생할 수밖에 없다. 이를 보완하기 위해 집단 간 차이는 최대로 하되, 집단 내부의 분산은 최소화되도록 하는 projection을 선택해야 한다.

<font color="green">

* 집단 간 차이 최대

적절한 weight vector $\boldsymbol{w}$를 잘 선택해서 class separation을 최대로 하는 projection을 선택한다면 정보의 손실을 어느정도 보완할 수 있다. 즉, 볼드체의 m 이 분류된 집단의 평균값이 라 할 때

$$
m_2 - m_1 = \boldsymbol{w}^T(\boldsymbol{m}_2 - \boldsymbol{m}_1) \ \ \ \ \ \ \textrm{where} \ m_k = \boldsymbol{w}^T \boldsymbol{m_k}  \ \ \ \ \textrm{(mean of projected data from class)}
$$

값을 최대화하는 길이 1의 벡터인 $\boldsymbol{w}$ 를 찾으면 된다
(길이를 1로 둠으로써 $m_2-m_1$이 무한정 커지진 않도록 제약을 준다) 

Lagrange multiplier를 사용하면 다음과 같은 결과를 얻는다

$$
\boldsymbol{w} \propto (\boldsymbol{m}_2 - \boldsymbol{m}_1)
$$


* 집단 내 분산 최소

within class variance of transformed data from class $C_k$는 다음과 같이 정의한다.

$$
s_k^2 = \sum_{n \in C_k} (y_n - m_k)^2 \ \ \ \ \textrm{where} \ y_n = \boldsymbol{w}^T\boldsymbol{x_n}
$$

이 때 binary classification 의 경우 total within class variance는 $s_1^2 + s_2^2$

</font>

<br>

이를 만족하는 projection vector w를 찾기 위해 Fisher는 다음과 같은 기준을 제시했다.

$$
\begin{align}
&\mathcal{J}(\boldsymbol{w}) = \frac{(m_2 - m_1)^2}{s_1^2 + s_2^2} = \frac{\boldsymbol{w}^T \boldsymbol{S}_B \boldsymbol{w}}{\boldsymbol{w}^T \boldsymbol{S}_W\boldsymbol{w}} \\ \\

&S_B = (\boldsymbol{m}_2 - \boldsymbol{m}_1)(\boldsymbol{m}_2 - \boldsymbol{m}_1)^T\\
&S_W = \sum (\boldsymbol{x}_n - \boldsymbol{m}_1)(\boldsymbol{x}_n - \boldsymbol{m}_1)^T + \sum (\boldsymbol{x}_n - \boldsymbol{m}_2)(\boldsymbol{x}_n - \boldsymbol{m}_2)^T
\end{align}
$$

이 때 J 를 w 로 미분하면 $\mathcal{J}(\boldsymbol{w})$는 
$(\boldsymbol{w}^T \boldsymbol{S}_B \boldsymbol{w})\boldsymbol{S}_W\boldsymbol{w} = (\boldsymbol{w}^T \boldsymbol{S}_W\boldsymbol{w})\boldsymbol{S}_B \boldsymbol{w}$ 를 만족할 때 최대가 됨을 알 수 있다. 
$S_B$ 식으로부터 
$S_B\boldsymbol{w}$ 는 $\boldsymbol{m}_2 - \boldsymbol{m}_1$ 와 비례함을 알 수 있으므로 
$\boldsymbol{w} \propto S_W^{-1}(\boldsymbol{m}_2 - \boldsymbol{m}_1)$ 이다. 
이를 만족하는 $\boldsymbol{w}$를 Fisher linear discriminant 라고 한다. (사실 discriminant 라기 보다는 projection direction을 의미한다고 말하는게 더욱 맞다)

<img src='{{"/assets/img/prml-4-6.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

<br>



#### 4.1.5 Relation to least squares 

+ Least -Square : target value를 최대한 정확하게 맞출 수 있을지(error를 최소화)

+ Fisher-criterion : target의 class를 정확하게 분류할 수 있을지(maximum class separation)



그러나 1 - of -K coding (K class 중 가장 높은 확률값을 가지는 class 를 선택)이 아닌 다른 target coding scheme을 사용할 때, Least - square 와 Fisher solution은 동일하게 생각할 수 있다. 아래는 그 방법을 나타낸 것이다.



<font size=4> [different target coding scheme for binary] </font>

target for class $C_1$ 
to be $N/N_1$, 
target for class $C_2$ 
to be $- N/N_2$

라고 두면 sum of squares error function은 다음과 같다

$$
E = \frac{1}{2} \sum_{n=1}^N (\boldsymbol{w}^T \boldsymbol{x}_n + w_0 - t_n ) ^2
$$

이를 $\boldsymbol{w}$ 와 $w_0$로 미분하면 

$$
w_0 = -\boldsymbol{w}^T \boldsymbol{m} \\
(S_W + \frac{N_1 N_2}{N}S_B)\boldsymbol{w} = N(\boldsymbol{m}_1 - \boldsymbol{m}_2)
$$

이므로 $S_B \boldsymbol{w}$가 
$\boldsymbol{m}_2 - \boldsymbol{m}_1$의 방향을 나타내는 것을 고려할 때, 결국 Least-square에서도 Fisher - criterion에서 얻은 
$\boldsymbol{w} \propto S_W^{-1}(\boldsymbol{m}_2 - \boldsymbol{m}_1)$ 식을 얻게 됨을 알 수 있다.

<br>



#### 4.1.6 Fisher's discriminant for multiple classes

앞서 Fisher-criterion을 설명할 때는 binary class를 기반으로 input space 차원 축소를 1차원으로 했다면 multiple class 의 경우 inpute space 차원 축소를 D' 차원으로 해 볼 것이다. multiple class 이므로 weight vector $\boldsymbol{w}$ 를 column으로 하는 행렬 
$\boldsymbol{W}$를 생각하자. 즉 모델은 아래와 같다.

$$
y =\boldsymbol{W}^T \boldsymbol{x}
$$


within-class covariance matrix to the case of K classes

$$
S_W = \sum ^K S_k \ \ \textrm{where} \ \ S_k = \sum (\boldsymbol{x}_n - \boldsymbol{m}_k)(\boldsymbol{x}_n - \boldsymbol{m}_k)^T \ \ \& \ \ \boldsymbol{m}_k = \frac{1}{N_k}\sum_{n \in C_k}\boldsymbol{x_n}
$$


Total covariance matrix

$$
S_T = \sum_{n=1}^N (\boldsymbol{x}_n - \boldsymbol{m})(\boldsymbol{x}_n - \boldsymbol{m})^T
$$


Between class covariance

$$
S_B = \sum ^K N_k(\boldsymbol{m}_k - \boldsymbol{m})(\boldsymbol{m}_k - \boldsymbol{m})^T
$$

세가지 공분산 행렬은 $S_T = S_B + S_W$ 관계를 만족한다.



D' 차원으로 projection한 데이터에 대해서도 유사한 공분산 행렬(소문자 s 사용)을 정의할 수 있다.



binary class와 유사하게 Fisher-criterion $\mathcal{J}(\boldsymbol{W}) = Tr(s_W^{-1}s_B)$ 로 구할 수 있고 projection 이전의 데이터를 사용할 때 
$\mathcal{J}(\boldsymbol{w}) = (\boldsymbol{W}^T \boldsymbol{S}_W \boldsymbol{W})^{-1} \boldsymbol{W}^T \boldsymbol{S}_B\boldsymbol{W}$ 이다.



즉 weight vlaue ($\boldsymbol{W}$)는 
$S_W^{-1}S_B$ 의 eigenvector에 의해 결정되고 ??

<br>

#### 4.1.7 The perceptron algorithm

...

<br>


### 4.4 The Laplace Approximation

Bayesian treatment of logistic regression 에서는 
$p(C_k | \boldsymbol{x})$가 
더이상 Gaussian이 아니므로 적분이 어렵다. 따라서 Laplace apporximation을 통해서 확률밀도에 대한 Gaussian approximation을 하려고 한다. 

알고 싶은 확률밀도 $p(z)$ 에 대해
$f(z)$를 활용해 근사한다고 하자.  

$$
p(z) = \frac {1}{Z}f(z)
$$

step 1 : $p(z)$ 의 mode 
값 $z_0$을 찾는다


step 2 : $ln f(z)$를 mode인 
$z_0$에 대해 Talyor expansion 한다

$$
ln f(z) \ \simeq \ ln f(z_0) - \frac{1}{2}(z - z_0)^TA(z - z_0) \\


f(z) \ \simeq \ f(z_0)exp\{ - \frac{1}{2}(z - z_0)^T A (z- z_0)  \} \\


\textrm{where} \ \  A = - \nabla \nabla ln f(z) |_{z=z_0} 
$$

step 3 : Gaussian 에 근거해서 normalized dist q(z) 로 근사한다

$$
q(z) = (\frac{|A|^{1/2}}{(2\pi)^{M/2}}) exp\{ - \frac{1}{2}(z - z_0)^T A (z- z_0)  \} 
$$


(단, precision matrix인 A는 positive definite 이어야 한다.)



데이터가 많아지면 Gaussian으로의 근사가 더욱 효과적일 것이다(CLT)

그러나 1. multimodal 인 경우 위의 Laplace apporximation은 한계가 있다. 2. 또한, Gaussian을 사용해서 근사를 하기 때문에 real variable로 정의된 함수에 대해서만 근사가 가능하다(즉 양수인 경우 로그변환이 필요). 3. 무엇보다 distribution에만 집중에서 global properties를 간과하지 말아야한다.(?) 



<br>

#### 4.4.1 Model comparison and BIC


$p(z)$를 근사하기 위해서는 normalization constant Z에 대해서도 알아야한다. 
Gaussian approximation $f(z)$에 대해서 다음이 성립한다. 

$$
\begin{align}
Z & = \int f(z) dz \\
& \simeq f(z_0)\int exp\{ - \frac{1}{2}(z - z_0)^T A (z- z_0)  \} dz \\
& = f(z_0) \frac{(2\pi)^{M/2}}{|A|^{1/2}}
\end{align}
$$



또한 모델의 집합 $\{ \mathcal{M}_i  \}$ 에 대해 
각 모델의 parameter를 $\{ \theta_i  \}$ 라고 할 때 
model evidence $p(\mathcal{D} | \mathcal{M}_i)$ 는 다음과 같이 쓸 수 있다.

$$
p(\mathcal{D}) = \int p(\mathcal{D}|\theta) p(\theta) d\theta = \int f(\theta) d\theta
$$

normalization constant 근사식을 활용할 때, 위 식을 다시 써보자.


$$
p(\mathcal{D}) = f(\theta_{MAP})\frac{(2\pi)^{M/2}}{|A|^{1/2}} = p(\mathcal{D}|\theta_{MAP}) p(\theta_{MAP}) \frac{(2\pi)^{M/2}}{|A|^{1/2}} \\

ln \ P(\mathcal{D}) \simeq ln \ p(\mathcal{D}|\theta_{MAP}) + \ln p(\theta_{MAP}) + \frac{M}{2} ln \ 2\pi - \frac{1}{2} ln |A|
$$

첫 번째 term은 log-likelihood 이고 두 번째 term 부터는 model complexity term이다. 그리고 matrix A는 posterior에 대한 Hessian이다.  


이 때, Gaussian prior의 분산을 매우 크게 하면 상수처럼 취급될 수 있으므로 positive definte matrix A 에 대해 다음이 성립한다.

$$
ln \ P(\mathcal{D}) \simeq ln \ p(\mathcal{D}|\theta_{MAP}) - \frac{M}{2} ln \ N + C
$$

즉 이는 model selection에서 지표로 사용하는 BIC와 동일한 form임을 알 수 있다.(사실 $-2 ln \ p(\mathcal{D}$) 가 BIC 와 form이 완벽히 일치한다.

<br>

### 4.5 Bayesian Logisitic Regression

sigmoid 함수에 대한 적분이 쉽지 않기 때문에, logistic regression 에 대한 Bayesian inference는 쉽지 않다. 이를 해결하기위해 Laplace approximation을 사용할 것이다.


<br>

#### 4.5.1 Laplace approximation

posterior에 대한 Gaussian representation을 행할 것이므로 prior를 Gaussain으로 생각해보자.

$$
p(w) = \mathcal{N}(w|m_0, S_0)
$$

이때 posterior는 다음과 같다.

$$
\begin{align}

&p(w|\boldsymbol{t}) \propto p(w)p(\boldsymbol{t}|w) \\ 

&ln \ p(w|\boldsymbol{t}) = -\frac{1}{2}(w-m_0)^T S_0 ^{-1}(w-m_0) + \sum {t_n ln  \ y_n + (1-t_n) ln \ (1-y_n)} + C \\

&\textrm{where} \ \ \ \  p(\boldsymbol{t}|w) = \prod y_n^{t_n} \{1- y_n \}^{1-t_n}

\end{align}
$$


Laplace approximation의 논리와 동일하게 MAP 값으로 Taylor expansion 하면 posterior 에 대한 Gaussian 근사는 다음과 같다

$$
q(w) = \mathcal{N}(w|w_{MAP}, S_N) \\

\textrm{where} \ \ S_N = - \nabla \nabla ln \ p(w| \boldsymbol{t})
$$

<br>


#### 4.5.2 Predictive distribution

이제 predictive distribution을 생각해보자. 새로운 feature vector $\phi(x)$ 가 주어졌을 때 
class $\mathcal{C_1}$ 에 대한 predictive distribution은 다음과 같이 정의 및 근사 된다.

$$
p(\mathcal{C}_1|\phi, \boldsymbol{t}) = \int p(\mathcal{C}_1|\phi, w) \ p(w|\boldsymbol{t}) dw \simeq \int \sigma(w^T \phi) \ q(w) dw 
$$


이때 Gaussian의 marginal dist 또한 Gaussian이므로 $w$ 를 marginalized out 시킨 
$p(a) = \int \delta(a - w^T\phi) q(w) dw$ 를 활용해서 아래와 같이 적을 수 있다.

$$
p(\mathcal{C}_1|\boldsymbol{t}) \simeq \int \sigma(a) p(a) \ da = \int \sigma(a) \mathcal{N}(a|\mu_a, \sigma_a^2) \  da
$$

위 식은 Gaussian과 sigmoid의 convolution을 나타낸 것이므로 계산하기가 까다로우나, sigomid function 대신 probit function을 사용하면 analytic한 결과를 얻을 수 있다.


$$
p(\mathcal{C}_1|\boldsymbol{t})  \simeq  \int \Phi(\lambda a) \ \mathcal{N}(a|\mu_a, \sigma_a^2) \  da \simeq \sigma(\kappa(\sigma_a^2)\mu_a)

$$

"Marginalization of the logistic sigmoid model under a Gaussian approximation to the posterior dist will be illustrated in the context of variational inference in Fig 10.13"

<br>


