---
layout : post
title : CDA - chap1
subtitle : categorical data analysis
date : 2021-10-25
#categories:
tags : [categorical data]
toc_sticky : true
use_math : true
comments: true
---


# Introduction : Distributions and Inference for Categorical Data

<br>

### 1.1 Categorical Response Data
범주형 자료의 다양한 예시 e.g. classification, how good a product

<br>

#### 1.1.1 Response-Explanatory variable Distinction

설명변수의 데이터 형태는 다양하다!

<br>

#### 1.1.2 Binary-Nominal-Ordinal Scale Distinction

1. binary - two categories 
2. nominal - more than two categories without ordering 
3. ordinal - ordered categories - distances between categories are unknown - e.g. patient condition
4. interval variable - numerical distances exist - e.g. annual income
+
5. discrete interval variable
6. continuous variable

어떻게 변수를 측정하는지에 따라 같은 자료에 대해 여러가지 형태의 변수를 만들 수 있다.

binary ~ interval variable 은 lower level 에서 higher level로 정렬되어 있는데, 낮은 수준에서 사용한 분석법은 높은 수준의 변수에서도 사용가능 (역은 안됨) 

<br>

#### 1.1.3 Discrete-Continuous Variable Distinction

discrete 와 continuous 의 차이는 셀수 있는지 여부. discrete 변수의 값이 너무 많으면 그건 연속형 변수로 보겠다!

<br>

#### 1.1.4 Quantitative - Qualitative Variable Distinction

Nomial - Qualitative

oridnal - fuzzy (순서형 자료의 경우 크고 작고는 있으므로 이를 활용해서 양적변수처럼 인식할 수도 있을 것이다 => scale을 잘 활용해서 numerical score를 할당하면 가능할 것)

interval - Quantitative

<br>

### 1.2 Distributions for Categorical Data

1. binomial ---- binary obs 가 독립이 아니라면 hypergeometric
2. Multinomial ----  $E(n_j) = n \pi_j$ , $var(n_j)=n \pi_j ( 1- \pi_j)$ , $cov(n_j, n_k) = -n \pi_j \pi_k$
3. Poisson ---- binomial 에서 n이 매우 크고 $\pi$ 가 매우 작을 때 포아송 근사 가능. 포아송분포는 평균값이 증가면서 정규근사 됨.

<br>

#### 1.2.4 Overdispersion

정의 : 특정 분포를 가정하고 관측을 했을 때, 매우 특이한 이상치들이 나올 수 있다. 

상황 : 
1. 특정 상황의 변화에 따라 분포의 모수가 바뀔 수 있음(베이지안 관점). 
2. 분포가 서로 다른 모수를 가진 binomial 분포의 mixture 분포 


해결 : Quasi-Likelyhood and GLM (chapter 4.7)

일반적으로 GLM에서 score function을 구하면 다음과 같다

exponential dispersion family 

$$
f(y ; \theta_i, \phi) = exp(\frac {y_i\theta_i - b(\theta_i)} {a(\phi)} + c(y_i,\phi))
$$

에 대해 

$$
Score \ f = \sum_{i=1} ^N \frac{(y_i - \mu_i)x_{ij}}{var(y_i)} \frac{\partial \mu_i}{\partial \eta_i} = 0 \ \ -- (*)
$$


반면 Quasi-likelihood estimate 은 특정 분포가 아닌 평균과 분산의 관계만을 가정한다. (예를 들자면 포아송분포의 평균은 분산과 동일한데, 이때 그 관계를 $\nu(\mu_i) = \mu_i$ 이런식으로 정의하는게 끝)

그럼 평균과 분산만의 관계로 어떻게 추정이 가능할까?  ===> 위의 (*) 함수를 사용한다! (exponential family 가 아니어도 이 식을 사용한다)

$$
\sum_{i=1} ^N \frac{(y_i - \mu_i)x_{ij}}{\nu(\mu_i)} \frac{\partial \mu_i}{\partial \eta_i} = 0
$$

를 만족하는 모수값을 추정값으로 삼는다. 이 때 평균과 분산의 관계에 의해서 식이 아래와 같이 변형되고 포아송분포에서는 quasi-likelihoood 를 사용한 추정량이 MLE와 동일한 것을 알 수 있다.

$$
\sum_{i=1} ^N \frac{(y_i - \mu_i)x_{ij}}{\mu_i} \frac{\partial \mu_i}{\partial \eta_i} = 0
$$


추가적으로, QL estimateor 는 GLM에서 찾은 MLE의 점근적 분산과 동일하다. $var(score \ f) = I_n(\beta) = var(X^TDV^{-1}(y-\mu)) = X^TWX$


<br>

이제 실제로 overdispersion을 quasi-likelihood를 사용해서 다뤄보자.

$$
\nu(\mu_i) = \phi \mu_i
$$


라고 가정하면, $\phi > 1 $ 일 때 overdispersion을 다룰 수 있다.

즉 분산이 $\nu(\mu_i) = \phi \nu^* (\mu_i) $ 일 때

$$
\chi^2 = \sum_{i=1} ^N \frac{(y_i - \hat \mu_i)}{\nu^*(\hat \mu_i)}
$$

에 대해 $E(\chi^2 / \phi) \approx N-p$ 이므로 $\hat \phi = \chi^2 / (N-p)$ 로 추정 가능하다. 이를 활용해서 일반적인 GLM 에서 찾은 분산값에 $\hat \phi$ 를 곱해주면 overdispersion을 다룰 수 있다.

<br>

#### 1.2.5 Connection between Poisson and Multinomial Distributions


iid 포아송분포의 합은 당연히 포아송분포를 따르게 되는데, 만약 그 합의 개수가 N으로 정해져있다면?

$N= \sum Y_i $ 가 given이면 $Y_i$는 절대 N을 넘을 수 없고 이때는 더이상 Y 가 포아송분포가 아니게 된다. 

pf)

<img src='{{"/assets/img/cda1_1.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

<br>

#### 1.2.6 The Chi-Squared Distribution

카이제곱분포는 데이터들의 분포가 아니라 sampling distribution 이다.

자유도가 증가하면 normal로 근사


~~~R
par(mfrow = c(1,4))
n <- c(5,30,50,100)


for (i in 1:4){
  chi.square <- c()
  for (j in 1:1000) {
    x <- rnorm(n[i],0,1)
    chi.square[j] <- sum(x*x)
  }
  hist(chi.square,probability=T, main = paste0('df=' , n[i]))
  y <- seq(from=0, to=10*n[i], by=0.1)
  fy <- dchisq(y, n[i])
  lines(y, fy, col='red')
}

~~~

<img src='{{"/assets/img/cda1_2.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

<br>


### 1.3 Statistical Inference for Categorical Data

#### 1.3.1 Likelihood Functions and Maximum Likelihood Estimation


MLE 는 regularity condition 하에서 asymtotically consistent and asymtotically efficient

(MLE 정의? likelihood function을 최대로 하는 추정값. 관찰된 데이터가 가장 잘 일어나게끔 하는 파라미터 추청값)

그렇기 때문에 Likelihood function이 더욱 curvature 할수록 estimator의 분산은 더욱 줄어든다.( 더욱 curvature 할수록 mle 값에서 멀어질수록 곡선이 빠르게 감소한다 )

<br>

#### 1.3.2 Likelihood Function and ML Estimate for Binomial Parameter

binomial distribution에 대한 mle를 구하면 $\hat \pi = y/n$ , $E(\hat \pi)$ 와 $var(\hat \pi)$ 를 구할 수 있음.

또한 $\hat \pi$ 에 대한 asymtotic variance를 fisher information을 통해 구할 수 있는데, 이는  $var(\hat \pi)$ 와 같다.

<br>

#### 1.3.3 Wald - Likelihood Ratio - Score Test Traid

우선 여기서 제시하는 3가지 방법이 모두 likelihood function을 활용해서 추정을 하는 것임을 숙지해야 한다.

null hypothesis : $\beta = \beta_0$

##### **Wald**
standard error 를 사용하는 방법. 가장 기본 form은 다음과 같은데, fisher information이 모수에 대한 f 이므로 모수 추정값을 plug-in

$$
Z = (\hat \beta - \beta_0)/SE, \ \ \ where \ \ \ SE = 1/\sqrt{\mathbf(I(\hat\beta )}
$$

Z 통계량은 샘플이 커질 수록 Normal로 근사하고, 양측검정일 경우 Z의 제곱값은 카이제곱분포를 따르므로 카이제곱 분포를 활용한 검정이 가능하다. 


Multivariate으로 확장하면

$$
(\hat \beta - \beta_0)^T[cov(\hat \beta)]^{-1}(\hat \beta - \beta_0)
$$
##### **LR**

$$
-2 log \Lambda = -2 log (L_0/L_1) = 2(l_1 - l_0)
$$

이 통계량은 귀무가설하에 카이제곱분포로 근사하고, 자유도는 전체모수공간에서 파라미터 dimension 과 귀무가설하에서 파라미터 dimension의 차이이다.
##### **Score**

score function을 사용하기 때문에 score test라고 불린다. 다른말로는 Lagrange multiplier test

score function $\dot l(\beta)$에 대해서 

$$
\dot l(\beta)^T [\mathbf{I}(\beta)]^{-1} \dot l(\beta)
$$

형태를 생각하자. 그런 다음 귀무가설 하에서 모수값에 대한 MLE인 $\beta_0$ 값을 위 식에 plug-in

즉

$$
\dot l(\beta_0)^T [\mathbf{I}(\beta_0)]^{-1} \dot l(\beta_0)
$$









<br>

#### 1.3.4 Constructing Confidence Intervals by Inverting Tests

위에서 제시한 세가지 추정 방법을 통해 각기 다른 CI를 찾아낼 수 있다. 특히 Normal 가정하에서 regression setting이면 추정량에 대한 CI는 모두 동일

$100(1-\alpha)%$ Confidence interval
<br>
##### **Wald**
$$
|\hat \beta - \beta_0|/SE < Z_{\alpha /2}
$$

##### **LR**
$$
2(l_1 - l_0) < \chi^2 (\alpha)
$$

신뢰구간을 구하기 복잡함

##### **Score**
이건 특별한 조건 하에서만 가능.




<종합>
만약 sample size N 이 충분히 크지 않아 Normal 가정이 성립하지 않는다면 신뢰구간 또한 올바르지 않을 것이다. 또한 parameter의 개수가 너무 많으면 MLE 추정량의 정확성이 떨어지기 때문에 이를 고려할 필요가 있다. 

<br>


### 1.4 Statistical Inference for Binomial Parameters

binomial setting에서 wald와 score를 비교해보면 모수값에 대한 SE가 형태는 동일하되, wald 는 추정값을, score는 귀무가설 값을 plug-in 하기 때문에 귀무가설하에서의 추청량을 활용한 극한분포는 score의 경우에 normal 근사가 더욱 잘될 것이다. 특히 참값이 귀무가설과 유사할 경우는 더욱 그러할 것.

#### 1.4.4 Exact Small-Sample Inference and the Mid P-Value

computation power로 인해서, normal 근사를 하지 않고도 Exact p-value를 구할 수 있다.

다만 이산형자료의 경우 정확하게 p-value가 0.05가 되지 않기 때문에 때로는 보수적인 검정을 하게된다. 즉 유의수준보다 더 작은 값을 갖는 p-value를 기준으로 기각역을 설정하게 된다. 표본이 작은 경우 이산형 자료에 맞는 p-value 값을 주기 위해서 아래의 p-value를 활용할 수 있다.

$$
mid \ \ P-value \ = \frac {P(T=t_0)}{2} + P(T>t_0) 
$$

이 값을 활용하면 이산형에서도 p-value = 0.5를 만들 수 있다 (즉 홀수개의 자료에서도 p-value 값을 합리적으로 구할 수 있다.)

<br>


### 1.5 Statistical Inference for Multinomial Parameters

#### 1.5.1 Estimation of multinomial parameters

라그랑지안을 사용해서 증명이 가능하다. 

$$
\hat \pi = n_j / n
$$

#### 1.5.2 Pearson Chi-Squared Test of a Specified Multinomial 

pearson chi-squared test는 categorical data 분석에 아주 큰 영향을 주었다.

귀무가설 $H_0 : \pi_j = \pi_{j0} $ ,=1,2,3,...,c 에 대한 통계량은 다음과 같다. $\mu_j = n\pi_{j0}$ 일 때

$$
\chi^2 = \sum_j \frac{(n_j - \mu_j)^2}{\mu_j} \ \ \sim \chi^2 (c-1) \ \ under \ H_0
$$

multinomial 에서 score statistic과 일치한다
#### 1.5.3 Likelihood-Ratio Chi-squared test of a specified multinomial

귀무가설 $H_0 : \pi_j = \pi_{j0} $ ,=1,2,3,...,c 에 대한 통계량은 다음과 같다.

$$
G^2 = 2log\Lambda = 2 \sum_j n_j log(n_j/n \pi_{j0}) \ \ \sim \chi^2 (c-1) \ \ under \ H_0
$$




#### 1.5.5 Testing with Estimated Expected Frequencies

만약 귀무가설 $H_0 : \pi_j = \pi_{j0} $ 에 대해 $\pi_{j0} = \pi_{j0}(\theta)$ 라고 하자. 즉 귀무가설이 어떤 추정량의 함수형태로 나온다.(1.5.6 참고) MLE의 plug-in 성질에 의해 $\hat \mu_j = n\pi_{j0}(\hat \theta)$ 이고 이를 활용해 $\chi^2$ 이나 $G^2$ 값을 구한 다. 이 때 자유도는 $\theta$의 차원이 p 일 때 (c-1)-p 이다.

#### 1.5.7 Chi-Squared Theoretical Justification

CLT 에 의해

$$
\sqrt{n}(\hat \pi - \pi _0) \Rightarrow^d N(0, \Sigma_0)
$$


<br>

### 1.6 Bayesian Inference for Binomial and Multinomial Parameters

베이지안은 CI 대신에 poeterior interval or credible interval을 사용한다. 이때는 tail 양 끝 값의 확률이 동일하도록 신뢰구간을 설정.(예를 들어 95% 신뢰수준이면 2.5 ~ 97.5로)

unimodal의 경우 *highest posterior density(HPD)* 을 사용하는데 이는 구간의 길이를 가능한 짧게 만드는 신뢰구간이다. 



