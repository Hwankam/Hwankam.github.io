---
layout : post
title : PRML-5
subtitle : PRML
date : 2022-02-09
#categories:
tags : [PRML, Machine learning]
toc_sticky : true
use_math : true
comments: true
--- 


## 5. Neural Networks

chap 3과 4에서는 linear combination of fixed bases function에 대해 배웠다. 그러나 high dimension에서는 과적합의 문제가 발생하는 한계가 있었다. 이를 해결하기 위해 데이터에 맞게 basis function을 바꾸는 것은 어떠할까? 

SVM은 데이터에 맞게 hyperplane을 설정하는 것이므로 맥락에 부합한다. 이는 basis function의 후보 가운데 일부를 선택해서 데이터에 적합시키는 것이므로 '차원의 저주'로 부터 좀 더 자유로워질 수 있다. 또한 convex optimization 문제로 풀 수 있기 때문에 solution이 명확하다.

좀 더 다른 방식으로, basis function 개수는 고정하지만 parameter를 통해 data에 맞는 basis function 자체를 바꾸는 것을 생각할 수 있다. 가장 대표적인 방식이, feed forward neural network(multilayer perceptron)이다. 이 모델은 SVM에 비해 compact하며 학습속도가 빠르다. 그러나 convex optimization 문제가 아니므로 해를 찾기 어려울 수 있다. 

multilayer perceptron에서 결정해야할 parameter는 Maximum likelihood를 사용해서 결정한다.(뒤에서 Jacobian 이나 Hessian을 통해서 error backpropagation의 작동원리를 배울 것임)


<br>


### 5.1 Feed-forward Network Functions

chap 3 과 4의 모델은 다음과 같다.

$$
y(\boldsymbol{x}, \boldsymbol{w}) = f (\sum_{i=1}^M w_j \phi_j (\boldsymbol{x}) )
$$

multilayer perceptron 모델에서는 non-linear basis fucntion인 $\phi_j (\boldsymbol{x})$ 가 adjustable parameter에 의존하도록 하겠다.( how? layer를 거치면서 
$\phi$ 는 input과 weight 
$\boldsymbol{w}$의 결합으로 만들어짐 )

[modeling]

input variable : D개 (i)

linear combination of input variable : M개 (j)

superscript : layer

activation : 
$a_j$

differentiable nonlinear activation function : h (data의 특성에 의해 / target의 분포적 특성에 의해 결정됨) (Regression : identity function // binary classification : sigmoid function // Multiclass classification : softmax)

hidden unit : z

$$
a_j = \sum _{i=1} ^D w_{ji} ^{(1)} x_i + w_{j0} ^{(1)} \\

z_j = h(a_j)
$$

아래 그림은 two layer neural network를 나타내는데, node는 deterministic variable이므로, probabilistic graphical model(chap 8)은 아니다. 

<img src='{{"/assets/img/prml-5-1.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

식과 그림에서도 알 수 있듯, 각 layer는 perceptron 모델과 매우 유사하다. 결국 perceptron 모델 또한 에러를 줄이는 perceptron parameter w를 결정하고자 한다. 그러나 perceptron 모델은 class를 +1 과 -1 로 나누는 데에서 activation function이 step function인 반면, neural network에서는 differentiable 하다는 것이 차이라 할 수 있다. 

만약 hidden unit이 linear라면 결국 식 (5.7) 와 같이 중첩해서 식을 쓸 필요가 없고, input 과 output만이 있는 매우 simple한 모델을 만들 수 있을 것이다. 이러한 형태는 나중에 12장에서 배울 principal component analysis와 매우 유사하다고 할 수 있다.


network architecture에서 자주 사용되는 skip-layer connection은 경사소실을 막아, sparse 모델이 특정 gredient에만 영향을 크게 받는 것을 방지한다.

"모든 input 과 oupter이 hidden layer 에 의해 연결된다면 weight는 어떻게 정할 것인가 => maximum likelihood and Bayesian apporach "

아래 그림은 hidden unit과 network 학습 결과를 보여준다.


<img src='{{"/assets/img/prml-5-2.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>


<img src='{{"/assets/img/prml-5-3.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>


<br>


#### 5.1.1 Weight-space symmetries

feed-forward network의 특징 중 하나는, weight vector에 대한 multiple distinct choice 가 결국 같은 mapping function을 만들어 낸다는 것이다. 

ex1 ) tanh function와 같은 기함수의 특징을 활용할 때 M개의 hidden unit에 대해 $2^M$ 개의 weight vector가 동일한 network를 만들어 낼 것이다. 

ex2 ) hidden unit의 순서를 바꾸면 된다.  


<br>


### 5.2 Network Training


polynomial curve fitting(chap 1) 에서는 sum of square error function을 minimize 하는 parameter 값을 추정하는 것을 배웠다. neural network에서도 마찬가지로 parameter를 추정하는 것이 중요하다. 

우선 network output에 대한 확률적 접근을 할 수 있다.

target $\boldsymbol{t}$ 가 Gaussian distribution을 따른다고 생각해보자. Regression setting에서 unit activation function을 생각하면 parameter에 대한 MLE를 찾을 수 있다. 또한 Gaussian 가정 하에서는 likelihood function을 최대화 하는 것은 sum of square error function을 최소화 하는 것과 같다. 

가우시안의 경우에는 convex 문제를 쉽게 풀 수 있지만, 실제로 많은 경우에는 network function이 non-linearity를 가지고 있고, error function 또한 마찬가지이다. => analytic 하게 구해야 한다.


binary classification의 경우는 maximum likelihood 방식으로 parameter를 추정하는 것이 cross entropy error function을 최소화 하는 것과 동일하다. (sum of square error는 학습의 속도 또한 느리며, ML 방식에서 도출할 수 없는 error 이므로 generalization이 어렵다) 또한 target이 특정 분포를 따르는 것이 아니라, 0 또는 1로 명확하게 labelling 되어 있기 때문에 분산을 정의할 필요가 없다. 


binary classification with multiple target의 경우, linear classification은 각 linear model이 output과 각각 linearly independent 하게 영향을 주지만, neural network에서는 각 input이 각 output에 non-linearly 영향을 준다는 것이다. 

<br>


#### 5.2.1 Parameter optimization

geometric apporach

<img src='{{"/assets/img/prml-5-4.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>



weight space에서 $\delta \boldsymbol{w}$ 만큼 움직였을 때, error function의 변화는 
$\delta E \simeq \delta \boldsymbol{w}^T \nabla E(\boldsymbol{w}$) 이고 이때 gredient는 error를 가장 많이 줄이는 방향이다. 따라서 gredient가 0에 매우 근사할 때 error를 가장 작게 하는 값에 도달했다고 할 수 있다.

"Points at which the gradient vanishes are called stationary points and may be further classified into minima, maxima, and saddle point"

Network 안에는 수많은 inequivalent stationary point 와 inequivalent minima 가 존재한다. 그러나 반드시 global minima를 찾을 필요는 없으며 존재하지 않을 수도 있다. 실제로 할 수 있는 것은 local minima 후보지를 몇개 선정한 다음 이를 비교해서 최적의 값을 찾는 것이다. 

(Newton-Rhapson 방법과 같이 iterative하게 parameter 값을 추정해야할 경우도 있다.)

<br>