---
layout : post
title : ISLR - LAB8
subtitle : statistical learning - 8장 - LAB
date : 2021-08-08
#categories:
tags : [datascience, datamining, machinelearning, ISLR]
toc_sticky : true
use_math : true
comments: true
---

### 8장 코드 정리

~~~R
## Fitting Classification Trees

library(tree)

library(ISLR)
attach(Carseats)
High = ifelse(Sales<=8, "No","Yes")
High = as.factor(High)

Carsetas = data.frame(Carseats,High) #기존의 Carseats data와 High를 합치는 것. 
tree.carseats = tree(High ~ . - Sales, data = Carseats) # 회귀 할 때 lm 처럼 tree 를 사용함.
summary(tree.carseats)
"""
Classification tree:
tree(formula = High ~ . - Sales, data = Carseats)
Variables actually used in tree construction:
[1] "ShelveLoc"   "Price"       "Income"      "CompPrice"   "Population" 
[6] "Advertising" "Age"         "US"         
Number of terminal nodes:  27 
Residual mean deviance:  0.4575 = 170.7 / 373 
Misclassification error rate: 0.09 = 36 / 400
"""
plot(tree.carseats)
text(tree.carseats, pretty=0, cex=0.5) #display node labels, pretty는 categorical 변수 안에 항목 중에 어떤 값으로 분류되었는지도 말해주는 것.
tree.carseats #plot의 내용을 라인별로 console에 띄어줌.

#prediction
set.seed(2)
train = sample(1:nrow(Carseats),200)
Carseats.test = Carseats[-train,]
High.test = High[-train] #high는 데이터프레임이 아니라서 [-train,]라고 쓰면 오류남

tree.carseats = tree(High ~ . -Sales, Carseats, subset = train)
tree.pred = predict(tree.carseats, Carseats.test, type='class') #type = 'class' 명령어를 통해 실제 class 예측값을 return 받는다. 
# tree.pred = predict(tree.carseats, Carseats.test)
table(tree.pred, High.test)


#prune
set.seed(3)
cv.carseats = cv.tree(tree.carseats, FUN=prune.misclass) #pruning 과정에서 classification error rate을 최소화하는 cv 값을 가지고 모델을 선택하게 하는것.
names(cv.carseats) # 항목이 무엇인가
cv.carseats # 결과값을 보고 dev(corresponde to cv - error) 를 가장 작게 하는 terminal node의 개수를 찾아 이에 해당하는 size 그리고 k값(alpha 값)을 선정

plot(cv.carseats$size, cv.carseats$dev, type="b")
plot(cv.carseats$k, cv.carseats$dev, type="b")

#9개의 노드로 prune 하기 : prune.misclass() 명령어 사용해서 기존에 적합한 tree.carseats를 prune 하기
prune.carseats = prune.misclass(tree.carseats,best=9)
plot(prune.carseats)
text(prune.carseats, pretty=0, cex = 0.5)

#pruning 한 모형의 예측력이 얼마나 뛰어날까
tree.pred = predict(prune.carseats, Carseats.test, type='class')
table(tree.pred, High.test)

#결과를 보니 terminal node를 줄여서 더 해석력도 좋으면서도 accuracy도 높은 모델이 완성이 되었다. 


## Fitting Regression Trees
library(MASS)
set.seed(1)
train = sample(1:nrow(Boston),nrow(Boston)/2)
tree.boston=tree(medv ~ ., Boston, subset=train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston, pretty=0)
cv.boston = cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type='b')
prune.boston = prune.tree(tree.boston, best=5)
plot(prune.boston)
text(prune.boston,pretty=0)

#prediction
yhat = predict(tree.boston, newdata = Boston[-train,])
boston.test = Boston[-train,'medv']
plot(yhat, boston.test)#plot이 왜 이산형처럼 나올까? => tree기반 모델이니까 각 region 마다 동일한 값으로 예측
abline(0,1)
mean((yhat - boston.test)^2)


## Bagging and Random Forest
#Bagging은 Random Forest의 특별한 경우이므로 randomForest() function으로 randomforest와 bagging을 모두 사용할 수 있다. 

library(randomForest)
set.seed(1)
bag.boston = randomForest(medv~., data= Boston, subset = train, mtry=13, importance = TRUE) #mtry=13 의 의미는 각 split에 13개의 변수 모두 들어가도록  = bagging
bag.boston


#randomForest 에 ntree 인자를 통해 tree의 개수를 조절할 수 있다,
bag.boston = randomForest(medv~.,data=Boston, subset= train, mtry=13, ntree=25)
yhat.bag = predict(bag.boston, newdata = Boston[-train,])
mean((yhat.bag - boston.test)^2)

#randomForest에 mtry를 조절해서 bagging에서 RF로
rf.boston = randomForest(medv~., data= Boston, subset = train, mtry=6, importance = TRUE) 
rf.boston
yhat.rf = predict(rf.boston, newdata = Boston[-train,])
mean((yhat.rf - boston.test)^2)

importance(rf.boston) #importance는 변수의 중요도를 보여준다. 이 결과값은 모두 감소량을 의미하는데, 첫번째는 각 변수가 빠졌을 때 OOB 샘플의 예측 정확성의 평균 감소, 두번째는 split 과정에서 줄어드는 node impurity의 total 값이며 그 total 값을 모델마다 평균한것.
varImpPlot(rf.boston) #교과서 figure 8.9



## Boosting
library(gbm)
set.seed(1)
boost.boston = gbm(medv~., data=Boston[train,], distribution = 'gaussian', n.trees=5000, interaction.depth=4)
#distribution = 'gaussain' 은 회귀에서, distribution = 'bernoulli'는 분류에서
#n.trees 는 tree 원하는 개수
#depth는 tree의 깊이

#shrinkage를 주려면 인자로 shrinkage = 0.2 와 같이 준다.
boost.boston = gbm(medv~., data=Boston[train,], distribution = 'gaussian', n.trees=5000, interaction.depth=4, shrinkage = 0.2)
yhat.boost = predict(boost.boston, newdata = Boston[-train,], n.trees=5000)
mean((yhat.boost - boston.test)^2)


#boosting 과정에서 변수의 중요도 보기
summary(boost.boston)
names(summary(boost.boston)) # "rel.inf" 는 relative influence statistic. 즉 중요한 변수
plot(boost.boston, i = 'rm') # rm 변수가 y 에 미치는 marginal effect


~~~

 

