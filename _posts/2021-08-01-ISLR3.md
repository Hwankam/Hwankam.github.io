---
layout : post
title : ISLR chap 3
subtitle : statistical learning - 3장
date : 2021-07-30
#categories:
tags : [datascience, datamining, machinelearning, ISLR]
toc_sticky : true
use_math : true
comments: true
---

### 3. Linear Regression

이 장은 선형회귀에 관한 내용을 담고 있으며, 매우매우매우 많이 다루고 본 내용이기 때문에 핵심내용 + 놓치기 쉬운 포인트 들만 집고 넘어가도록 하겠다.



#### 3.1 Simple Linear Regression

Y on X

##### 3.1.1 Estimating the Coefficients

가장 대표적으로 쓰이는 방법은 LSE

#####  3.1.2 Assessing the Accuracy of the Coefficient Estimates

LSE 는 unbiased estimator. 즉 데이터가 매우 많아서 각 데이터 set의 LSE 값을 평균한다면 이는 모수에 매우 가까워질 것 이다.

<img src='{{"/assets/img/islr3-1.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>


그럼 LSE의 평균은 그렇다 치고, 데이터 set 에서 나온 하나의 추정값의 성능은?

이는 그 추정량의 분산을 살펴보면 알 수 있을 것.



simple linear regression 에서

$
SE(\hat \beta_0)^2 = \sigma^2[\frac {1}{n} + \frac {\bar x^2}{\sum (x_i - \bar x)^2}]
$

$
SE(\hat \beta_1)^2 = \sigma^2 \frac {1}{\sum (x_i - \bar x)^2}
$

slope 의 분산은 데이터가 넓게 퍼져있을 수록 작아진다. 이는 기울기 추정에 있어 leverage 효과를 가져오는 것과 같다. intercept의 분산은 데이터 셋의 평균이 0일 때에 평균에 대한 추정값의 분산과 같다.

또한 모분산(무엇에 대한 모분산인가 헷갈릴 수 있는데 여기서는 입실론이 분포를 가지는 확률변수이고 이 입실론에 대한 분산을 말하는 것이다!) 의 값을 사실 알지 못하는 경우가 많기 때문에

$
\hat \sigma^2 = \frac {\sum(y_i - \hat y_i)^2}{n-2}
$

으로 추정하게 된다.(이를 Residual Standard error ^2)

또한 추정값에 대한 신뢰구간 혹은 T statistic 또한 자연스럽게 구할 수 있다. 



##### 3.1.3 Assessing the Accuracy of the Model

계수들의 accuracy 뿐만 아니라 모형 전체의 accuracy 도 구하고 싶다.

이 때에는 Residual Standard Error(바로 위에서 제시됨)  혹은 R- square 값을 사용한다.



###### RSE

estimate of the standard devitation of Epsilon.

RSE의 값은 y의 값과 단위를 맞춘 것으로 y의 평균값과 비교했을 때 얼마나 그 수치가 큰 것인지 비교 가능하다. 또한 RSE는 lack of fit에 대한 기준으로 사용가능하다. 



###### R-square

RSE와는 다르게 fit에 대한 직관적인 수치를 제공한다.  단순선형회귀에서는 상관계수의 제곱값과 동일하다.

#### 3.2 Multiple Linear Regression

##### 3.2.1 Estimating the Regression Coefficients

최소제곱법 사용 가능. 변수간의 관계를 따져야 한다. multiple regression은 여러가지 변수를 predictor로 집어넣기 때문에 predictor가 모두 target값과 관련이 있을 지라도 multiple regression의 회귀계수는 이와 상이할 수 있다. 

예를 들어 다중회귀에서는 아이스크림 판매량과 상어에 의한 피서객 사고 간의 양의 상관관계를 잡아내고 매우 큰 회귀계수값을 추정해낼 수 있는데, 이는 온도가 높을 수록 해변으로 피서를 많이 가게되고 자연스럽게 아이스크림 판매량이 늘어나는 논리를 최소제곱법이라는 단순 계산에서는 찾아내지 못하기 때문에 발생한 결과라고 할 수 있다. 

##### 3.2.2  Some Important Questions

###### Relationship

$
H_0 : \beta_1 = \beta_2 = ... = \beta_p = 0
$

에 대한 가설검정을 해야한다. 귀무가설이 맞다는 가정 하에서 F-statistic을 만들어낼 수 있다.

또한 다중회귀에서는 개별적인 회귀계수들의 p 값이 우연한 결과로 매우 작을 수 있으나, 모형 전체의 회귀계수에 대한 F-statistic은 매우 작아 유의하지 않을 수도 있다. 또한 개별적으로는 target 변수와 유의미한 관계를 가질지언정, multiple regression setting에서 볼 때 여러가지 변수들이 종합적으로 target변수와 큰 관계를 가지지 못할 수 도 있다. 물론 반대로 개별 회귀계수들의 t statistic에 대한 p 값이 매우 클지라도 multiple setting에서 F값에 대한 p 값은 매우 작게 나올 수도 있다.

즉 데이터에 대한 이해를 바탕으로 유기적으로 생각하는 것이 중요하다.



###### Deciding on Important Variables(변수선택)

앞서 말한대로, multiple regression에서 ''회귀계수가 모두 0이다'' 라는 귀무가설이 기각 된다고해도, 개별적인 회귀계수의 t statistic으로 중요한(유의미한) 변수를 결정하는 것에는 한계가 있다.  따라서 변수 선택을 잘 하기 위한 여러가지 접근법이 존재하는데 이를 숙지할 필요가 있다.

1. Mallow's Cp
2. AIC
3. BIC
4. adjusted R-square
5. 잔차가 작은 모델

만약 변수의 개수가 매우 많다면? (1) Forward selection (2) Backward selection (3) Mixed selection

###### Model fit

simple linear regression과 마찬가지로 (1) R-square  (2) Residual Squared Error

(1) R-square는 변수가 많아질수록 높아질 수밖에 없다.

(2) 또한 변수가 추가될 때 RSE가 늘어날 수 있는지에 대해 생각해보아야 한다. 변수가 추가되면 자연스럽게 RSS(SSE)는 줄어들 수밖에 없다. 그러나
$
RSE = \sqrt \frac{RSS}{n-p-1}
$
이므로 RSS가 줄어든 것에 비해 변수의 개수가 더 많이 늘어나면 RSE는 증가할 수 있다. 이 점을 주의하자. **<u>(RSE는 변수의 개수와 관련이 있다는 사실!!)</u>**

(3) 직접 그려보면 잔차들의 형태를 알수있다(negative or positive)

###### predictions

함수 f의 추정량에 대해 reducible error(비선형 모델인데 선형으로 선택, 회귀계수에 대한 추정이 잘못되었다던지)와 irreducible error(prediction 과정에서 individual point에 대한 irreducible error ) 를 모두 고려할 필요가 있다.



#### 3.3 Other Considerations in the Regression Model

##### 3.3.1 Qualitative Predictors

가변수로 만들기

1. class가 두개 일 때

(0,1) 코딩 혹은 (-1,1) 코딩 => 어떤 방법을 쓰든 결과는 같다. 계수는 차이가 있을지언정 해석은 동일

2. class가 두 개 초과 할 때

코딩할 때 category를 하나 줄여야한다. 즉 아무것도 코딩 안된 상태를 baseline으로

코딩 방식에 따른 prediction 값은 차이가 없으나 어떻게 코딩하느냐에 따라 회귀계수에 대한 p-value가 달라질 수 있다. 따라서 가변수를 활용할 때, 특히 class가 3개 이상인 경우 변수에 유의성을 확인할 때는 가변수의 회귀계수들에 대한 F-test를 해야한다. 

##### 3.3.2 Extensions of the Linear Model

Linear regression의 가정은 변수들이 (1)additive (2) linear 하다는 것이다.

즉 변수들은 서로 독립이며 특정 변수의 한단위 증가가 target에 미치는 영향은 오로지 그 변수 뿐이라는 것이다. 

(1)번 가정을 완화해보자

interaction effect를 고려할 수 있다. 

※ 또한 hierarchical principal 에 따르면 두 변수의 interaction이 중요하다고 생각될 때(사전지식에 의해), 개별 변수의 계수값의 p-value가 낮다고 하더라도 개별 변수, interaction term 을 모두 넣어줘야 한다.  또한 개별 변수를 제외하고 교차항만을 넣는것은 의미가 없다.

질적변수와 양적변수에 대한 interaction term을 만들어내면 질적변수의 class에 따라 기울기와 절편이 모두 바뀐다. 



(2)번 가정을 완화해보자

polynomial regression... 더 복잡한 것은 7장에서

##### 3.3.3 Potential Problems

비단 선형회귀 뿐만 아니라 많은 모델에서 아래와 같은 문제들에 대해 고심해볼 필요가 있다

###### Non-linearity of the response-predictor relationships(선형성 가정)

선형 회귀 적합 후 잔차도를 그려봤을 때, 특별한 패턴이 발견된다면 변수를 추가할 필요가 있는데 특히 U자형의 그래프가 그려진다면 non-linearity를 고려해봐야 한다. 이를 해결하기 위해
$
logX,\ \sqrt X, \ X^2 
$
 등의 항들을 추가해야 한다.



###### Correlation of error terms(error 간 독립성 가정)

선형모형을 가정할 때
$
\epsilon \sim^{iid} (0, \sigma^2)
$
인데 만약 error term 간에 상관관계가 존재한다면 standard error 추정시 과소추정이 발생한다. 그 결과 신뢰구간이나 예측구간이 실제보다 더욱 좁아진다. 또한 계수에 대한 p-value 또한 실제값 보다 작아져서 유의하지 않은 변수를 유의하다고 판단할 우려가 있다. 

특히 이런 상관관계는 시계열 데이터에서 자주 나타난다. 인접한 시간에서는 error들이 양의 상관관계를 가지는 경우가 많다. 

또한 time이라는 변수 이외에도 데이터 간의 상관관계가 나타날 수 있으므로 주의해야 한다. 예를 들어 sampling 과정에서 데이터들 간에 동질성으로 인해 서로 독립이 아닌 obs가 표본으로 잡힐 수 있다. 즉 시계열 뿐만 아니라 실험설계 과정에서도 독립성 가정이 유지되는지 확인해 볼 필요가 있다. 



###### Non-constant variance of error terms (등분산성 가정) (heteroscedasticity)

잔차도를 그려보았을 때 funnel shape 형태를 띈다. 이때 로그 변환이나 제곱근 변환을 해주면 heteroscedasticity 문제를 해결할 수 있다. 또한  독립성 가정은 성립하나 등분산성 가정이 깨질 시에는  weighted least square 을 통해서 문제를 해결할 수 있다.

<img src='{{"/assets/img/islr3-2.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>


###### Outliers ( y 축의 관점)

아웃라이어 제거 이후에 fitting된 모델의 큰 변화가 없는 경우도 있다!

그러나 RSE나 R-square는 이상치가 제거 되었을 때 보다 좋아질 것이다.

잔차 또한  단위의 영향을 받으므로 잔차를 standard error의 추정값으로 나눈 studentized residual을 사용하면 이상치를 판단하는데에 더욱 유리할 것이다.(3 이상이면 이상치일 가능성이 크다) 무엇보다 잔차는 모델에 의해 나온 것이므로 모델이 잘못된 경우 실제로 이상치가 아닌 경우에도 이상치로 판정될 수 있으니 혹여나 모델이 잘못된 것은 아닌지 생각해 볼 필요도 있다. 



###### High-leverage points (x 축의 관점)

leverage point는 추정 회귀 계수에 영향을 준다. 그러나 다중회귀에서는 이러한 포인트를 그림을 통해 찾기 힘들다. 따라서 leverage statistic 을 통해 이를 판정하자. 단순회귀의 경우에 통계량은 다음과 같다.

$$
h_i = \frac {1}{n} + \frac {(x_i - \bar x)^2}{\sum (x_i - \bar x)^2}
$$

x가 평균으로부터 멀리 떨어져 있는 경우 statistic은 커진다. 또한 모든 관측치를 통한 statistic의 평균이 (p + 1) / n 이므로 이를 기준으로 leverage point를 찾을 수 있다. 



###### Collinearity(변수간 독립성 가정)

<img src='{{"/assets/img/islr3-3.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>


contour를 살펴보면 다중공선성이 높은 두 변수를 사용했을 때는 변수들의 순서쌍 중 대부분이 유사한 RSS를 갖는다. 따라서 data가 살짝만 바뀌더라도 RSS를 최소로 만드는 순서쌍 값일 가능성이 높다. 즉 계수 추정치가 불안정하다. 회귀 계수 추정치가 불안정하다는 것은 계수의 분산 값 또한 크다는 의미이므로, 계수의 t-test 값이 작아진다. 

다중공선성을 감지하기 위해 

(1) 상관계수 행렬을 찾는다

(2)  두 가지 이상의 변수에서 공선성이 나타나는 경우에는 (multi-collinearity) variance - influence - factor를 이용한다.  이를 구하는 공식은 다음과 같다.

$$
VIF(\hat \beta_j) = \frac {1}{1-R^2_{X_j|X_-j}}
$$

(사실, VIF는 모든 변수를 포함했을 때 $ \hat \beta_j $ 의 분산을 j 번째 변수만을 포함했을 때 $ \hat \beta_j $ 의 분산으로 나눈 값으로 정의된다)

#### 3.5 Comparison of Linear Regression with K-Nearest

 KNN Regressor
 
$$
\hat f(x_0) = \frac {1}{K}\sum _ {x_i  \in N_0} y_i
$$

K 를 정하는 기준은 bias-variance trade off 관계에 의해 적절하게 결정해야 한다.

small K : flexible, risk of overfitting

large K : smoother, bias



==> test error rate 을 통해 최적의 K를 설정해야.

(헷갈리지 말아야 할 게 위 내용은 non-parametric 인 KNN 에서 K에 따라 바뀌는 것이고, 앞에서 우리가 봤던 것 중 Interpretability / Predictability 는 Linear / Non - Linear 의 차이었다. )



일반적으로 true model 을 아는 경우에는 parametric 이 낫고, 그렇지 않은 경우는 non-parametirc 접근이 더 나을 것이다. 그러나 현실에서 true model을 아는 경우는 드물다. 그럼 반드시 non-parametric 한 접근이 더 좋은 것인가? 

그렇지 않다. 

(1) 모델의 변수가 많아진다면 그에 따른 데이터 또한 많이 필요한데 non-parametric한 접근으로는 추정에 어려움이 더더욱 커진다. 따라서 비록 true model을 정확히 모른다고 할 지라도, 변수의 개수에 비해 데이터 개수가 적다면 parametric 한 접근이 더 나을 것이다. 

(2) simple 한 모델의 장점(해석의 편의, 단순성) 때문에 test MSE의 차이가 별로 없다면 우리는 더 쉬운 모델인 simple(여기서는 linear) 모델을 사용하는 것이 좋다.
