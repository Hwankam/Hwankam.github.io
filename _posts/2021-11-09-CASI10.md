---
layout : post
title : Computer Age Statistical Inference - chap 10
subtitle : statistical method 10
date : 2021-11-09
#categories:
tags : [datascience, statistical method, Efron]
toc_sticky : true
use_math : true
comments: true
---

## The Jackknife and the Bootstrap

<br>

목적은 SE의 계산!

permuation test 는 without replacement $ calcuate p-value
bootstrap 은 with replacement % calculate Standard Error

standard error란 무엇인가? 추정량이 얼마나 정확한지 알고싶을 때!


컴퓨팅 기술이 발전하기 전까지는 plug-in 방식으로 binomial에서 p에 대한 se 추정값을 $\sqrt{\frac{\hat p(1- \hat p)}{n}}$ 로 구하는 것처럼 SE에 대한 추정을 하거나, Taylor expansion을 사용해서 구했다. 

그러나 잭나이프나 붓스트랩의 경우 nonformulaic computation based approach이다. 

<br>

### 10.1 The Jackknife Estimate of Standard Error

$$
x_i \sim \mathbf F \ \ \ \ \ \ \ for \ \ i=1,2,3,...n
$$

일 때 $\theta$의 
추정량은 $\hat \theta = s(\mathbf x) $ 이다. 가지고 있는 샘플이 
$x_1,x_2,\cdots, x_n$ 밖에 없을 때, 추정량에 대한 standard deviation을 어떻게 구할 수 있을까?

특정 i 번째 데이터를 제거한 뒤 동일한 계산식을 통해 $\theta$의 추정량을 구하고 이를 
$\hat \theta_{(i)} = s(\mathbf x_{(i)})$ 라고 하면 jackknife estimate of standard error for 
$\hat \theta$은 다음과 같이 구할 수 있다

$$
\hat {se}_{jack} = [\frac {n-1}{n} \sum_{i=1} ^n (\hat \theta_{(i)} - \hat \theta_{(\cdot)} )^2]^{1/2}, \ \ \ \ \ \ with \ \ \hat \theta_{(\cdot)} = \sum \hat \theta_{(i)}/n
$$

즉 $s(\cdot)$만 알고 있으면 잭나이프 SE는 쉽게 구할 수 있다.

jackknife formula의 특징은 다음과 같다

+ non-parametric
+ $s(\cdot)$만 알면 계산이 빠르다
+ n-1 개의 데이터를 사용한다. 여기서는 데이터를 하나 빼더라도 값이 smooth하게 바뀔 것이라는 가정이 깔려있다. 즉 not robust to sample 인 경우는 고려하지 않는 것 같다.
+ jackknife SE는 true SE에 대해 upwardly biased 이다

<img src='{{"/assets/img/casi-10-1.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

그림에서 age=25 일 때 jackkife SE가 크게 나타나는 것을 알 수 있다. 특히 이 그림은 교과서 Figure 1.2 의 lowess curve와 SE 를 나타낸 것인데, age=25 주변부분에서 그래프가 smooth한 성질을 지니지 못하기 때문에 SE 가 크게 변할 수밖에 없다.

즉 여기서 jackkife SE는 $s(\cdot)$ 의 derivative와 관련 있다는 것을 알 수 있다.

<br>

### 10.2 The Nonparametric Bootstrap


* motivation 

$$
F \longrightarrow x \longrightarrow \hat \theta
$$

모수에 대한 추정량을 얻는 방식은 위와 같다. 그러나 $F$ 를 모르기 때문에 empirical probability distribution 
$\hat F$ 를 통해 bootstrap 샘플과 추정량을 얻는다. Empirical probability distribution은 샘플데이터의 추출 확률을 모두 균등하게 둔 누적분포로 
$F$에 대한 nonparametric MLE 이다. 

$$
\hat F \longrightarrow x^{\star} \longrightarrow \hat \theta^{\star}
$$

Bootstrap SE를 얻는 공식은 다음과 같다. 
$$
\hat {se}_{boot} = [\frac {1}{B-1} \sum_{b=1} ^B (\hat \theta^{*b} - \hat \theta^{{\star}\cdot} )^2]^{1/2}, \ \ \ \ \ \ with \ \ \hat \theta^{*\cdot} = \sum \hat \theta^{{\star}b}/B
$$

Bootstrap formula의 특징은 다음과 같다

+ 여기서 묘사하는 것은 one-sample non-parametric bootstrap
+ $s(\cdot)$만 알면 계산이 빠르다
+ bootstrap은 모든 데이터를 shake(wit replacement)하기 때문에 local derivative에 의존하는 jackknife 보다 더욱 unsmoothness에 의존적이다. 
+ jackknife SE는 true SE에 대해 upwardly biased 이다
+ SE 뿐만 아니라, bootstrap sample을 사용해서 absolute error, fisher information 등등을 만들 수 있다. 특히 fisher information의 경우 bootstrap 추정량을 plug in 하는 형태로 구할텐데, bootstrap 과 fisherian 의 연관성이 뒷장 parametric bootstrap에 잘 드러난다.


추가로, SE를 구하면 normality 가정하에 CI도 구할 수 있다. 그러나 bootstrap 샘플의 히스토그램을 그려보고 과연 normality 가정을 사용할 수 있을지 생각해봐야 한다. 


지금까지 나온 것으로 볼 때, jackknife는 SE와 bias 등을 살펴보는 frequentist device임을 알수있다. 반면 bootstrap은 frequentist, fisherian, Bayesian의 견해와 모두 연결되는 부분이 있는데 뒤에서 이를 자세히 살펴본다.


<br>

### 10.3 Resampling Plans

앞서서 jackknife와 bootstrap을 설명할 때에는 샘플이 뽑힐 확률을 모두 균등하게 둔 상태였다. 그러나 가지고 있는 sample data의 weight를 바꾸면 어떻게 될까? 

sample $x$와 
resampling vector $P = (P_1, P_2, \cdots, P_n)'$ 에 대해 (p는 확률값이므로 sum = 1)

새로운 weight를 사용해서 뽑은 샘플이 있을텐데 이를 활용한 추정량을 $\hat \theta ^{\star} = S(\mathbf P)$ 라고 하자. 

예를들어, 분산에 대한 불편추청량은 $s(\mathbf x) = \sum(x_i - \bar x)^2 / (n-1) = \frac {n}{n-1} [\frac{1}{n} \sum x_i^2 - (\frac{1}{n} \sum x_i)^2 ]$ 이므로 reweight 추정량은 아래와 같다

$$
S(\mathbf P) = \frac {n}{n-1} [\sum \mathbf P_i x_i^2 - (\sum \mathbf P_i x_i)^2 ]
$$

이제 jackknife weight 와 bootstrap weight 중 기존의 균등 weight와 더 거리가 가까운 것은 어떤 것인지 살펴보자


<img src='{{"/assets/img/casi-10-2.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

<img src='{{"/assets/img/casi-10-3.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

이 결과를 가지고 아래 그림을 보면 조금 이상한 점이 있다. 

$$
\frac {(E||p^{\star} - p_0||^2)^{1/2}} {||p_{(i)} - p_0||} \sim \sqrt n \rightarrow \infty \ \ as \ \ n \rightarrow \infty
$$

인 것에 비해 $P_{(i)}$ 와
$P^{\star}$ 의 거리가 너무 가깝다는 점이다. 

<img src='{{"/assets/img/casi-10-4.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

<br>


#### remarks

+ $\hat {se}_{jack}$는 
$P_0$가 
$P_{(i)}$ 로 바뀔 때, 
$S(\mathbf P_0)$ 가 
$S(\mathbf P_{(i)})$ 로 얼마만큼 바뀌는지에 관한 것을 보여준다.

즉 " directional derivative of $S(P)$ at 
$P_0$ along with the direction 
$P_{(i)} - P_0$ " 를 다음과 같은 식으로 표현할 수 있다.

$$
D_i = \frac {S(p_{(i)}) - S(p_0)} {||p_{(i)} - p_0||} = \frac {\hat \theta _{(i)} - \hat \theta} {1/ \sqrt{n(n-1)}}
$$

이를 활용할 때, 

$$
\hat {se}_{jack} = [\frac {\sum D_i^2}{n^2}]^{1/2}
$$



+ 동일한 논리로 $ \hat{se}_{boot}$는 
$P_0$가 
$P^{\star}$로 바뀔 때, 
$S(\mathbf P_0)$ 가 
$S(\mathbf P^{\star})$ 로 얼마만큼 바뀌는지에 관한 것을 보여준다.

<br>

### 10.4 The Parametric Bootstrap

앞서 non-parametric bootstrap에서는 경험분포를 통해서 $\hat F$를 추정하고 이를 통해 bootstrap sample을 뽑아내었다. 그러나 parametric 접근으로 가게 되면 모수에 대한 MLE를 구한 다음 추정하고 이를 plug-in 하면 parametric bootstrap sample을 얻게 되는 것이다. 즉,

$$
F_{\hat \mu} \longrightarrow x^{\star} \longrightarrow \hat \theta^{\star}
$$


여기서는 bootstrap을 활용한 모델의 fitting에 대해 살펴본다

아래 그림은 polynomial poisson model(log linear model)의 fitting을 모델 복잡도에 따라 다르게 나타낸 것이다. 함수식은 df에 따라 다음과 같다

$$
f(x) = exp(\beta_0 + \beta_1x + \cdots + \beta_df x^{df}) , \ \ \ df = 2,3,4,5,6,7
$$



<img src='{{"/assets/img/casi-10-5.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

설명을 명확히 하자면, 기존 가지고 있는 데이터로부터 MLE를 추정해서 이를 plug in 한 분포를 그려본 것인데 fitting의 과정은 아래와 같다.


샘플을 k개의 구간으로 쪼갠 다음, 구간의 중위수를 $x_{(k)}$ 라고 하자. 그리고 각 구간에 속한 데이터의 개수를 
$y_k$ 라고 하면
($x_{(k)}$ , 
$y_k$) 를 포아송모형에 적합시킬 수 있다.

$$
log(\mu_k) = \sum_{j=0} ^ {df} \beta_j x_{(k)}^j
$$

이 모형에서 회귀계수값의 MLE 값을 구하게 되면 

$$
log(\hat \mu_k) = \sum_{j=0} ^ {df} \hat \beta_j x_{(k)}^j
$$

일 것이므로 기존 bootstrap sample 을 얻는 프로세스와 동일하게,


$\hat \mu$ 를 plug in 한 분포 $F_{\hat \mu}$ 로 부터 bootstrap sample을 얻고 이를 활용해 bootstrap MLE
$s(x^{\star}) = \hat \mu ^{\star} $ 를 얻을 수 있을 것이다.(즉 기존 데이터로부터 모수를 추정하고, 추정한 모수를 plug-in 한 분포로부터 다시 데이터를 추출해서(bootstrap) 모수를 bootstrap sample로 재추정(즉 bootstrap data를 다시 구간으로 쪼개고 포아송모델에 적합해서 회귀계수와 $\mu$를 bootstrap방식으로 다시 추정하는것). 이를 통해 각각의 gfr값 (x값)에 대한 적합값의 SE를 구할 수 있다.

df 값의 변화에 따라 (모델의 변화에 따라) SE 값이 어떻게 변화하는지, 그리고 데이터는 잘 적합되는지 살펴봄으로서 최적의 모델을 구할 수 있을 것이다.  


단 parametric bootstrap의 경우에는 이미 모델 자체를 결정한 상태이므로, outlier들의 영향을 줄여버린다. 따라서 outlier들이 일부 존재해서 실제 분포가 분산이 큼에도 불구하고 이를 과소 추정해버릴 우려가 있다. (*parametric familiies act as regularizers*)