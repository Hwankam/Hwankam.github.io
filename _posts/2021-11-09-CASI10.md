---
layout : post
title : Computer Age Statistical Inference - chap 10
subtitle : statistical method 10
date : 2021-11-09
#categories:
tags : [datascience, statistical method, Efron]
toc_sticky : true
use_math : true
comments: true
---

## The Jackknife and the Bootstrap

<br>

목적은 SE의 계산!

permuation test 는 without replacement $ calcuate p-value
bootstrap 은 with replacement % calculate Standard Error

standard error란 무엇인가? 추정량이 얼마나 정확한지 알고싶을 때!


컴퓨팅 기술이 발전하기 전까지는 plug-in 방식으로 binomial에서 p에 대한 se 추정값을 $\sqrt{\frac{\hat p(1- \hat p)}{n}}$ 로 구하는 것처럼 SE에 대한 추정을 하거나, Taylor expansion을 사용해서 구했다. 

그러나 잭나이프나 붓스트랩의 경우 nonformulaic computation based approach이다. 

<br>

### 10.1 The Jackknife Estimate of Standard Error

$$
x_i \sim \mathbf F \ \ \ \ \ \ \ for \ \ i=1,2,3,...n
$$

일 때 $\theta$의 
추정량은 $\hat \theta = s(\mathbf x) $ 이다. 가지고 있는 샘플이 
$x_1,x_2,\cdots, x_n$ 밖에 없을 때, 추정량에 대한 standard deviation을 어떻게 구할 수 있을까?

특정 i 번째 데이터를 제거한 뒤 동일한 계산식을 통해 $\theta$의 추정량을 구하고 이를 
$\hat \theta_{(i)} = s(\mathbf x_{(i)})$ 라고 하면 jackknife estimate of standard error for 
$\hat \theta$은 다음과 같이 구할 수 있다

$$
\hat se_{jack} = [\frac {n-1}{n} \sum_{i=1} ^n (\hat \theta_{(i)} - \hat \theta_{(\cdot)} )^2]^{1/2}, \ \ \ \ \ \ with \ \ \hat \theta_{(\cdot)} = \sum \hat \theta_{(i)}/n
$$

즉 $s(\cdot)$만 알고 있으면 잭나이프 SE는 쉽게 구할 수 있다.

jackknife formula의 특징은 다음과 같다

+ non-parametric
+ $s(\cdot)$만 알면 계산이 빠르다
+ n-1 개의 데이터를 사용한다. 여기서는 데이터를 하나 빼더라도 값이 smooth하게 바뀔 것이라는 가정이 깔려있다. 즉 not robust to sample 인 경우는 고려하지 않는 것 같다.
+ jackknife SE는 true SE에 대해 upwardly biased 이다

<img src='{{"/assets/img/casi-10-1.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

그림에서 age=25 일 때 jackkife SE가 크게 나타나는 것을 알 수 있다. 특히 이 그림은 교과서 Figure 1.2 의 lowess curve와 SE 를 나타낸 것인데, age=25 주변부분에서 그래프가 smooth한 성질을 지니지 못하기 때문에 SE 가 크게 변할 수밖에 없다.

즉 여기서 jackkife SE는 $s(\cdot)$ 의 derivative와 관련 있다는 것을 알 수 있다.

<br>

### 10.2 The Nonparametric Bootstrap


* motivation 

$$
F \longrightarrow x \longrightarrow \hat \theta
$$

모수에 대한 추정량을 얻는 방식은 위와 같다. 그러나 $F$ 를 모르기 때문에 empirical probability distribution 
$\hat F$ 를 통해 bootstrap 샘플과 추정량을 얻는다. Empirical probability distribution은 샘플데이터의 추출 확률을 모두 균등하게 둔 누적분포로 
$F$에 대한 nonparametric MLE 이다. 

$$
\hat F \longrightarrow x^* \longrightarrow \hat \theta^*
$$

Bootstrap SE를 얻는 공식은 다음과 같다. 
$$
\hat se_{boot} = [\frac {1}{B-1} \sum_{b=1} ^B (\hat \theta^{*b} - \hat \theta^{*\cdot} )^2]^{1/2}, \ \ \ \ \ \ with \ \ \hat \theta^{*\cdot} = \sum \hat \theta^{*b}/B
$$

Bootstrap formula의 특징은 다음과 같다

+ 여기서 묘사하는 것은 one-sample non-parametric bootstrap
+ $s(\cdot)$만 알면 계산이 빠르다
+ bootstrap은 모든 데이터를 shake(wit replacement)하기 때문에 local derivative에 의존하는 jackknife 보다 더욱 unsmoothness에 의존적이다. 
+ jackknife SE는 true SE에 대해 upwardly biased 이다
+ SE 뿐만 아니라, bootstrap sample을 사용해서 absolute error, fisher information 등등을 만들 수 있다. 특히 fisher information의 경우 bootstrap 추정량을 plug in 하는 형태로 구할텐데, bootstrap 과 fisherian 의 연관성이 뒷장 parametric bootstrap에 잘 드러난다.


추가로, SE를 구하면 normality 가정하에 CI도 구할 수 있다. 그러나 bootstrap 샘플의 히스토그램을 그려보고 과연 normality 가정을 사용할 수 있을지 생각해봐야 한다. 


지금까지 나온 것으로 볼 때, jackknife는 SE와 bias 등을 살펴보는 frequentist device임을 알수있다. 반면 bootstrap은 frequentist, fisherian, Bayesian의 견해와 모두 연결되는 부분이 있는데 뒤에서 이를 자세히 살펴본다.


<br>

### 10.3 Resampling Plans

앞서서 jackknife와 bootstrap을 설명할 때에는 샘플이 뽑힐 확률을 모두 균등하게 둔 상태였다. 그러나 가지고 있는 sample data의 weight를 바꾸면 어떻게 될까? 

sample $x$와 
resampling vector $P = (P_1, P_2, \cdots, P_n)'$ 에 대해 (p는 확률값이므로 sum = 1)

새로운 weight를 사용해서 뽑은 샘플이 있을텐데 이를 활용한 추정량을 $\hat \theta ^* = S(\mathbf P)$ 라고 하자. 

예를들어, 분산에 대한 불편추청량은 $s(\mathbf x) = \sum(x_i - \bar x)^2 / (n-1) = \frac {n}{n-1} [\frac{1}{n} \sum x_i^2 - (\frac{1}{n} \sum x_i)^2 ]$ 이므로 reweight 추정량은 아래와 같다

$$
S(\mathbf P) = \frac {n}{n-1} [\sum \mathbf P_i x_i^2 - (\sum \mathbf P_i x_i)^2 ]
$$

이제 jackknife weight 와 bootstrap weight 중 기존의 균등 weight와 더 거리가 가까운 것은 어떤 것인지 살펴보자


<img src='{{"/assets/img/casi-10-2.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

<img src='{{"/assets/img/casi-10-3.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

이 결과를 가지고 아래 그림을 보면 조금 이상한 점이 있다. 

$$
\frac {(E||p^* - p_0||^2)^{1/2}} {||p_{(i)} - p_0||} \sim \sqrt n \rightarrow \infty \ \ as \ \ n \rightarrow \infty
$$

인 것에 비해 $P_{(i)}$ 와
$P^*$ 의 거리가 너무 가깝다는 점이다. 

<img src='{{"/assets/img/casi-10-4.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

<br>


#### remarks

1. $\hat se_{jack}$는 
$P_0$가 
$P_{(i)}$ 로 바뀔 때, 
$S(\mathbf P_0)$ 가 
$S(\mathbf P_{(i)})$ 로 얼마만큼 바뀌는지에 관한 것을 보여준다.

즉 " directional derivative of $S(P)$ at 
$P_0$ along with the direction 
$P_{(i)} - P_0$ " 를 다음과 같은 식으로 표현할 수 있다.

$$
D_i = \frac {S(p_{(i)}) - S(p_0)} {||p_{(i)} - p_0||} = \frac {\hat \theta _{(i)} - \hat \theta} {1/ \sqrt{n(n-1)}}
$$

이를 활용할 때, 

$$
\hat se_{jack} = [\frac {\sum D_i^2}{n^2}]^{1/2}
$$


2. 동일한 논리로 $\hat se_{boot}$는 
$P_0$가 
$P^*$ 로 바뀔 때, 
$S(\mathbf P_0)$ 가 
$S(\mathbf P^{*})$ 로 얼마만큼 바뀌는지에 관한 것을 보여준다.



### 10.4 The Parametric Bootstrap

