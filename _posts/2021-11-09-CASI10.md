---
layout : post
title : Computer Age Statistical Inference - chap 10
subtitle : statistical method 10
date : 2021-11-09
#categories:
tags : [datascience, statistical method, Efron]
toc_sticky : true
use_math : true
comments: true
---

## The Jackknife and the Bootstrap

<br>

목적은 SE의 계산!

permuation test 는 without replacement $ calcuate p-value
bootstrap 은 with replacement % calculate Standard Error

standard error란 무엇인가? 추정량이 얼마나 정확한지 알고싶을 때!


컴퓨팅 기술이 발전하기 전까지는 plug-in 방식으로 binomial에서 p에 대한 se 추정값을 $\sqrt{\frac{\hat p(1- \hat p)}{n}}$ 로 구하는 것처럼 SE에 대한 추정을 하거나, Taylor expansion을 사용해서 구했다. 

그러나 잭나이프나 붓스트랩의 경우 nonformulaic computation based approach이다. 

<br>

### 10.1 The Jackknife Estimate of Standard Error

$$
x_i \sim \mathbf F \ \ \ \ \ \ \ for \ \ i=1,2,3,...n
$$

일 때 $\theta$의 
추정량은 $\hat \theta = s(\mathbf x) $ 이다. 가지고 있는 샘플이 
$x_1,x_2,\cdots, x_n$ 밖에 없을 때, 추정량에 대한 standard deviation을 어떻게 구할 수 있을까?

특정 i 번째 데이터를 제거한 뒤 동일한 계산식을 통해 $\theta$의 추정량을 구하고 이를 
$\hat \theta_{(i)} = s(\mathbf x_{(i)})$ 라고 하면 jackknife estimate of standard error for 
$\hat \theta$은 다음과 같이 구할 수 있다

$$
\hat se_{jack} = [\frac {n-1}{n} \sum_{i=1} ^n (\hat \theta_{(i)} - \hat \theta_{(\cdot)} )^2]^{1/2}, \ \ \ \ \ \ with \ \ \hat \theta_{(\cdot)} = \sum \hat \theta_{(i)}/n
$$

즉 $s(\cdot)$만 알고 있으면 잭나이프 SE는 쉽게 구할 수 있다.

jackknife formula의 특징은 다음과 같다

+ non-parametric
+ $s(\cdot)$만 알면 계산이 빠르다
+ n-1 개의 데이터를 사용한다. 여기서는 데이터를 하나 빼더라도 값이 smooth하게 바뀔 것이라는 가정이 깔려있다. 즉 not robust to sample 인 경우는 고려하지 않는 것 같다.
+ jackknife SE는 true SE에 대해 upwardly biased 이다

<img src='{{"/assets/img/casi-10-1.png"| relative_url}}'  width="70%" height="70%" title="1" alt='relative'>

그림에서 age=25 일 때 jackkife SE가 크게 나타나는 것을 알 수 있다. 특히 이 그림은 교과서 Figure 1.2 의 lowess curve와 SE 를 나타낸 것인데, age=25 주변부분에서 그래프가 smooth한 성질을 지니지 못하기 때문에 SE 가 크게 변할 수밖에 없다.

즉 여기서 jackkife SE는 $s(\cdot)$ 의 derivative와 관련 있다는 것을 알 수 있다.

<br>

### 10.2 The Nonparametric Bootstrap


* motivation 

$$
F \longrightarrow x \longrightarrow \hat \theta
$$

모수에 대한 추정량을 얻는 방식은 위와 같다. 그러나 $F$ 를 모르기 때문에 empirical probability distribution 
$\hat F$ 를 통해 bootstrap 샘플과 추정량을 얻는다. Empirical probability distribution은 샘플데이터의 추출 확률을 모두 균등하게 둔 누적분포로 
$F$에 대한 nonparametric MLE 이다. 

$$
\hat F \longrightarrow x^* \longrightarrow \hat \theta^*
$$

Bootstrap SE를 얻는 공식은 다음과 같다. 
$$
\hat se_{boot} = [\frac {1}{B-1} \sum_{b=1} ^B (\hat \theta^{*b} - \hat \theta^{*\cdot} )^2]^{1/2}, \ \ \ \ \ \ with \ \ \hat \theta^{*\cdot} = \sum \hat \theta^{*b}/B
$$

Bootstrap formula의 특징은 다음과 같다

+ 여기서 묘사하는 것은 one-sample non-parametric bootstrap
+ $s(\cdot)$만 알면 계산이 빠르다
+ bootstrap은 모든 데이터를 shake(wit replacement)하기 때문에 local derivative에 의존하는 jackknife 보다 더욱 unsmoothness에 의존적이다. 
+ jackknife SE는 true SE에 대해 upwardly biased 이다
+ SE 뿐만 아니라, bootstrap sample을 사용해서 absolute error, fisher information 등등을 만들 수 있다. 특히 fisher information의 경우 bootstrap 추정량을 plug in 하는 형태로 구할텐데, bootstrap 과 fisherian 의 연관성이 뒷장 parametric bootstrap에 잘 드러난다.


추가로, SE를 구하면 normality 가정하에 CI도 구할 수 있다. 그러나 bootstrap 샘플의 히스토그램을 그려보고 과연 normality 가정을 사용할 수 있을지 생각해봐야 한다. 


지금까지 나온 것으로 볼 때, jackknife는 SE와 bias 등을 살펴보는 frequentist device임을 알수있다. 반면 bootstrap은 frequentist, fisherian, Bayesian의 견해와 모두 연결되는 부분이 있는데 뒤에서 이를 자세히 살펴본다.


<br>

### 10.3 Resampling Plans

<br>

### 10.4 The Parametric Bootstrap

