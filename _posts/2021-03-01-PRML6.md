---
layout : post
title : PRML-6
subtitle : PRML
date : 2022-03-01
#categories:
tags : [PRML, Machine learning]
toc_sticky : true
use_math : true
comments: true
--- 

## 6. Kernel Methods

there is a class of pattern recognition techniques

==> training data points are kept, memory based method

ex ) nearest neighborhood 방법은 training data와 가장 유사한 label을 test data에서 선택함. 즉 여기서 training data 가 storing 된다는 것은 모델 내부에서 파라미터를 정하는 학습이 아니라는 뜻. 

kerenl function : $k(x, x') = \phi(x)^T \phi(x')$

특징 : symmetric

종류 

identity kernel(linear kernel) : $k(x, x') = x^T x'$

stationary kernel : $k(x, x') = k(x - x') $ , 오직 차이에만 의존하므로 translation invarinace 를 갖는다.

homogeneous kernel(radial basis kernel) : $k(x, x') = k(||x - x'||) $ , 오직 거리의 크기에만 의존

large margin classifier로 머신러닝에서의 kernel function 중요성이 재조명됨

"extenstion of kernels to handle symbolic objects" ??


<br>


### 6.1 Dual Representations

Many linear models can be reformulated in terms of a dual representation => kernel function

예측함수를 $w^t \phi(x)$ 라고 할 때, regularized sum of squares error function을 고려하면 이를 최소화하는 벡터 w는 $\phi(x)$ 의 linear combination으로 표현 가능하다. 

즉 dual formulation을 통해서 최소제곱의 문제를 kernel function을 통해 표현 가능하다. 

gram matrix $ K = \boldsymbol{\phi} \boldsymbol{\phi}^T $  에 대해서 error function을 다음과 같이 적을 수 있다

$$
\begin{align} 

&J(a) = \frac{1}{2} a ^T \boldsymbol{K}^T\boldsymbol{K}a - a^T \boldsymbol{K} \boldsymbol{t} + \frac{1}{2} \boldsymbol{t}^t\boldsymbol{t} + \frac{\lambda}{2} a^T \boldsymbol{K} a
\\
&\textrm{where} \ \ \ \ w =  \boldsymbol{\phi}^T a \ \ \ \ \& \ \ \ \ a_n = \frac {-1}{\lambda} \{ w^T \phi(x_n) - t_n \}

\end{align}
$$

error function의 gradient 를 0으로 하는 a를 구해 prediction을 구하면

$$
y(x) = w^T \boldsymbol{\phi}(x) = a^T \boldsymbol{\phi} \ \boldsymbol{\phi}(x)
$$


파라미터 w 에서 이를 변환한 a 를 고려하게 되면 일반적으로 차원은 더 커지게 된다. 그러나 kernel function $k(x, x')$을 통해서 온전히 식을 표시할 수 있으며 feature vector 
$\phi(x)$가 무엇인지 명확하게 제시할 필요도 없어진다. => high dimension을 다룰 수 있다.  
<br>


### 6.2 Constructing Kernels

construct valid kernel function이 중요하다

첫번째 방법으로는 mapping $\phi(x)$를 설정한 이후에 내적을 통해 kernel function을 찾는 것이 있고, 두번째 방법으로는 kernel fucntion을 directly 찾는 것이 있다. 두번째 방법에서는 내가 정한 kernel function이 정말 유효한 kernel function인지 찾는 것이 중요하다. 이때 gram matrix(각 원소가 kernel $\phi(x)$의 내적) positive semi - definite이라는 조건만 있으면 kernel function이 되는 필요충분조건을 만족한다. 

따라서 기본 형태의 kernel fucntion을 찾은 뒤 building block의 형태로 kernel을 만들어나간다. 

gaussian kernel : $k(x, x') = exp(-|| x - x' || ^2 / 2\sigma^2) = exp(-x^tx / 2\sigma^2) exp(x^tx/\sigma^2) exp(-(x')^tx' / 2\sigma^2 ) $

gaussian kernel은 결국 linear kernel인 $k(x, x') = x^t x'$를 block으로 해서 만든 것이므로 kernel function이 됨을 알 수 있다. 


<br>

kernel 종류

* discriminant model과 generative model 섞기 : similarity measure

  use generative model to define a kernel and then use this kernel in a discriminative approach.
  
  ex ) Fisher kerenl 

  $\dot l(\theta) \sim  \mathcal{N}(0 , n I(\theta)) $ 과 연관되어 있지 않을까

* sigmoidal kernel
  실제로 많이 이용되는 커널. SVM을 neural network와 연결짓는 kernel 

  "in the limit of an infinite number of basis functions, a Bayesian neural network with an appropriate prior reduces to a Gaussian process"
<br>


### 6.3 Radial Basis Function Networks

앞 장에서부터 basis function 을 활용한 linear regression 과 classification 에 대해 이야기했다. 대표적인 basis function으로는 radial basis function이 있다. radial basis function은 Euclidean distance를 기반으로 한 basis function이다. 

본래 radial basis function은 exact function interpolation을 목적으로 만들어졌다. input과 target을 정확하게 fitting 하는 smooth function을 만들기위해서 radial basis function의 linear combination을 만든다.

$$
f(x) = \sum_{n=1} w_n h(||x-x_n||)
$$

그러나 exact interpolation은 overfitting의 문제가 있다. 


input이 noisy 한 경우 interpolation을 고려하는 경우에도 radial basis function을 고려할 수 있다. 

$$
E = \frac{1}{2} \sum_{n=1} \int \{y(x_n + \xi) - t_n   \}^2 v(\xi) d \xi 
$$

이 때 optimization은

$$
y(x_n) = \sum t_n h(x-x_n) \ \ \ \ \ \ \textrm{where} \ \ \ h(x- x_n) = \frac {v(x-x_n)}{\sum v(x - x_n)}
$$
<br>


#### 6.3.1 Nadaraya-Watson model


<br>


### 6.4 Gaussian Process


<br>


#### 6.4.1 Linear regression revisited


<br>
