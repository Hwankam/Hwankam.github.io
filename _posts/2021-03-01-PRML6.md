---
layout : post
title : PRML-6
subtitle : PRML
date : 2022-03-01
#categories:
tags : [PRML, Machine learning]
toc_sticky : true
use_math : true
comments: true
--- 

## 6. Kernel Methods

there is a class of pattern recognition techniques

==> training data points are kept, memory based method

ex ) nearest neighborhood 방법은 training data와 가장 유사한 label을 test data에서 선택함. 즉 여기서 training data 가 storing 된다는 것은 모델 내부에서 파라미터를 정하는 학습이 아니라는 뜻. 

kerenl function : $k(x, x') = \phi(x)^T \phi(x')$

특징 : symmetric

종류 

identity kernel(linear kernel) : $k(x, x') = x^T x'$

stationary kernel : $k(x, x') = k(x - x') $ , 오직 차이에만 의존하므로 translation invarinace 를 갖는다.

homogeneous kernel(radial basis kernel) : $k(x, x') = k(||x - x'||) $ , 오직 거리의 크기에만 의존

large margin classifier로 머신러닝에서의 kernel function 중요성이 재조명됨

"extenstion of kernels to handle symbolic objects" ??


<br>


### 6.1 Dual Representations

Many linear models can be reformulated in terms of a dual representation => kernel function

예측함수를 $w^t \phi(x)$ 라고 할 때, regularized sum of squares error function을 고려하면 이를 최소화하는 벡터 w는 $\phi(x)$ 의 linear combination으로 표현 가능하다. 

즉 dual formulation을 통해서 최소제곱의 문제를 kernel function을 통해 표현 가능하다. 

gram matrix $ K = \boldsymbol{\Phi} \boldsymbol{\Phi}^T $  에 대해서 error function을 다음과 같이 적을 수 있다

$$
\begin{align} 

&J(a) = \frac{1}{2} a ^T \boldsymbol{K}^T\boldsymbol{K}a - a^T \boldsymbol{K} \boldsymbol{t} + \frac{1}{2} \boldsymbol{t}^t\boldsymbol{t} + \frac{\lambda}{2} a^T \boldsymbol{K} a
\\
&\textrm{where} \ \ \ \ w =  \boldsymbol{\Phi}^T a \ \ \ \ \& \ \ \ \ a_n = \frac {-1}{\lambda} \{ w^T \phi(x_n) - t_n \}

\end{align}
$$

error function의 gradient 를 0으로 하는 a를 구해 prediction을 구하면

$$
y(x) = w^T \boldsymbol{\phi}(x) = a^T \boldsymbol{\Phi} \ \boldsymbol{\phi}(x)
$$


파라미터 w 에서 이를 변환한 a 를 고려하게 되면 일반적으로 차원은 더 커지게 된다. 그러나 kernel function $k(x, x')$을 통해서 온전히 식을 표시할 수 있으며 feature vector 
$\phi(x)$가 무엇인지 명확하게 제시할 필요도 없어진다. => high dimension을 다룰 수 있다.  
<br>


### 6.2 Constructing Kernels

construct valid kernel function이 중요하다

첫번째 방법으로는 mapping $\phi(x)$를 설정한 이후에 내적을 통해 kernel function을 찾는 것이 있고, 두번째 방법으로는 kernel fucntion을 directly 찾는 것이 있다. 두번째 방법에서는 내가 정한 kernel function이 정말 유효한 kernel function인지 찾는 것이 중요하다. 이때 gram matrix(각 원소가 kernel $\phi(x)$의 내적) positive semi - definite이라는 조건만 있으면 kernel function이 되는 필요충분조건을 만족한다. 

따라서 기본 형태의 kernel fucntion을 찾은 뒤 building block의 형태로 kernel을 만들어나간다. 

gaussian kernel : $k(x, x') = exp(-|| x - x' || ^2 / 2\sigma^2) = exp(-x^tx / 2\sigma^2) exp(x^tx/\sigma^2) exp(-(x')^tx' / 2\sigma^2 ) $

gaussian kernel은 결국 linear kernel인 $k(x, x') = x^t x'$를 block으로 해서 만든 것이므로 kernel function이 됨을 알 수 있다. 


<br>

kernel 종류

* discriminant model과 generative model 섞기 : similarity measure

  use generative model to define a kernel and then use this kernel in a discriminative approach.
  
  ex ) Fisher kerenl 

  $\dot l(\theta) \sim  \mathcal{N}(0 , n I(\theta)) $ 과 연관되어 있지 않을까

* sigmoidal kernel
  실제로 많이 이용되는 커널. SVM을 neural network와 연결짓는 kernel 

  "in the limit of an infinite number of basis functions, a Bayesian neural network with an appropriate prior reduces to a Gaussian process"
<br>


### 6.3 Radial Basis Function Networks

앞 장에서부터 basis function 을 활용한 linear regression 과 classification 에 대해 이야기했다. 대표적인 basis function으로는 radial basis function이 있다. radial basis function은 Euclidean distance를 기반으로 한 basis function이다. 

본래 radial basis function은 exact function interpolation을 목적으로 만들어졌다. input과 target을 정확하게 fitting 하는 smooth function을 만들기위해서 radial basis function의 linear combination을 만든다.

$$
f(x) = \sum_{n=1} w_n h(||x-x_n||)
$$

여기서 $w_n$은 least square를 통해 얻을 수 있다.

그러나 exact interpolation은 overfitting의 문제가 있다. 


input이 noisy 한 경우 interpolation을 고려하는 경우에도 radial basis function을 고려할 수 있다. 

$$
E = \frac{1}{2} \sum_{n=1} \int \{y(x_n + \xi) - t_n   \}^2 v(\xi) d \xi 
$$

이 때 optimization은

$$
y(x_n) = \sum t_n h(x-x_n) \ \ \ \ \ \ \textrm{where} \ \ \ h(x- x_n) = \frac {v(x-x_n)}{\sum v(x - x_n)}
$$
<br>

이때 basis function h 는 normalized 되어있으므로, 이를 통해 모든 basis function이 작은 값을 갖게 되는 경우를 방지할 수 있다. 



<br>

normalized radial basis function의 확장으로, regression 문제에서의 kernel density estimation이 있다. 
여기서는 computing cost를 절감하기 위한 방안으로써, data point 보다 더 작은 개수의 basis function을 setting 하는 법이 있음을 말하고 있다. 
일반적으로 basis function의 개수와 center에 대한 location $\mu_i$ 는 data point에 의해 정해지며
그 형태는 사전적이다. 또한 계수 $w_i$ 는 최소제곱을 통해 정해진다. 이 때에 basis function의 개수를 data point 개수보다 줄이기 위해서는 basis function의 center를 sum-of-square error를 최대한으로 줄이는 data point에 상응하는 center로 선택하는 sequential selection process가 있다. 이 방식은 통상적으로 orthogonal least square 라고 불린다.

###### orthogonal least square -- 논문 orthogonal least square regression : a unifed apporoach for data modeling

sparse kernel data modeling을 통해 kernel matrix를 QR 분해해서 orthogonoalize

[사진 1]

[사진 2]

#### 6.3.1 Nadaraya-Watson model

(3.62)에서는 equivalent kernel $k(x, x') = \beta \phi(x)^T S_N \phi(x')$ 를 제시하며 predictive distribution mean이 training data의 target
$t_n$의 선형결합으로 나타내어짐을 보였다. 

이제 kernel density estimation의 관점에서 이를 살펴보자.

joint distribution $p(x,t)$ 를 추정하기 위해 Parzen density estimator를 활용해보자. 

$$
p(x,t) = \frac {1}{N} \sum_{n=1} ^N f(x - x_n, t- t_n)
$$


여기서 Paren density estimator는 kernel density estimation을 위한 방법 중 하나이다. 아래는 이에 대한 간단한 설명이다.


[색깔 넣기]
[https://jayhey.github.io/novelty%20detection/2017/11/08/Novelty_detection_Kernel/]
1장에서도 짧게 배웠듯이, $P(x) \simeq \frac {K}{N V}$ 로 적을 수 있고, V를 고정시키고 K를 구하는 것은 kernel density estimator와 같이 생각할 수 있다.(반대로 K를 고정시키고 V를 찾는 것은 K-nearest neighborhood 라고 할 수 있다) 이를 활용해 d-dimension에서
$V = h^d$ 라고 할 때 x, t 를 기준으로 각 차원으로 h/2 안에 존재하는 데이터의 개수를 세어 밀도를 추정한다고 할 때 

$$
p(x, t) =\frac {1}{N h^d} \sum_{n=1} ^N f(\frac{x - x_n}{h}, \frac{t- t_n}{h})
$$

위의 식으로 밀도추정이 가능하다. 



이제 target variable 에 대한 conditional expectation을 구하는 것에 대해 생각해보자. 

$$
\begin{align}
y(x) & = E[t|x] = \int t p(t|x) dt \\

& = \frac{\int t p(x,t) dt }{\int p(x,t) dt} \\

& = \frac {\sum_n \int t f(x-x_n, t- t_n) dt }{\sum _m\int f(x-x_m, t-t_m) dt} \ \ \ \ \cdots \textrm{using Parzen density estimation }
\end{align}
$$


component f에 대한 zero mean을 가정하고, $g(x) = \int f(x,t) dt$ 라고 정의하면

$$
\begin{align}
y(x) & = \frac {\sum_n g(x-x_n) t_n}{\sum_m g(x-x_m)} \\
& = \sum k(x,x_n) t_n
\end{align}
$$

위 식의 마지막 결과를 보면 앞에서 Bayesian regresssion에서 보았던 predictive distribution mean과 비슷한 형태를 가지고 있음을 알 수 있다. 


<br>


### 6.4 Gaussian Process

kernel을 사용해 probablistic discriminative model로 확장해보자.(6.1은 non-probablistic model) 이 장에서는 kernel이 Bayesian setting과 어떻게 연관되는지 보여준다. 


<br>


#### 6.4.1 Linear regression revisited

다시 linear regression 문제로 돌아가서, predictive distirbution을 도출해내는 것을 생각해보자. (predictive dist 는 다음 절에서 나오는 듯?)

basis function의 선형결합으로 구성된 모델을 생각해보자.

$$
y(x,w) = w^T \phi(x)
$$

이때 w 의 Gaussian prior를 다음과 같이 정의한다.

$$
p(w) = \mathcal{N}(w |0, \alpha^{-1} I)
$$


추정하고자 하는 함수는 본래 $y(x,w)$ 인데, w 에 대한 확률분포를 알고 있으므로 우리는 
$y(x,w)$ 에 대한 확률분포를 구할 수 있다.

training data point x 에 대한 joint distribution은 다음과 같다.

$$
\boldsymbol{Y} = \boldsymbol{\Phi} w
$$


$\boldsymbol{Y}$ 는 w 에 대한 linear combination이므로, 그 자체로 이미 Gaussian 이다. 그리고 평균과 분산은 다음과 같다.

$$
\begin{align}
& E[\boldsymbol{Y}] = \Phi E[w] = 0 \\

& cov[\boldsymbol{Y}] = E[\boldsymbol{Y} \boldsymbol{Y}^T] = \Phi E[w w^T] \Phi^T = \frac{1}{\alpha} \Phi \Phi^T \\ & \ \ \ \ \ \ \ \ \ \ \ \ =  K \textrm{(gram matrix)}

\end{align}
$$

이 모델(linear model)은 Gaussian process 의 특별한 모델로서 일반적으로 Gaussian process 는 joint distribution $\boldsymbol{Y}(x)$ 로 정의된다. 

Gaussian 이므로 평균과 분산은 second order statistic (second moment) 가 중요하고, w에 대한 prior를 0으로 잡을 때는, covariance matrix 인 gram matrix가 중요하다. Gaussian process 에서도 kernel을 직접 정할 수 있으며 아래 그림은 GP에서 각각 Gaussian kernel과 exponential kernel을 설정해 densitiy estimation을 수행한 결과이다.

[fig 6.4]

<br>

