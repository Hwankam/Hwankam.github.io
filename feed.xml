<feed xmlns="http://www.w3.org/2005/Atom"> <id>https://hwankam.github.io/</id><title>For Statistics</title><subtitle>A minimal, portfolio, sidebar, bootstrap Jekyll theme with responsive web design and focuses on text presentation.</subtitle> <updated>2022-03-17T21:57:37+09:00</updated> <author> <name>your_full_name</name> <uri>https://hwankam.github.io/</uri> </author><link rel="self" type="application/atom+xml" href="https://hwankam.github.io/feed.xml"/><link rel="alternate" type="text/html" hreflang="en" href="https://hwankam.github.io/"/> <generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator> <rights> © 2022 your_full_name </rights> <icon>/assets/img/favicons/favicon.ico</icon> <logo>/assets/img/favicons/favicon-96x96.png</logo> <entry><title>PRML-6</title><link href="https://hwankam.github.io/posts/PRML6/" rel="alternate" type="text/html" title="PRML-6" /><published>2022-03-01T00:00:00+09:00</published> <updated>2022-03-17T21:57:15+09:00</updated> <id>https://hwankam.github.io/posts/PRML6/</id> <content src="https://hwankam.github.io/posts/PRML6/" /> <author> <name>your_full_name</name> </author> <summary> 6. Kernel Methods [서론] there is a class of pattern recognition techniques ==&amp;gt; training data points are kept, memory based method ex ) nearest neighborhood 방법은 training data와 가장 유사한 label을 test data에서 선택함. 즉 여기서 training data 가 storing 된다는 것은 모델 내부에서 파라미터를 정하는 학습이 아니라는 뜻. 즉, 모델을 달라고한다면 선형회귀모델 같은 경우에는 파라미터 계수를 주면 되겠지만, NN의 경우에는 데이터 자체를 줘야한다. kerenl function $\ \ $ : $k(x, x’) = \phi(x)^T... </summary> </entry> <entry><title>PRML-5</title><link href="https://hwankam.github.io/posts/PRML5/" rel="alternate" type="text/html" title="PRML-5" /><published>2022-02-09T00:00:00+09:00</published> <updated>2022-03-01T22:43:39+09:00</updated> <id>https://hwankam.github.io/posts/PRML5/</id> <content src="https://hwankam.github.io/posts/PRML5/" /> <author> <name>your_full_name</name> </author> <summary> 5. Neural Networks chap 3과 4에서는 linear combination of fixed bases function에 대해 배웠다. 그러나 high dimension에서는 과적합의 문제가 발생하는 한계가 있었다. 이를 해결하기 위해 데이터에 맞게 basis function을 바꾸는 것은 어떠할까? SVM은 데이터에 맞게 hyperplane을 설정하는 것이므로 맥락에 부합한다. 이는 basis function의 후보 가운데 일부를 선택해서 데이터에 적합시키는 것이므로 ‘차원의 저주’로 부터 좀 더 자유로워질 수 있다. 또한 convex optimization 문제로 풀 수 있기 때문에 solution이 명확하다. 좀 더 다른 방식으로, basis function 개수는 고정하지만 ... </summary> </entry> <entry><title>PRML-4</title><link href="https://hwankam.github.io/posts/PRML4/" rel="alternate" type="text/html" title="PRML-4" /><published>2022-01-19T00:00:00+09:00</published> <updated>2022-01-27T23:39:26+09:00</updated> <id>https://hwankam.github.io/posts/PRML4/</id> <content src="https://hwankam.github.io/posts/PRML4/" /> <author> <name>your_full_name</name> </author> <summary> 4. Linear Models for Classification 이번 chapter는 “Input space를 K개의 Class로 나누는 것”이 핵심이다. 이때 나눠지는 영역은 decision region, 나누는 boundary를 decision boundary 혹은 decision surface라 한다. 특히 이번 chapter에서 중요한 것은 linear model이다. 즉 분류를 위한 decision surface가 linear function이며 D-dimensional input space를 (D-1)-dimensional hyperplane으로 나누는 것을 의미한다. linear model을 활용한 classification에서 주목하는 세 가지 접근방식은 다음과 같다. thre... </summary> </entry> <entry><title>Computer Age Statistical Inference - chap 15</title><link href="https://hwankam.github.io/posts/CASI15/" rel="alternate" type="text/html" title="Computer Age Statistical Inference - chap 15" /><published>2021-11-21T00:00:00+09:00</published> <updated>2021-11-21T16:54:25+09:00</updated> <id>https://hwankam.github.io/posts/CASI15/</id> <content src="https://hwankam.github.io/posts/CASI15/" /> <author> <name>your_full_name</name> </author> <summary> Large-Scale Hypothesis Testing and False-Discovery Rates large-scale data analysis like microarrays -&amp;gt; thousands of simultaneous hypothesis test 15.1 Large-Scale Testing 앞선 여러 장에서 prostate cancer data를 활용한 여러가지 예제들을 살펴보았었다. 이 데이터는 6033 by 102 Matrix 이며 columns은 cancer pateints 52, normal controls 50 으로 이뤄져있다. 6033개의 gene에 대해 각각 Two-sample t-test를 실행하면 각각의 t-statistic 은 자유도가 100인 t 분포를 따른... </summary> </entry> <entry><title>Computer Age Statistical Inference - chap 11</title><link href="https://hwankam.github.io/posts/CASI11/" rel="alternate" type="text/html" title="Computer Age Statistical Inference - chap 11" /><published>2021-11-17T00:00:00+09:00</published> <updated>2021-11-21T13:13:53+09:00</updated> <id>https://hwankam.github.io/posts/CASI11/</id> <content src="https://hwankam.github.io/posts/CASI11/" /> <author> <name>your_full_name</name> </author> <summary> Bootstrap Condfidence Intervals 일반적으로 CI를 구한다고 하면 다음과 같은 form을 떠올릴 것이다. \[\hat \theta \ \pm \ 1.96 \hat{se}\] 그러나 만약 데이터들이 포아송분포를 따른다고 하면, 위와 같은 형태로 신뢰구간을 구할 때 asymmetric distribution의 특징을 제대로 잡아내지 못할 것이다. 11.1 Neyman’s Construction for One-Parameter Problems 우선 pivot에 대해 간단히 언급하도록 하겠다. pivot이란, 분포가 파라미터 값에 의존하지 않는 확률변수를 의미힌다. 예를들어 $\bar X \sim N(\mu,1)$ 인 경우 $\bar X - \mu \sim N(0,1)$ ... </summary> </entry> </feed>
